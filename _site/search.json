[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Mark Druffel",
    "section": "",
    "text": "Hi There!\nI’m a data scientist, I mutate ☕ + 🍺 into  code.\n\n\n\nMotivation\nData science and technology are constantly evolving and keeping up can be difficult. I set up this blog as a tool to share some of the things I’ve learned with others and for my own posterity. R is my favorite data science language so most of my posts are written in R, but I also use Python, SQL, and others regularly so I try to share content regarding those as well.\n\n\n\nA bit about me\nCurrently, I am a lead data scientist at 84.51°. I live in Portland, OR with my amazing wife Brittany and our twin kiddos. When I’m not working with data, I love to cook, hike, rock-climb, play video games, travel, and explore the Pacific-Northwest.\n\nIf you ever have questions about a post or want to connect, please don’t hesitate to reach out!"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Web Scraping w/ R\n\n\nWeb scraping with R using rvest & RSelenium\n\n\n\n\nwalkthrough\n\n\nR\n\n\n\n\n\n\n\n\n\n\n\nAug 8, 2023\n\n\nMark Druffel\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a blogdown site\n\n\nBuild a blogdown site from scratch with code-folding, a custom footer, & other hugo theme hacks\n\n\n\n\nwalkthrough\n\n\n\n\n\n\n\n\n\n\n\nJan 5, 2022\n\n\nMark Druffel\n\n\n\n\n\n\n  \n\n\n\n\nEuropean Energry\n\n\nTidy Tuesday analysis of European energy production from 2016 to 2018.\n\n\n\n\ntidy-tuesday\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2021\n\n\nMark Druffel\n\n\n\n\n\n\n  \n\n\n\n\nFrench Train Delays\n\n\nTidy Tuesday analysis of the TGV high-speed rail service from 2015 to 2018.\n\n\n\n\ntidy-tuesday\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2021\n\n\nMark Druffel\n\n\n\n\n\n\n  \n\n\n\n\nGreat American Beer Festival\n\n\nTidy Tuesday analysis of the Great American Beer Festival winners from 1987 to 2020.\n\n\n\n\ntidy-tuesday\n\n\n\n\n\n\n\n\n\n\n\nOct 26, 2020\n\n\nMark Druffel\n\n\n\n\n\n\n  \n\n\n\n\nHello World\n\n\nStarting a blog to share thoughts & code\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2019\n\n\nMark Druffel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-05-04-french-trains/index.html#background",
    "href": "posts/2021-05-04-french-trains/index.html#background",
    "title": "French Train Delays",
    "section": "Background",
    "text": "Background\n\nFrench Trains Data\nThis Tidy Tuesday, I’m analyzing french trains data from TGV, France’s inter-city high-speed rail service. This is an older Tidy Tuesday data set, but it looks like a fun one and I’ve been wanting to spend some time learning a few networking packages. The TGV has been in operation since the 80’s and mostly goes throughout France, although a few legs are international. The TGV has data services that provide information about past train service with some documentation here. To download the data I use the tidytuesdayR package.\n\n\nCode\nknitr::opts_chunk$set(fig.width = 9)\nlibrary(knitr)\nlibrary(htmltools)\nlibrary(markUtils)\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(janitor)\nlibrary(patchwork)\nlibrary(tidymodels)\nlibrary(glue)\nlibrary(ggforce)\nlibrary(tidygraph)\nlibrary(ggraph)\nlibrary(tweenr)\nlibrary(animation)\nlibrary(rlang)\nlibrary(wikifacts)\nlibrary(sf)\nlibrary(leaflet)\nlibrary(ggrepel)\nlibrary(sfnetworks)\nlibrary(measurements)\nlibrary(osmdata)\nfull_trains &lt;- tidytuesdayR::tt_load('2019-02-26') |&gt; \n  pluck('full_trains')\n\n\n\n    Downloading file 1 of 3: `full_trains.csv`\n    Downloading file 2 of 3: `regularite-mensuelle-tgv-aqst.csv`\n    Downloading file 3 of 3: `small_trains.csv`"
  },
  {
    "objectID": "posts/2021-05-04-french-trains/index.html#analysis",
    "href": "posts/2021-05-04-french-trains/index.html#analysis",
    "title": "French Train Delays",
    "section": "Analysis",
    "text": "Analysis\nWe can summarize the data set with skimr, which provides a high-level understanding of the data. This data set has ~5.4k observations and 27 attributes. Most of the data is numeric and a number of columns start with num_ and avg_ which indicates that the data is aggregated. Most of the attributes have records for all observations (n_missing). The num_ fields are all counts so those should never be missing, meaning we can replace missing records with zero. The service attribute is missing 1.4k observations, but it’s a category of either national or international - should be easy to impute based on the departure_station and arrival_station. The 6 numeric attributes with names that start with delay_cause_ are all missing 3 percent of the observations. Rows missing delay_cause_ appear to be legs that had no delays. The comment_delays_on_arrival attribute is a comments field so it’s reasonable to have so many missing.\n\n\nCode\nskimr::skim(full_trains)\n\n\n\nData summary\n\n\nName\nfull_trains\n\n\nNumber of rows\n5462\n\n\nNumber of columns\n27\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nlogical\n2\n\n\nnumeric\n21\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nservice\n1430\n0.74\n8\n13\n0\n2\n0\n\n\ndeparture_station\n0\n1.00\n4\n30\n0\n59\n0\n\n\narrival_station\n0\n1.00\n4\n30\n0\n59\n0\n\n\ncomment_delays_on_arrival\n3921\n0.28\n10\n2678\n0\n681\n0\n\n\n\nVariable type: logical\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\ncount\n\n\n\n\ncomment_cancellations\n5462\n0\nNaN\n:\n\n\ncomment_delays_at_departure\n5462\n0\nNaN\n:\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1.00\n2016.52\n1.12\n2015.00\n2016.00\n2017.00\n2018.00\n2018.00\n▇▇▁▇▇\n\n\nmonth\n0\n1.00\n6.37\n3.39\n1.00\n3.00\n6.00\n9.00\n12.00\n▇▅▅▅▇\n\n\njourney_time_avg\n0\n1.00\n165.39\n78.94\n45.96\n100.77\n160.84\n205.70\n481.00\n▇▇▃▁▁\n\n\ntotal_num_trips\n0\n1.00\n281.07\n155.09\n6.00\n181.00\n238.00\n390.00\n878.00\n▅▇▃▂▁\n\n\nnum_of_canceled_trains\n0\n1.00\n7.74\n23.65\n0.00\n0.00\n1.00\n4.00\n279.00\n▇▁▁▁▁\n\n\nnum_late_at_departure\n0\n1.00\n41.58\n50.99\n0.00\n10.00\n23.00\n51.75\n451.00\n▇▁▁▁▁\n\n\navg_delay_late_at_departure\n0\n1.00\n16.81\n8.80\n0.00\n11.98\n15.84\n20.28\n173.57\n▇▁▁▁▁\n\n\navg_delay_all_departing\n0\n1.00\n2.54\n3.97\n-4.47\n0.90\n1.78\n3.24\n173.57\n▇▁▁▁▁\n\n\nnum_arriving_late\n9\n1.00\n38.03\n30.33\n0.00\n17.00\n30.00\n50.00\n235.00\n▇▂▁▁▁\n\n\navg_delay_late_on_arrival\n9\n1.00\n32.45\n13.63\n0.00\n23.81\n30.76\n38.77\n258.00\n▇▁▁▁▁\n\n\navg_delay_all_arriving\n0\n1.00\n5.29\n5.00\n-143.97\n2.71\n4.58\n7.25\n36.82\n▁▁▁▁▇\n\n\ndelay_cause_external_cause\n170\n0.97\n0.28\n0.16\n0.00\n0.17\n0.26\n0.37\n1.00\n▆▇▂▁▁\n\n\ndelay_cause_rail_infrastructure\n170\n0.97\n0.25\n0.15\n0.00\n0.15\n0.24\n0.33\n1.00\n▇▇▂▁▁\n\n\ndelay_cause_traffic_management\n170\n0.97\n0.18\n0.14\n0.00\n0.08\n0.16\n0.26\n1.00\n▇▅▁▁▁\n\n\ndelay_cause_rolling_stock\n170\n0.97\n0.18\n0.13\n0.00\n0.09\n0.16\n0.24\n1.00\n▇▃▁▁▁\n\n\ndelay_cause_station_management\n170\n0.97\n0.07\n0.08\n0.00\n0.00\n0.05\n0.10\n1.00\n▇▁▁▁▁\n\n\ndelay_cause_travelers\n170\n0.97\n0.04\n0.05\n0.00\n0.00\n0.02\n0.06\n0.67\n▇▁▁▁▁\n\n\nnum_greater_15_min_late\n5\n1.00\n26.09\n21.48\n0.00\n11.00\n20.00\n35.00\n192.00\n▇▂▁▁▁\n\n\navg_delay_late_greater_15_min\n5\n1.00\n28.98\n18.87\n-118.02\n8.99\n31.53\n41.00\n258.00\n▁▇▇▁▁\n\n\nnum_greater_30_min_late\n5\n1.00\n11.65\n10.41\n0.00\n4.00\n9.00\n16.00\n91.00\n▇▂▁▁▁\n\n\nnum_greater_60_min_late\n5\n1.00\n4.20\n4.68\n0.00\n1.00\n3.00\n6.00\n36.00\n▇▂▁▁▁\n\n\n\n\n\nBased on the summary table, we can create a date field (year + month), format station names from all upper to normal sentence, create a leg attribute (departure station + arrival station), and impute the appropriate missing values.\n\n\nCode\n# Add date, get rid of all caps for stations, create leg column, fill in missing service records\nservice &lt;- full_trains |&gt;\n  select(service, arrival_station, departure_station) |&gt; \n  group_by() |&gt; \n  distinct() |&gt; \n  filter(!is.na(service)) |&gt; \n  ungroup()\ntrains &lt;- full_trains |&gt;\n  mutate(across(starts_with(\"num_\"), ~replace_na(.x, 0))) |&gt; \n  select(-service) |&gt; \n  left_join(service) |&gt;\n  mutate(\n    date = as.Date(sprintf(\"%d-%02d-01\", year, month)),\n    departure_station = str_to_title(departure_station),\n    arrival_station = str_to_title(arrival_station),\n    leg = glue::glue(\"{departure_station} to {arrival_station}\")) |&gt;\n  mutate(service = if_else(\n    is.na(service),\n    case_when(str_detect(leg,\"Madrid\") ~ \"International\",\n              str_detect(leg, \"Barcelona\") ~ \"International\",\n              str_detect(leg, \"Francfort\") ~ \"International\",\n              str_detect(leg, \"Stuttgart\") ~ \"International\",\n              str_detect(leg, \"Zurich\") ~ \"International\",\n              str_detect(leg, \"Geneve\") ~ \"Internationl\",\n              T ~ \"National\"),\n    service)) |&gt; \n  select(service, leg, date, everything())\n\n\nTotal trips and late arrivals appear to have a linear relationship, which makes sense. There is an upward trend and significantly more variability from month to month than I would have expected. It appears there may have been a strike sometime in May or July of 2016 and it looks like there may be seasonality in the number of trips.\n\n\nCode\ntrips &lt;- trains |&gt;\n  group_by(date) |&gt;\n  summarise(trips = sum(total_num_trips)) |&gt;\n  ungroup() |&gt; \n  ggplot(aes(x = date, y = trips)) +\n  geom_line() +\n  geom_smooth(method = loess, se = T) +\n  scale_x_date(breaks = \"9 months\", date_labels = \"%b-%Y\") +\n  scale_y_continuous(labels = scales::number_format(accuracy = .1, scale = 1/1000, suffix = \"K\")) +\n  labs(y = \"Trips\", x = \"Date\", title = \"Scheduled Trips\", subtitle = \"Overall service increased over the ~4 years\") + \n  theme_blog()\nlate_arrivals &lt;- trains |&gt;\n  replace_na(list(num_arriving_late = 0)) |&gt; \n  group_by(date) |&gt;\n  summarise(late = sum(num_arriving_late)) |&gt;\n  ungroup() |&gt; \n  ggplot(aes(x = date, y = late)) +\n  geom_line() +\n  geom_smooth(method = loess, se = T) +\n  scale_x_date(breaks = \"9 months\", date_labels = \"%b-%Y\") +\n  scale_y_continuous(labels = scales::number_format(accuracy = .1, scale = 1/1000, suffix = \"K\")) +\n  labs(y = NULL, x = \"Date\", title = \"Late Arrivals\", subtitle = \"Late arrivals increased with service\") +\n  theme_blog() \n# Put plots side by side with patchwork https://github.com/thomasp85/patchwork\ntrips + late_arrivals\n\n\n\n\n\nTo get a better look of each leg we can plot the trips & late arrival data on heatmaps. Since the number of trips is not evenly distributed across the legs, it’s helpful to scale the data. We can use a log scale for the number of trips, but we can use a percentage for late arrivals since that’s easier to understand. A few legs had the majority of all trips (bright yellow) and a few of the legs didn’t start operating until January of 2018 (white). A legs had months where 40+ percent of all trips arrived late (e.g. June / July of 2018).\n\n\nCode\nservice &lt;- trains |&gt;\n  group_by(departure_station, year, date) |&gt; \n  summarise(trips = sum(total_num_trips, na.rm = F), .groups = \"drop\") |&gt; \n  arrange(departure_station) |&gt;\n  mutate(month = lubridate::month(date, label = T, abbr = T),\n         row = row_number()) |&gt; \n  ggplot(aes(x = month, y = reorder(departure_station, -row), fill = trips)) +\n  geom_tile() +\n  scale_fill_viridis(labels = c(\"\",\"Lowest\",\"\",\"\",\"\",\"\",\"Highest\",\"\"), n.breaks = 7, trans = c(\"log\")) +\n  scale_x_discrete(breaks = c(\"Jan\",\"Apr\",\"Jul\",\"Oct\")) +\n  facet_wrap(~year, nrow = 1, scales = \"free_x\") +\n  labs(y = \"Departure Station\", \n       x = NULL,\n       fill = \"Trips\",\n       title = \"Trips by Month\") +\n  theme_blog_facet(axis.text.x = element_text(size = 9, angle = 90, hjust = .5),\n                   axis.text.y = element_text(size = 9),\n                   panel.grid = element_blank(),\n                   axis.ticks = element_blank(),\n                   legend.text = element_text(size = 10),\n                   legend.position=\"bottom\",\n                   legend.justification=\"center\",\n                   legend.direction=\"horizontal\",\n                   legend.key.width= unit(1, 'cm'))\nlate &lt;- trains |&gt; \n  group_by(departure_station, year, date) |&gt; \n  summarise(late_arrivals = sum(num_arriving_late, na.rm = F),\n            trips = sum(total_num_trips, na.rm = F)) |&gt; \n  ungroup() |&gt; \n  mutate(pct_late_arrivals = late_arrivals / trips, \n         month = lubridate::month(date, label = T, abbr = T),\n         row = row_number()) |&gt; \n  ggplot(aes(x = month, y = reorder(departure_station, -row), fill = pct_late_arrivals)) +\n  geom_tile() +\n  scale_x_discrete(breaks = c(\"Jan\",\"Apr\",\"Jul\",\"Oct\")) +\n  scale_fill_viridis(option = \"magma\", labels = scales::percent_format()) +\n  facet_wrap(~year, nrow = 1, scales = \"free_x\") +\n  labs(y = NULL, \n       x = NULL, \n       fill = NULL,\n       title = \"Late Arrivals by Month\") +\n  theme_blog_facet(axis.text.x = element_text(size = 9, angle = 90, hjust = .5),\n                   axis.text.y = element_blank(),\n                   panel.grid = element_blank(),\n                   legend.text = element_text(size = 10),\n                   legend.position=\"bottom\",\n                   legend.justification=\"center\",\n                   legend.direction=\"horizontal\",\n                   legend.key.width= unit(1, 'cm'))\nservice + late\n\n\n\n\n\n\nNetworking\nNo fear, I’m not talking about a cringy coffee date with a complete stranger, I’m talking about math networks (i.e. graphs). Viewing our data in a network will provide a lot of context we can’t get from other plotting methods. The tidygraph package makes it fairly easy to work with network graphs and the ggraph package provides ggplot2 with graphing grammar of graphics (wow that sounds confusing :joy:). These packages (and network analysis in general) are incredibly robust so there is a bit of a learning curve, I’ve noted some other materials I’ve found helpful getting started. 1\n\n\n\nThe short of it, tidygraph allows us to perform network analysis in data frames so we still take advantage of our the tidy workflow even though graph objects aren’t rectangular in nature. To create a graph object, we have to create a nodes list (points in the network) and an edges list (connections between nodes). We can store our nodes list as a data frame with a record for each node in the network, in this case train stations, and any attributes related to the nodes. We can store our edges list as a data frame with a record for each connection in the network, in this case legs, and any attributes related to the edges. All graph objects are either directed or un-directed - this graph is un-directed because the trains go in both directions so the edges do not represent a direction. Once we have a node list and edge list, we can transform the data frames into a tbl_graph.\n\n\nCode\nroutes_df &lt;- trains |&gt; \n  mutate(total_journies_time = total_num_trips * journey_time_avg) |&gt; \n  group_by(departure_station, arrival_station, leg, service) |&gt; \n  summarise(\n    journey_time_weighted_avg = sum(total_journies_time, na.rm = T) / sum(total_num_trips, na.rm = T),\n    trips = sum(total_num_trips, na.rm = T),\n    late_departures = sum(num_late_at_departure, na.rm = T),\n    late_arrivals = sum(num_arriving_late, na.rm = T),\n    late_trips = sum(num_late_at_departure, na.rm = T) + sum(num_arriving_late, na.rm = T)\n  ) |&gt;\n  rename(\"from\" = \"departure_station\", \"to\" = \"arrival_station\")  |&gt;\n  ungroup() \ndepartures_df &lt;- trains |&gt;\n  group_by(departure_station) |&gt;\n  summarise(\n    departures = sum(total_num_trips, na.rm = T),\n    late_departures = sum(num_late_at_departure, na.rm = T),\n    pct_late_departures = sum(num_late_at_departure, na.rm = T) / sum(total_num_trips, na.rm = T),\n    .groups = \"drop\") |&gt;\n  rename(\"station\"=\"departure_station\") |&gt; \n  mutate(id = row_number(), .before = 1)\narrivals_df &lt;- trains |&gt;\n  group_by(arrival_station) |&gt;\n  summarise(\n    arrivals = sum(total_num_trips, na.rm = T),\n    late_arrivals = sum(num_arriving_late, na.rm = T),\n    pct_late_arrivals = sum(num_arriving_late, na.rm = T) / sum(total_num_trips, na.rm = T),\n    .groups = \"drop\") |&gt;\n  rename(\"station\"=\"arrival_station\") \nstations_df &lt;- inner_join(departures_df, arrivals_df, by = c(\"station\" = \"station\"))\nroutes_df &lt;- routes_df |&gt; \n  left_join(stations_df |&gt; select(id, station), by = c(\"from\" = \"station\")) |&gt; \n  mutate(from = id) |&gt; \n  select(-id) |&gt; \n  left_join(stations_df |&gt; select(id, station), by = c(\"to\" = \"station\")) |&gt; \n  mutate(to = id) |&gt; \n  select(-id)\nnetwork_df &lt;- tbl_graph(nodes = stations_df, edges = routes_df, directed = F)\n\n\n\n\n\nWith the tbl_graph, we can take advantage of ggraph to visualize our data. There is an autograph() function to quickly visualize your network automagically, but I’ll spell it out as well with ggraph(). Layout determines the position of the nodes and edges (more on that in a minute), we can start with layout='auto'. Then we use the typical ggplot flow but there are new geoms to pay attention to, primarily geom_node_point() and geom_edge_link(), these are very basic geoms to visualize the nodes and edges of the graph. Visualizing the data as a network has immediate benefits, this visual shows there are several major hubs that most legs connect through which wasn’t clearly discernible from our other plots.\n\n\nCode\nautograph(network_df) \n\n\n\n\n\nCode\nauto &lt;- autograph(network_df) + labs(x = NULL, y = NULL, title = \"Autograph\") + theme_blog_network()\nverbose &lt;- network_df %N&gt;%\n  ggraph(layout = 'auto') + \n  geom_edge_link(width = .15) + \n  geom_node_point(aes(linewidth = arrivals + departures), show.legend = F)  +\n  scale_size(rang = c(1, 15)) +\n  labs(title = \"Verbose\", x = \"\", y = \"\", size = NULL) +\n  theme_blog_network()\nauto + verbose\n\n\n\n\n\nRegarding layout, ggraph provides a number of layouts from igraph 2 and graphlayouts 3, with option to create your own. The ggraph function defaults to layout='auto', which seems to do a pretty good job selecting a layout in my novice experience, but it’s recommended that a layout is intentionally chosen for the graph. All the layout choices can be overwhelming, but it’s pretty easy to visually narrow down choices and then do a bit of research to understand the differences between those options. 4 Many of the algorithms are non-deterministic meaning you’ll get different results when re-running, which makes selecting the best layout all the more challenging. I saw a cool example by Thomas Lin Pederson where he showed the different layouts in a neat animation using tweenr. I tried that here, more for fun, to see the different layouts and mess with tweenr. I decided to go with layout='stress' because it seemed to produce the least overlap for both nodes and edges making it the easiest to read - funnily enough the same layout layout='auto' chose :joy:\n\n\nCode\nlayout_names &lt;- c('star', 'circle', 'gem', 'dh', 'graphopt', 'grid', 'mds', 'randomly', 'fr', 'kk', 'drl', 'lgl', 'stress')\nlayouts &lt;- map(layout_names, function(x){\n  layout &lt;- create_layout(x, graph = network_df) |&gt; select(x, y, circular) |&gt; mutate(layout_name = !!x)\n})\nn &lt;- 5\nn_frames &lt;- length(layout_names) * n\nlayouts_tween &lt;- tween_states(layouts, tweenlength = 1, \n                              statelength = 1, ease = 'cubic-in-out', \n                              nframes = n_frames)\ntitle_transp &lt;- tween_t(c(0, 1, 0), n, 'cubic-in-out')[[1]]\nplots &lt;- map(unique(layouts_tween$.frame), function(i){\n  tmp_layout &lt;- layouts_tween[layouts_tween$.frame == i, ]\n  layout_name &lt;- tmp_layout$layout_name[1]\n  title_alpha &lt;- title_transp[i %% n + 1]\n  p &lt;- ggraph(network_df, layout = tmp_layout) +\n    geom_edge_link(width = .15) +\n    geom_node_point(aes(size = arrivals + departures), show.legend = F)  +\n    scale_size(rang = c(1, 15)) +\n    labs(title = paste0('Layout: ', layout_name), x = NULL, y = NULL) +\n    theme_blog_network() +\n    theme(plot.title = element_text(colour = alpha('black', title_alpha)))\n  return(p)\n})\nwalk(plots, print)\n\n\n\n\n\nWe can use many (not all) tidyverse functions with a tbl_graph by activating the nodes or edges and using the verb of choice (e.g. mutate(), filter(), morph() etc.). The activate() function can be called directly or as pipeline operators %N&gt;% and %E&gt;%, which active nodes and edge respectively. I ran into some challenges in this post that I won’t spell out for brevity, but know functions like pivot_longer() and unnest() aren’t available with tbl_graph objects. I ended up writing a lot of clumsy code with pull(). There may be better approaches, but I wasn’t finding much on Stackoverflow or Github. I’ll post some questions later and link them in here if I can get some answers.\n\n\n\n\nWith dplyr verbs, we can calculate attributes on our nodes & edges to add to our visuals.\n\n\nCode\nl &lt;- network_df %N&gt;%\n  mutate(outlier = abs(scale(late_arrivals + late_departures) / scale(arrivals + departures)) &gt;= 1.5) |&gt; \n  mutate(node_label = ifelse(outlier, glue::glue(\"{station}:\\n{scales::label_percent(accuracy = .1)((late_arrivals + late_departures) / (arrivals + departures))}\"), NA_character_)) |&gt; \n  ggraph(layout = 'stress') + \n  geom_edge_link(width = .15) + \n  geom_node_point(aes(size = arrivals + departures, color = (late_arrivals + late_departures) / (arrivals + departures)))  +\n  geom_node_label(aes(label = node_label), alpha = .75, repel = T) +\n  scale_size(rang = c(1, 15), guide = NULL) +\n  scale_color_distiller(labels = scales::label_percent(), palette = \"Reds\", direction = 1) +\n  labs(title = \"Stations w/ outlier lateness\", subtitle = \"Labeled stations had total late arrivals & departures 2 standard deviations outside the mean\", x = \"\", y = \"\", color = \"Arrived / Departed Late\", size = NULL) +\n  theme_blog_network(legend.position = \"bottom\")\nduration_outliers &lt;- network_df %E&gt;%\n  mutate(journey_time_z = scale(journey_time_weighted_avg)) |&gt; \n  mutate(outlier = abs(scale(journey_time_weighted_avg)) &gt;= 2) |&gt; \n  filter(outlier == T)\nduration_outliers &lt;- unique(c(duration_outliers |&gt; pull(from), duration_outliers |&gt; pull(to)))\nd &lt;- network_df %E&gt;%\n  mutate(outlier = abs(scale(journey_time_weighted_avg)) &gt;= 2) %N&gt;%\n  mutate(node_label = if_else(id %in% duration_outliers, station, NA_character_)) |&gt; \n  ggraph(layout = 'stress') + \n  geom_edge_link(aes(edge_width = journey_time_weighted_avg, color = outlier), alpha = .5) + \n  scale_edge_width(range = c(.15, 3)) +\n  scale_edge_color_manual(values = c(\"TRUE\" = \"red\", \"FALSE\" = \"black\"), guide = NULL) +\n  geom_node_point(aes(size = arrivals + departures))  +\n  geom_node_label(aes(label = node_label), alpha = .7, repel = T) +\n  scale_size(rang = c(1, 15), guide = NULL) +\n  labs(title = \"Stations w/ routes w/ outlier duration\", subtitle = \"Labeled stations had destinations with weighted average journey time 2 standard deviations outside the mean\", x = \"\", y = \"\", edge_width = \"Journey Time (mins)\") +\n  theme_blog_network(legend.position = \"bottom\")\nl / d\n\n\n\n\n\nAside from the nice workflow, tidygraph also gives us access to a lot of new functionality and functionality from other network packages (other packages that are tougher to use with data frames imho) including centrality_* functions. There are a number of ways to measure centrality including betweenness, closeness, eigen, etc. Our network has pretty big hubs (i.e. Paris & Lyon Part Dieu) so the central nodes are pretty obvious, but often determining the central nodes is more difficult. Below we used betweenness to measure centrality, which measures the centrality of a node by how many times it is used to create the shortest path to other nodes in the network.\n\n\nCode\ncb &lt;- network_df %N&gt;%\n  mutate(cb = centrality_betweenness())\nmean_cb &lt;- mean(cb |&gt; pull(cb))\nsd_cb &lt;- sd(cb |&gt; pull(cb))\ncb %N&gt;%  \n  mutate(node_label = if_else(cb &gt;= mean_cb + sd_cb*2 , station, NA_character_)) |&gt; \n  ggraph(layout = 'stress') +\n  geom_edge_link(width = .15) +\n  geom_node_point(aes(size = arrivals + departures, color = cb)) +\n  geom_node_label(aes(label = node_label), alpha = .75, repel = T) +\n  scale_size(range = c(1, 15), guide = NULL) +\n  scale_colour_steps(labels = NULL, low = \"#d9d9d9\", high = \"#000000\") + \n  labs(title = \"Centrality Betweenness\", x = \"\", y = \"\", color = \"Centrality\", size = NULL) +\n  theme_blog_network(legend.position = \"bottom\")\n\n\n\n\n\nI read a post by David Schoch that inspired me to compare different centrality measures. It seems like a good exploratory way to understand the network and to understand the best centrality measure(s) to use. The eigen measure was particularly interesting for this network, it looks for nodes that benefit from the connections of their nearest neighbors.\n\n\nCode\ncentrality_df &lt;- network_df %N&gt;% \n  mutate(\n    cd = tidygraph::centrality_degree(),\n    cb = centrality_betweenness(),\n    cc = centrality_closeness(),\n    ce = centrality_eigen(),\n    cs = centrality_subgraph()\n  ) \ncentrality_df &lt;- tibble(\n  id = centrality_df  |&gt; pull(id),\n  station = centrality_df  |&gt; pull(station),\n  degree = centrality_df  |&gt; pull(cd),\n  betweenness = centrality_df  |&gt; pull(cb),\n  closeness = centrality_df  |&gt; pull(cc),\n  eigen = centrality_df  |&gt; pull(ce),\n  subgraph = centrality_df  |&gt; pull(cs)\n) \nis_top_n &lt;- function(x, n){\n  top_n &lt;- tail(sort(x), n)\n  x &gt;= min(top_n)\n}\ncentrality_df |&gt; \n  mutate(across(.cols = c(degree, betweenness, closeness, eigen, subgraph), .fns = is_top_n, n = 3, .names = \"{.col}_top_5\")) |&gt; \n  mutate(across(.cols = c(degree, betweenness, closeness, eigen, subgraph), .fns = ~abs(log10(.x)))) |&gt; \n  filter(if_any(.cols = contains(\"_top_\"), ~ . == T  )) |&gt; \n  select(-contains(\"_top_\")) |&gt; \n  pivot_longer(cols = c(degree, betweenness, closeness, eigen, subgraph), names_to = \"measure\", values_to = \"value\") |&gt;\n  mutate(x_order = case_when(\n    measure == \"betweenness\" ~ 1,\n    measure == \"closeness\" ~ 2,\n    measure == \"degree\" ~ 3,\n    measure == \"eigen\" ~ 4,\n    measure == \"subgraph\" ~ 5\n  )) |&gt; \n  ggplot() +\n  geom_col(aes(x = reorder(measure, -x_order), y = value, fill = station), position = \"dodge\") +\n  scale_fill_viridis_d() +\n  labs(title = \"Centrality Measures\", subtitle = \"Comparing centrality measures for the stations in the top 5 (including ties)\", fill = \"Station\", y = NULL, x = \"Measure\") +\n  theme_blog() +\n  theme(axis.text.y = element_blank())\n\n\n\n\n\nWe can also use the group_* functions to perform community detection, for example group_edge_betweenness().\n\n\nCode\ngroup_edge_betweenness_plots &lt;- function(tbl_graph, i){\n  # This feels very untidy, but names() does not work on tbl_graph objects and I can't find any way to use certain tidy\n  # methods (e.g. pivot_longer, map_dfc, etc.)\n  # pull() is the only programmatic way I can figure out to extract things from my df's once they're in the tbl_graph\n  cb_grps_df &lt;- tbl_graph %N&gt;%\n    mutate(grp_cols = list(paste0('grps_', as.character(i))))\n  for(n in i){\n    cb_grps_df &lt;- cb_grps_df %N&gt;%\n      mutate(\"{paste0('grps_', as.character(n))}\" := forcats::as_factor(group_edge_betweenness(n_groups = n)))\n  }\n  plots &lt;- purrr::map(i, function(n) {\n    net_df &lt;- cb_grps_df %N&gt;% \n      tidygraph::select(id:pct_late_arrivals, 9+n-1) |&gt; \n      rename(\"group\" =  9)\n    net_df |&gt;\n      ggraph(layout = \"stress\") +\n      geom_edge_link0(width = 0.15) +\n      geom_node_point(aes(size = arrivals + departures, fill = group), show.legend = F) +\n      geom_mark_hull(\n        aes(x, y, group = group, fill =  group, label = group),\n        concavity = 4,\n        expand = unit(2, \"mm\"),\n        alpha = 0.25\n      ) +\n      scale_fill_brewer(palette = \"Set1\") +\n      labs(title = glue::glue(\"Groups: {n}\"), x = NULL, y = NULL, fill = \"Group\") +\n      theme_blog_network()\n  })\n}\n\nnetwork_df |&gt; \n  group_edge_betweenness_plots(i = 2:6) |&gt; \n  walk(print) \n\n\n\n\n\n\n\nGeospatial\nAfter plotting our network and seeing Stuttgart on the left and Barcelona on the right, I really wanted to source coordinates and map the train network. The dataset didn’t come with coordinates, but I thought maybe I could source them - this was a total can of worms called Wikidata.\n\n\n\n\n\nWikidata\nIn trying to source coordinates for the 59 train stations I stumbled across wikifacts, an R package to query Wikidata. Wikidata is a structured data source for Wikipedia, Wikivoyage, Wiktionary, Wikisource, and the many other wiki projects. In other words, it’s a data base you can query to get info from Wikipedia. Who knew! I’m going to cover the basics of wikifacts in a separate post for posterity, but I used it to get coordinates for the stations so we could map the data.\nIt wasn’t quite as simple as a query and a join with the data because the names of the stations weren’t quite right in this data set (e.g. station Italie is not a station name). I had to do a little bit of Googling and make a few assumptions, but I think I got the wikidata item ID’s for all the stations. It’s a bit of a round about way to get the data, but it works.\n\n\nCode\nwikidata_ids &lt;- tibble::tribble(\n  ~station, ~wikidata, \n  \"Aix En Provence Tgv\", \"Q800366\", \n  \"Angers Saint Laud\", \"Q952091\", \n  \"Angouleme\", \"Q3095646\", \n  \"Annecy\", \"Q2043382\", \n  \"Arras\", \"Q1990647\", \n  \"Avignon Tgv\", \"Q174845\",\n  \"Barcelona\", \"Q800453\",\n  \"Bellegarde (Ain)\", \"Q3095950\", \n  \"Besancon Franche Comte Tgv\", \"Q599069\", \n  \"Bordeaux St Jean\", \"Q800564\", \n  \"Brest\", \"Q2255490\", \n  \"Chambery Challes Les Eaux\", \"Q3096183\", \n  \"Dijon Ville\", \"Q3096374\", \n  \"Douai\", \"Q2228617\", \n  \"Dunkerque\", \"Q2474563\",\n  \"Francfort\", \"Q165368\",\n  \"Geneve\", \"Q800819\",\n  \"Grenoble\", \"Q2545346\",\n  \"Italie\", \"Q801193\",\n  \"La Rochelle Ville\", \"Q2762145\",\n  \"Lausanne\", \"Q669678\",\n  \"Laval\", \"Q3096443\",\n  \"Le Creusot Montceau Montchanin\", \"Q801071\",\n  \"Le Mans\", \"Q2718010\",\n  \"Lille\", \"Q801099\",\n  \"Lyon Part Dieu\", \"Q2008\",\n  \"Macon Loche\", \"Q2253062\",\n  \"Madrid\", \"Q23081\",\n  \"Marne La Vallee\", \"Q801167\",\n  \"Marseille St Charles\", \"Q371217\",\n  \"Metz\", \"Q801179\",\n  \"Montpellier\", \"Q801197\",\n  \"Mulhouse Ville\", \"Q801205\",\n  \"Nancy\", \"Q2178663\",\n  \"Nantes\", \"Q2180175\",\n  \"Nice Ville\", \"Q738970\",\n  \"Nimes\", \"Q2360563\",\n  \"Paris Est\", \"Q757180\",\n  \"Paris Lyon\", \"Q747541\",\n  \"Paris Montparnasse\", \"Q631114\",\n  \"Paris Nord\", \"Q745942\",\n  \"Paris Vaugirard\", \"Q631114\",\n  \"Perpignan\", \"Q2283118\",\n  \"Poitiers\", \"Q2786522\",\n  \"Quimper\", \"Q2191696\",\n  \"Reims\", \"Q801356\",\n  \"Rennes\", \"Q1866058\",\n  \"Saint Etienne Chateaucreux\", \"Q2486002\",\n  \"St Malo\", \"Q2623224\",\n  \"St Pierre Des Corps\", \"Q3097610\",\n  \"Strasbourg\", \"Q801473\",\n  \"Stuttgart\", \"Q613766\",\n  \"Toulon\", \"Q2683553\",\n  \"Toulouse Matabiau\", \"Q373449\",\n  \"Tourcoing\", \"Q2598042\",\n  \"Tours\", \"Q1876295\",\n  \"Valence Alixan Tgv\", \"Q2052261\",\n  \"Vannes\", \"Q1867341\",\n  \"Zurich\", \"Q224494\"\n)\nall_tgv_stations_wikidata &lt;- '\n  SELECT ?name ?nameLabel ?nameDescription ?coords\n  WHERE\n  {\n    {?name wdt:P1192 wd:Q129337}\n    UNION\n    {?name wdt:P1192 wd:Q675756}\n    UNION\n    {?name wdt:P1192 wd:Q847141}\n    UNION\n    {?name wdt:P127 wd:Q358752}\n    OPTIONAL {?name wdt:P625 ?coords.}\n    SERVICE wikibase:label { bd:serviceParam wikibase:language \"en\". }\n  }\n  ' |&gt; \n  wiki_query() |&gt; \n  mutate(wikidata = str_split(name, \"entity/\")) |&gt; \n  mutate(wikidata = map_chr(wikidata, 2)) |&gt; \n  distinct()\n\n\n\n\nsf\nNot the city, the R package sf. It stands for simple features, a set of standards that specify a common storage and access model of geographic feature made of mostly two-dimensional geometries. [^5] The sf package is the tidygraph of geospatial analytics in R (or vice versa, I think sf might be older).\n\n\n\nWe can work with many geometries sf. Most of my experience has been with points and polygons, but given all the time we just spent working tidygraph, we should convert the edges to linestrings. This was my first time working with linestrings and I had a little trouble, but this great answer from lovalery got me through it.\n\n\nCode\nextract_lonlat &lt;- function(point, lon_or_lat = c(\"lon\", \"lat)\")){\n  args &lt;- c(\"lon\", \"lat\")\n  lon_or_lat &lt;- match.arg(lon_or_lat, choices = args)\n  index &lt;- which(args == lon_or_lat, arr.ind = F)\n  x &lt;- str_split_fixed(str_remove(str_remove(point, pattern = \"Point\\\\(\"), pattern = \"\\\\)\"), n = 2, \" \")[index]\n  return(x)\n}\nstations_sf &lt;- stations_df |&gt;  \n  left_join(wikidata_ids, by = c(\"station\" = \"station\")) |&gt; \n  left_join(all_tgv_stations_wikidata, by = c(\"wikidata\" = \"wikidata\")) |&gt;\n  rowwise() |&gt; \n  mutate(\n    lon = as.numeric(extract_lonlat(coords, \"lon\")),\n    lat = as.numeric(extract_lonlat(coords, \"lat\"))\n  ) |&gt; \n  ungroup() |&gt; \n  st_as_sf(wkt = \"coords\", remove = T) |&gt;\n  st_set_crs(4326)\nroutes_sf &lt;- map2_dfr(routes_df$from, routes_df$to, function(from, to) {\n  from_stations_sf &lt;- stations_sf |&gt; filter(id == from)\n  to_stations_sf &lt;- stations_sf |&gt; filter(id == to)\n  from_coords &lt;- stations_sf |&gt; filter(id == from) |&gt; select(coords)\n  to_coords &lt;- stations_sf |&gt; filter(id == to) |&gt; select(coords)\n  geometry &lt;- st_cast(st_union(from_coords,to_coords), \"LINESTRING\")\n  df &lt;- tibble(\n    from = from,\n    to = to,\n    from_lon = from_stations_sf$lon,\n    from_lat = from_stations_sf$lat,\n    to_lon = to_stations_sf$lon,\n    to_lat = to_stations_sf$lat,\n    geometry = geometry$coords\n  )\n  return(df)\n}) |&gt; \n  st_as_sf() |&gt; \n  st_set_crs(4326)\n\n\nCreating a map with sf and leaflet is so easy, it’s amazing! We can pass the points and the linestrings or just the points since the routes get really crowded unless we filter to one station maybe… This could be a good idea to build an accompanying table to filter the leaflet nodes to specific stations and all their connections. I’m going to skip that idea for now because I’ve done that before in with plotly and crosstalk in my Great American Beer Festival post, but I might do it in a separate post later. Note, I had a small hiccup getting the map background to load on my computer that was solved with this answer from Wilcar.\n\n\nCode\nleaflet() |&gt; \n  addTiles(attribution = '&copy; &lt;a href=\"http://www.openstreetmap.org/copyright\"&gt;OpenStreetMap&lt;/a&gt;') |&gt; \n  addMarkers(data = stations_sf) |&gt;  \n  addPolylines(data = routes_sf)\n\n\n\n\n\n\nMy first thought to look at the linestrings was to use geom_sf to make an arc diagram - I weirdly couldn’t figure out an easy way to do it with sf at first so I just used based ggplot2 and the old fashioned latitude / longitude approach with help from a post by Markus Konrad. This was actually a lot easier than I thought it’d be.\n\n\nCode\ngeom_map &lt;- borders(\"world\", ylim = c(38, 52), xlim = c(-4.5, 18), size = .04,\n                    fill = \"white\")\nplots &lt;- map(stations_sf$id, function(x){\n  station &lt;- stations_sf |&gt; filter(id == x) |&gt; pull(station)\n  e &lt;- routes_sf |&gt; \n    filter(from == x)\n  n &lt;- stations_sf |&gt; \n    filter(id %in% e$from | id %in% e$to)\n  ggplot(n) + \n    geom_map +\n    geom_point(aes(x = lon, y = lat),\n               shape = 21, alpha = .5, stroke = 0.5, fill = \"black\", color = \"black\") +\n    geom_text_repel(aes(x = lon, y = lat, label = station),\n              size = 3, color = \"black\", fontface = \"bold\") +\n    geom_curve(data = e, aes(x = from_lon, y = from_lat, xend = to_lon, yend = to_lat), \n               curvature = 0.33,\n               alpha = 0.5,\n               color = \"black\") +\n    #coord_cartesian(ylim = c(38, 52), xlim = c(-4.5, 18)) +\n    labs(title = station, x = NULL, y = NULL) +\n    theme_void() +\n    theme(panel.background=element_rect(fill=\"#d7e9f5\", colour=\"#d7e9f5\"))\n})\nwalk(plots, print) \n\n\n\n\n\nI finally got the map above using sf by leveraging another package, sfnetworks, which combines sf and tidygraphs. This package is the perfect way to tie our two different threads of network and spatial analysis together.\n\n\nCode\nnetwork_sf &lt;- sfnetwork(stations_sf, routes_df, node_key = \"id\", directed = F)\nnetwork_sf &lt;- tidygraph::convert(\n  network_sf, \n  to_spatial_explicit, \n  .clean = TRUE\n)\nggplot() +\n  geom_map +\n  geom_sf(data = st_as_sf(network_sf, \"edges\"), color = \"black\") +\n  geom_sf(data = st_as_sf(network_sf, \"nodes\"), color = \"black\") +\n  theme_void() +\n  theme(panel.background=element_rect(fill=\"#d7e9f5\", colour=\"#d7e9f5\"))\n\n\n\n\n\nIt also provides some really handy functionality like measuring edge length (i.e. distance) because it has both the graph data and the sf data. We can use that for all sorts of analysis including calculating the average speed of the routes (since we already have weighted average trip time), very cool!\n\n\nCode\nnetwork_sf &lt;- network_sf %E&gt;%\n  mutate(distance = edge_length()) |&gt; \n  mutate(distance_km = as.numeric(sub(\"\\\\s+\\\\D+$\", \"\", measurements::conv_unit(distance, from = \"m\", to = \"km\")))) |&gt; \n  mutate(avg_speed_kph = distance_km / (journey_time_weighted_avg / 60))\nggplot() +\n  geom_map +\n  geom_sf(data = st_as_sf(network_sf, \"edges\"), aes(color = avg_speed_kph)) +\n  geom_sf(data = st_as_sf(network_sf, \"nodes\"), color = \"black\") +\n  labs(title = \"Route Speeds (kph)\", color = \"Avg kph\") +\n  theme_void() +\n  theme(panel.background=element_rect(fill=\"#d7e9f5\", colour=\"#d7e9f5\"),\n        legend.position = \"bottom\")\n\n\n\n\n\nFinally, heavily borrowing from the package vignette, sfnetworks can use tidygraphs morphers to do really neat things like community detection in edges. Here we morph the edges into the nodes place so that we can use Louvain community detection, which can only be done on nodes, and then assign those groups to the edges so we can visualize them.\n\n\nCode\ngrouped_net = network_sf %N&gt;%\n  morph(to_linegraph) |&gt;\n  mutate(group = group_louvain()) |&gt;\n  unmorph()\ngrouped_net &lt;- grouped_net |&gt; \n  activate(\"edges\") %E&gt;%\n  st_as_sf() |&gt;\n  transmute(group = as.factor(group))\nggplot() +\n  geom_map +\n  geom_sf(data = st_as_sf(grouped_net, \"edges\"), aes(color = group)) +\n  geom_sf(data = st_as_sf(network_sf, \"nodes\"), color = \"black\") +\n  labs(title = \"Louvain method for community detection\", color = \"Groups\") +\n  theme_void() +\n  theme(panel.background=element_rect(fill=\"#d7e9f5\", colour=\"#d7e9f5\"),\n        legend.position = \"bottom\")"
  },
  {
    "objectID": "posts/2021-05-04-french-trains/index.html#wrap-up",
    "href": "posts/2021-05-04-french-trains/index.html#wrap-up",
    "title": "French Train Delays",
    "section": "Wrap Up",
    "text": "Wrap Up\nI really wanted to get more experience with some of the network analysis packages in R - mission accomplished. This exercise morphed into a little bit more of a package demo than pure analysis, but I feel like I learned a lot of new tools / techniques that I’ll be able to leverage in the future. There’s still a lot more to learn with tidygraph, sf, and sfnetworks, but I feel like this gave a solid foundation. Hopefully others find it useful as well!"
  },
  {
    "objectID": "posts/2021-05-04-french-trains/index.html#footnotes",
    "href": "posts/2021-05-04-french-trains/index.html#footnotes",
    "title": "French Train Delays",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNetwork analysis is huge area in analytics. Below are a few resources I’ve found helpful: Introducing tidygraph, tidygraph 1.1, A Tidy Hope, 1 giraffe, 2 giraffe, GO!, Network Analysis with R by Jesse Sadler, tidygraph & ggraph by Michael Taylor, Network analysis & visualization with R & igraph↩︎\nThe igraph Manual, Ch. 20 Generating layouts for graph drawing, provides detailed explanations, parameters, etc. for all the igraph layouts. The ggraph layout names match the igraph::layout_with_* function names.↩︎\nThe graphlayouts Package provides provides detailed explanations, parameters, etc. for all the graphlayout functions.↩︎\nIntroduction to layouts by Thomas Lin Pederson was a simple primer on layouts, Metwork Analysis: Layout of Nodes by Robert Wiederstein covered a bit more using igraph, and Stress based graph layouts by David Schoch provided additional insights. [5^]: Simple features Wikipedia page↩︎"
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html",
    "href": "posts/2022-01-05-building-blogdown-site/index.html",
    "title": "Building a blogdown site",
    "section": "",
    "text": "I’ve created a few different versions of my blog using blogdown. Creating new blog with blogdown is so simple, but customizing can be finicky. I started a guide when I upgrade my site to blogdown 1.0, but was still sitting in a stack of posts that never made their way to the web when I started writing this :flushed: Now a year later I’m hoping to use blogdown internally at my new job to disseminate analysis more easily and I could really use that post as a guide :scream: So, for posterity I’m going to document the process as I go through it. If you’d like to learn how to create blogdown site or learn how to customize on your existing blogdown site, read on.\n\n\n\nblogdown is an R package witten by Yihui Xie that wraps the Hugo web development framework. Blogdown provides a tool for R developers to build their own blogs and portfolio sites from Hugo templates. There are several other R libraries to consider for publishing including distill, bookdown, and hugodown. We’ll cover blogdown only in this post.\n\n\nIf you’re new to blogdown I recommend this article from Alison Presmanes Hill Hill. Alison spent a few years working on the RStudio team and her blog is loaded with blogdown walk-throughs and gotcha’s. I highly recommend looking through her blog for other content. Additionally, Yihui wrote a blogdown book that I always keep handy. Finally, I occasionally look through Yihui’s stack overflow answers just to find tidbits of info about how rmarkdown, knitr, blogdown, etc. really work because they are so complicated and black-box like… It’s not specifically a resource for blogdown, but blogdown relies on the knitr and rmarkdown infrastructure so it helps to understand those packages behind the package. Aside from that, blogdown relies on rmarkdown. If you’ve not used rmarkdown before I recommend at least going through a bit of this guide from RStudio.\n\n\n\n\n\nHugo is a web development framework used to build static websites. It’s is one of the most popular static site generators available at this time. If you’re looking to use blogdown, you do not necessarily need to know almost anything about Hugo to succeed. However, the more you understand Hugo the easier it becomes to customize your site, debug, etc.\n\n\n\nI’ll cover some of the basics in the next section, but I’ll barely scratch the surface on Hugo. If you want to learn more, the Hugo documentation and a youtube playlist Hugo provides are both good resources. I also found this tutorial by Adi Purdila useful. I haven’t gone through and built my own Hugo site outside blogdown, but just watching the tutorial helped me understand how Hugo works for customization. This is probably a great starting point if you want to learn how to build your own template, but I’m not going there just yet…"
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#what-is-blogdown",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#what-is-blogdown",
    "title": "Building a blogdown site",
    "section": "",
    "text": "blogdown is an R package witten by Yihui Xie that wraps the Hugo web development framework. Blogdown provides a tool for R developers to build their own blogs and portfolio sites from Hugo templates. There are several other R libraries to consider for publishing including distill, bookdown, and hugodown. We’ll cover blogdown only in this post.\n\n\nIf you’re new to blogdown I recommend this article from Alison Presmanes Hill Hill. Alison spent a few years working on the RStudio team and her blog is loaded with blogdown walk-throughs and gotcha’s. I highly recommend looking through her blog for other content. Additionally, Yihui wrote a blogdown book that I always keep handy. Finally, I occasionally look through Yihui’s stack overflow answers just to find tidbits of info about how rmarkdown, knitr, blogdown, etc. really work because they are so complicated and black-box like… It’s not specifically a resource for blogdown, but blogdown relies on the knitr and rmarkdown infrastructure so it helps to understand those packages behind the package. Aside from that, blogdown relies on rmarkdown. If you’ve not used rmarkdown before I recommend at least going through a bit of this guide from RStudio."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#what-is-hugo",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#what-is-hugo",
    "title": "Building a blogdown site",
    "section": "",
    "text": "Hugo is a web development framework used to build static websites. It’s is one of the most popular static site generators available at this time. If you’re looking to use blogdown, you do not necessarily need to know almost anything about Hugo to succeed. However, the more you understand Hugo the easier it becomes to customize your site, debug, etc.\n\n\n\nI’ll cover some of the basics in the next section, but I’ll barely scratch the surface on Hugo. If you want to learn more, the Hugo documentation and a youtube playlist Hugo provides are both good resources. I also found this tutorial by Adi Purdila useful. I haven’t gone through and built my own Hugo site outside blogdown, but just watching the tutorial helped me understand how Hugo works for customization. This is probably a great starting point if you want to learn how to build your own template, but I’m not going there just yet…"
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#html-hypertext-markup-language",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#html-hypertext-markup-language",
    "title": "Building a blogdown site",
    "section": "HTML (HyperText Markup Language)",
    "text": "HTML (HyperText Markup Language)\nHTML is the primary building block of web pages. There’s a ton to learn about HTML. I recommend w3school’s HTML tutorial if you want to get a broad understanding. The two things I’d definitely recommend reviewing are HTML elements (aka HTML tags) and HTML attributes. I always keep these lists of HTML elements and HTML attributes handy."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#css-cascading-style-sheets",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#css-cascading-style-sheets",
    "title": "Building a blogdown site",
    "section": "CSS (Cascading Style Sheets)",
    "text": "CSS (Cascading Style Sheets)\nCSS is code used to describe how html elements are to be displayed on a webpage, CSS “styles” a webpage. I borrowed the below image from a css-tricks post, but the image on the right shows what Wikipedia’s homepage would look like without CSS. \n\n\n\nAgain, I recommend w3school’s CSS tutorial to get a broad understanding of CSS. If you want to do something to style your site search it and don’t be afraid to post on stackoverflow. I find a lot of really specific CSS implementations on there that I’ve tried to jam into my personal site over time. Time more you do it the easier it gets. What you need to know in CSS can be especially specific to your goal, but I’d recommend reviewing CSS syntax and CSS selectors regardless because they’re very useful and fundamental."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#browser-developer-tools",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#browser-developer-tools",
    "title": "Building a blogdown site",
    "section": "Browser Developer Tools",
    "text": "Browser Developer Tools\nThe browser web developer tools that come in your standard issue web browser are critical to working on web development. The most commonly used tool within the set is the element inspector which usually is represented by a little square with an arrow in the corner. This video shows a few ways you can use the element inspector.\n\n\nUsing the element inspector allows you to find code related to an element so you can understand the code and make changes. We can try it on David Robinson’s website by opening web developer tools, selecting the image of David, and changing the image link to another image on the web.\n\n\n\n\nChanging the content can be useful for obvious reasons, but we can also modify the CSS around objects. We might not want the radial border around the image. We can find that CSS property if we select the image and look in the styles tab. Finding the right CSS property can require some knowledge, but it’s usually not too hard to figure looking at the different names and tinkering. David’s picture is inside an html img tag which contains a class class=\"bio-photo. The styles tab has a CSS selector called .article-author-side .bio-photo. That CSS selector has a property called border-radius. Un-check that property and see what the site would look like without it. A note, many of the bigger sites use system generated class names and things of that nature which can make this approach break down quickly.\n\n\n\n\nWe can also add new properties to see what a change might look like. Knowing all the styling options you can use is overwhelming, but just search by the html tag name, in this case img, and “CSS properties” (i.e. search img CSS properties) and you’ll usually get useful results. On this one I ended up back at w3schools CSS tutorial.\n\n\n\n\nWe could probably take this pretty far, but I’ll cover one more CSS change. We can change the fonts of text in the site using the same basic approach. Fonts have several CSS properties, but the property that is similar to “font” inside of a word processing software is font family. Just like most areas in technology, fonts can get very “deep.” To my understanding, no fonts are perfectly web safe, meaning they may not work as intended in some browsers or contexts. These fonts are considered the safest, but they’re also a bit bland which might not be the best choice for all sites. Regardless of what font you choose, always have a few fallback fonts. Meaning, instead of deleting the font family listed, we can just add our new font to the front. There are obvious style considerations like this new font looking way different than the fallback, but that’s a different discussion. When setting fonts in the CSS, include a fallback that you intend to be the fallback rather than relying on the browser’s default. If you want to find reasonably web safe fonts, Google fonts has a lot of options."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#testing",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#testing",
    "title": "Building a blogdown site",
    "section": "Testing ",
    "text": "Testing \nWhile we’re building our site we’ll want to test that it works. RStudio has a viewer pane that’s closest to development, which I love. However, there are always little differences that can pop up between what we see in RStudio and what shows up in the browser when it gets there. I recommend testing in a browser often. I use Brave as my primary web browser, but I also use Chrome and Safari to test my pages during web development because most users will likely be in one of the two. \n\n\n\nBeyond that, I also like to do a little bit of device testing. I don’t expect I have a ton of mobile users, but I still don’t want to have an awful mobile experience if I can help it. I use a tool called LT Browser to quickly see how the site will look on various devices."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#hugo-basics",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#hugo-basics",
    "title": "Building a blogdown site",
    "section": "Hugo Basics\n",
    "text": "Hugo Basics\n\nAs noted, I’ve never built a Hugo site without blogdown so take all my Hugo advice and instruction with a block of salt. That said, there are a few Hugo concepts that you do have to understand if you want to build a site with blogdown and / or customize the site. I’d recommend reading Alison’s article How much hugo do I need to know?. This will help you decide what parts you want to spend time trying to understand. I’ll be referencing parts of this article later as well, so it provides helpful context. Aside from that, here’s my high-level understanding of these concepts gives you more than enough context for the walk-through.\n\n\nThemes\n\nHugo sites are built off of themes, which I think of as a website template. Hugo provides mechanics to use a single template for many purposes by changing parameters before rendering the site. Hugo provides a list of themes or you can build your own. When selecting a theme to use, I recommend going to the github page and looking at the commit history, the contributors, and the documentation to assess how much you can rely on the theme. It’s obviously just a guess regardless, but I think it’s a good idea. Regardless, once you select a theme, test it with blogdown before you do anything else. Many themes don’t work with blogdown out of the box. I honestly don’t know exactly why, but I’d guess reasons vary from theme to theme.\n\n\n\nConfig file\n\nHugo uses a config file with a list of variables that allow users to easily customize their site. For example, the variable title is used to set the title of your site that shows up in search engines and on the browser tab. Themes generally come with a config file, but the variables included vary from theme to theme. The config variables are provided by Hugo so they can be added, but themes can also provide custom variables called params. Params can be in the config file or in a separate file in the config directory. Configs generally contain a menu which is how the site menu (i.e. navigation bar) is configured. The menu also has variables provided by Hugo and can be broken into a separate file in the config directory. This site is small so I don’t expect the config file to get too big, but for larger sites the config can be broken into several files.\n\n\n\nStatic Files\n\nStatic files get served as-is on the site root. This is useful for storing resources in the site’s root. The resources can be images, JS, CSS, etc. (e.g. an image on the home page).\n\n\nContent\n\nThe content is the substance of the site and Hugo assumes so organizational structure of the site which is explain here. This part seems complicated, but in my experience this is the easiest part to understand when using blogdown.\n\n\n\nTemplating\n\nTemplating is basically Hugo’s engine. It seems very intimidating when you start using blogdown, but it’s honestly not - we’re just not comfortable enough (or at least I’m not) with all the HTML, CSS, & JS to fully understand how simple Hugo is. Hugo uses templated HTML in combination with other resources in order to dynamically build webpages when the site gets built. Sounds kind of crazy, but if you use glue or jinja it’s pretty familiar. Template variables and functions are accessed within {{ }}."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#project-setup",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#project-setup",
    "title": "Building a blogdown site",
    "section": "Project setup\n",
    "text": "Project setup\n\nIn Rstudio, go to File -&gt; New Project and there is a simple template. Alternatively, we can use the function blogdown::new_site(). Once the site is built, check that the template is working properly by running blogdown::serve_site(). For more details refer to the book.\n\n\n\n\nBlogdown will automagically setup a repo structure (create folders and files) for the site, similarly to how devtools::create_package() does for R package development if that’s a familiar process. Since blogdown is a wrapper around Hugo, the repo has resources for both blogdown and Hugo. I’ll get into more detail as we customize certain parts of the site, but Alison provided an easy cheat sheet for the folder structure in the below tweet.\n\n\n\n\n{{% tweet \"1078494406301212672\" %}}"
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#customize-navbar",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#customize-navbar",
    "title": "Building a blogdown site",
    "section": "Customize Navbar\n",
    "text": "Customize Navbar\n\nThe navbar we start with has a number of components including the home button, page links, and CSS properties. To modify it, we’ll start in the config file - config.yaml.\n\n\n\n\n\nheader v.0\n\n\n\n\nPage Links\n\nWe can modify page links by changing the menu object in config.yaml. The name is the text that will show up in the header, the url is the path that will be added to the site’s base url, and the weight dictates the order on the header (lower numbers show up first reading left to right). The urls must be relative to the context root or an absolute url. For example, for the site www.mark-druffel.com, using /about/ in the menu would go to wwww.mark-druffel.com/about. and www.duckduckgo.com would go to that external site. This is very straight forward, but using absolute paths can be useful to forward a user to a resume or link in a shiny app.\n\nmenu:\n  main:\n    - name: \"About\"\n      url: /about/\n      weight: 1\n    - name: \"Blog\"\n      url: /post/\n      weight: 2\n\n\nHome Button\n\nThe logo object seems to be the home button, but it’s not immediately apparent where the logo.png file comes from. If you poke around the folders you’ll find the image at ~/themes/hugo-lithium/static/images/logo.png. You can add another image file there and change the config to update your header, but remember Alison warned us not to touch ~/themes. Changing the file in ~/themes will technically work, but the safer way to make the change is by adding the new logo to ~/static. Blogdown will take files from ~/static in replacement of files in ~themes/.../static. We’ll use static for a number of modifications so it’s easiest to create a few required folders now.\n\n\n\nCode\nfs::dir_create(\"static/css\")\nfs::dir_create(\"static/fonts\")\nfs::dir_create(\"static/images\")\nfs::dir_create(\"static/js\")\n\n\nAs you can probably guess, we’ll use the ~/static/images folder for the new logo. I added the image file, modified config.yaml to reflect the new file name, and ran blogdown::serve_site() again. The new image loaded, but the logo doesn’t look right. My image is a rectangle and the edges are being shaved off.\n\n  logo:\n    alt: Logo\n    height: 50\n    url: home_logo.svg\n    width: 50\n\n\n\nThis is just a CSS property. We can use the element inspector see the CSS properties on the logo element. The CSS selector .nav-logo img has a border-radius that looks like a good candidate. We confirm by un-checking the property from that selector.\n\n\n\n\nNow that we know the thing we want to change is coming from CSS, we need to modify the CSS file for the theme. Again, we’ll work from ~/static not ~/themes. You can simply copy the main.css file to static using the code below, open the css file, and comment out or delete line 69. CSS comments are applied like this - /* border-radius: 100%; */.\n\n\n\nCode\nfs::file_copy(\"themes/hugo-lithium/static/css/main.css\", \"static/css/main.css\", overwrite = F)\nusethis::edit_file(\"static/css/main.css\")\n\n\nNow the logo should load without the border radius. However, it’s possible it won’t even though the code is fine. Initially when I ran blogdown::serve_site() it had the border radius on all the browsers I tried. I restarted my R session and still had the issue. When I restarted RStudio and reran blogdown::serve_site() it worked in Brave, but it still didn’t work in Chrome (shown below). I’ve seen this behavior in a few situations using blogdown and I think there are a few reasons this can happen, but since it was working in one browser and not the other it seemed to be due to browser caching. I’d recommend reserving a specific browser for testing and continually clearing browsing data. This article explains a bit more about browsing data and instructions on deleting it.\n\n\n\n\n\nbrowser caches css\n\n\n\nSince the navbar is a pretty important part of the experience, it’s important it works everywhere. We can do device testing using LT Browser. My logo is pretty large and for some of the smaller devices it took over the header and moved the buttons. We can fix that for smaller devices specifically in the CSS. CSS has an @media rule that can be used to apply different styles to different devices based on things such as the screen size of the browser session. This CSS file already had an @media section setup, but the code I found on w3schools worked more consistently when I tested so I updated teh @media rule on line 253 to include only screen and. Additionally, we’ll add a max-height & max-width starting at line 262 to shrink the logo within this @media rule. The change fixed the issue on all devices.\n\n\n\nCode\n/* main.css */\n@media only screen and (max-width: 600px) {\n  .nav-logo img {\n    max-height: auto;\n    max-width: 150px;\n  }\n}\n\n\n\n\n\n\n\nBackground Color\nAgain, we can use the element inspector to figure out which CSS selector and property to change for the background color. The .header selector has a background property. We can replace that property on line 44 with a new color. Sometimes if I can’t find where a color is coming from in the web page, I use Color Slurp to pluck the color hex from the screen and search in the CSS files.\n\n\n\nCode\n/* main.css */\n.header {\n  padding: 20px 0;\n  position: relative;\n  background: #F6F9FB;\n  border-bottom: 1px solid #eaeaea;\n}\n\n\n\n\nFonts\nIf you want to use different fonts on the navbar (or anywhere else on the site), you’ll need to load new fonts. There are two ways I know of to accomplish this, download font files and import them from a folder or import them a font API in our CSS. I recommend the API route because I think it’s less work, but there are reasons you might want to download the files, namely if you plan to render the site without internet or in an environment with firewalls (that may interfere with API calls).\n\nDownload Font Files\nTo download the font files and import them, we need to move the foncts.css to ~/static/css. The fonts.css file sets up fonts for the site, which could be done in main.css.\n\nWe also need to move the font files themselves. We can define a font class, but the site still needs a font file to implement that font, similar to installing new fonts on your computer. Blogdown stores font files in ~/static/fonts. We can get fonts from a lot of places, but a great resource is Google Fonts. Select a font and download the files. One of the fonts I selected, Rubik, provided a variable font instead of a static font - you can read about the difference here. The files included are all .ttf, but we need .woff2. We can use this free conversion tool. Once the files are converted we can add them to our ~/static/fonts folder.\n\n\n\nCode\n# Copy & Edit CSS\nfs::file_copy(\"themes/hugo-lithium/static/css/fonts.css\", \"static/css/fonts.css\", overwrite = F)\nusethis::edit_file(\"static/css/fonts.css\")\n# Copy font files\nfs::dir_copy(\"themes/hugo-lithium/static/fonts\", \"static\")\n\n\n\n\n\nAfter adding the fonts to the folder, we need to import the fonts in our CSS. We can use the example from the variable font documentation as-is, but you can read more about the @font-face rule here.\n\n\nCode\n@font-face {\n    font-family: 'Rubik';\n    src: url('../fonts/Rubik-VariableFont_wght.woff2') format('woff2 supports variations'),\n       url('../fonts/Rubik-VariableFont_wght.woff2') format('woff2-variations');\n    font-weight: 100 900;\n    font-stretch: 25% 151%;\n}\n\n\n\n\nUse Font API\n\nAlternatively, we can directly import the fonts into our CSS. I recommend this as it’s much easier, but it requires an internet connection when rendering the site. To do this select a font, choose the styles, and copy the style sheet code. Make sure to select all the styles and weights you want available.\n\n\n\n\n\n\nCode\n@import url('https://fonts.googleapis.com/css2?family=Rubik:wght@300;400;500;700;800;900&display=swap');\n@import url('https://fonts.googleapis.com/css2?family=Overpass:wght@100;500;700;800;900&display=swap');\n@import url('https://fonts.googleapis.com/css2?family=Roboto:wght@100;300;500;700;900&display=swap');\n@import url('https://fonts.googleapis.com/css2?family=Merriweather:wght@300;400;700&display=swap');\n@import url('https://fonts.googleapis.com/css2?family=Fira+Code:wght@300;400;500&display=swap');\n\n\n\n\nApply New Fonts\n\nWe can add we can add the font to HTML elements or CSS selectors’ properties. We want to change the font of the navbar buttons so we’ll change the font-family property of the CSS selector .nav-links li. We can also make other changes like increasing font-size, weight, etc. Regarding font-size, you can use rem or em instead of px sizes, which scale to the browser settings, learn more here. I chose to use rem. Also, I set fallback fonts as recommended if you read about web safe fonts. I don’t think it’s necessary to fall back to serif or monospace unless you want to ensure a specific typeface, the browser will fallback to something automatically.\n\n\n\nCode\nbody {\n  font-family: Roboto, Merriweather;\n  font-size: 1rem;\n}\n.nav-links li {\n  display: inline-block;\n  margin: 0 0 0 15px;\n  font-family: Rubik, Overpass;\n  font-size: 1.25rem;\n}\n\n\nThat all looks great, but it’d also be nice to have some behavior when the user hovers on the button. We can do that by modifying a CSS selector with :hover. The :hover is a CSS Pseudo-selector applied to elements when you mouse over them. There are many Pseudo-selectors available you can read about here. The main CSS file already has a .nav-links a:hover selector so we can add a font-weight and text-decoration property to that. There’s a lot more we can do with buttons, but this site is very minimalist so I think this style works well. However, you can see several other options and how to implement them here. \n\n\nCode\n.nav-links a:hover {\n  color: #000;\n  font-weight: bold;\n  text-decoration: underline;\n}\n\n\nI also decided to remove 5px worth of padding from the navbar to make it just a tad shorter…\n\n\n\nCode\n.header {\n  padding: 15px 0;\n  position: relative;\n  background: #F6F9FB;\n  border-bottom: 1px solid #eaeaea;\n}\n\n\nNow we have a simple, but functional navbar!"
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#content-1",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#content-1",
    "title": "Building a blogdown site",
    "section": "Content\n",
    "text": "Content\n\nThe navbar spans across all the pages, but the content is each of the pages under the navbar. Site content goes in, you guessed it, ~/content/. The site will have an about page and posts by default, but you can add as many pages directly under ~/content/ as you’d like. Just don’t forget to add them to the menu in your config.yaml so the user can navigate to them. Beyond pages directly under ~/content/, the Hugo theme will generally have some default folders including ~/post/, which is the default location for blog posts.\n\n\nNew Posts\nTo create posts, I typically copy old posts because I customize my yaml header a bit. However, for a new site you can either copy one of the demo posts included or just use the RStudio add-in (shown below).\n\n\n\n\n\n\nPermalinks\n\nEach post’s url is generated by by Hugo using the the config.yaml and the document slug from the post’s yaml header. So if our site was www.mark-druffel.com and we used the configuration below, a post url would be www.mark-druffel.com/2021/10/15/this-posts-title. I personally find that to be overkill because I don’t post multiple times a day and my titles aren’t likely to be the same so I just use post: /:year/:slug/ for my blog.\n\npermalinks:\n  post: /:year/:month/:day/:slug/\n\n\nStyles\n\nThere are a number of changes that can be made to improve the look and feel of all pages under ~/content/, but for this site I’m sticking very close to the theme. Just know, you can change any of the CSS styles the same way we did with the navbar to apply CSS properties across all the content on the site! The one thing we will change for this site is fonts.\n\n\nFonts\n\nWe already changed fonts in the navbar so I won’t walk back through the entire process, but here’s the CSS code I used to modify fonts throughout my content. I just repeated the some process of finding parts of the page I wanted to modify, finding the CSS selectors, importing the font I wanted, and applying it to the CSS selector. \n\n\nCode\n.nav-links li {\n  display: inline-block;\n  margin: 0 0 0 15px;\n  font-family: Rubik, Overpass;\n  font-size: 1.25rem;\n}\nh1, h2, .article-title {\n  font-family: Roboto, Merriweather;\n  font-size: 1.5rem;\n  font-weight: bold;\n}\nh3, h4, .article-duration, .archive-item-link {\n  font-family: Roboto, Merriweather;\n  font-size: 1.3em;\n  font-weight: bold;\n}\nh5, h6, .footer {\n  font-family: Roboto, Merriweather;\n  font-size: 1rem;\n  font-weight: bold;\n}\n.article-date {\n  font-family: Roboto, Merriweather;\n  font-size: .9rem;\n}\nbody {\n  font-family: Roboto, Merriweather;\n  font-size: 1rem;\n}\ncode {\n  font-family: Fira Code, Source Code Pro, monospace;\n  font-weight: 300;\n}\n\n\nNow we have a blog with a custom navbar and fonts throughout all of the content. However, you might notice some content doesn’t pick up our CSS configurations.\n\n\n\n\n\n\nStyle Raster Graphics\n\nWhen we publish blog posts we’ll render all sorts of content using R libraries. Unfortunately, the outputs from our R code may not easily integrate to our site and this can be one of the trickier parts of maintaining a nice looking blog, especially if you choose a highly styled theme :grimacing: In this case, the image a raster graphic. Raster graphics are image files (pixels), which have no use for CSS code. We need to apply the same properties to our R graphics to match the style of the site. This applies to any plotting using the graphics device including ggplot2 :astonished:\n\n\n\n\nWe can resolve this problem a few different ways, but the first and most obvious approach is to customize the ggplot2 theme. The theme() function in ggplot2 provides a huge number of settings that can be used to customize the output including color, font, background, etc. This really becomes more of a ggplot2 tutorial, so I’m going to skip a bit of the details but the aforementioned resources should provide more than enough to customize your ggplot2 output for your blog. I try to be DRY (do not repeat yourself) when possible so I created a few themes for my blog for different visual types (e.g. one for standard visuals, one for faceted plots, one for non-grid based plots like network diagrams, etc.). Below is a gist of my standard theme if it’s a helpful starting point. You can choose existing themes and add onto them as show below (i.e. theme_minimal() + theme(...)).\n\n\n\nCode\nblogdown::shortcode(\"gist\", \"mark-druffel\", \"83b839abe61d056a2b4cef81d09f7bcc\")\n\n\n{{% gist \"mark-druffel\" \"83b839abe61d056a2b4cef81d09f7bcc\" %}}\n\n\nDepending how you style your blog, I think you could also use thematic and bslib, but since the blog doesn’t use bootstrap by default I personally haven’t tried to do that… Aside from the theme, some other helpful resources when using ggplot2 in blogdown include ggthemes, ragg, showtext, and ggtext.\n\nggthemes\n\nLibrary with complete ggplot2 themes including pre-built color scale functions. I don’t use ggthemes on my blog, but you could adopt a theme from there if any fit the style of your site which would simplify things tremendously. \n\n\nragg\n\nThe ragg library will simply improve R’s raster graphics rendering performance and quality. It will make ggplot2 output look sharper.\n\n\nshowtext\n\nThe showtext library makes it easier to use non-standard fonts with the R graphics device. Again, our site’s CSS is not available to ggplot2 when we render plots. Therefore, we’ll have to either install fonts on our machine (similar to adding them to a folder for the site) or load them from an API (similar to importing them in our CSS from the fonts API). Then we need to make them available to the R session. You can see an example in the package vignettes. Instructions for installing fonts on a machine will vary by OS, but it’s very simple to do on Linux, Mac, or Windows if you just use a search engine.\n\n\n\nggtext\n\nThe ggtext library allows the use of markdown syntax inside ggplot2. This does not change your theme, it’s just an easy way to bold titles and things like that on an individual plot.\n\nOk let’s look apply this stuff to our aforementioned plot from the demo post. Open ~/content/post/&lt;date&gt;-r-markdown/index.Rmd. Load the necessary libraries, load the fonts, run showtext_auto() to make the fonts available, and create a theme function you’ll use on your plot. I used a theme based on theme_void() since the demo is a pie chart.\n\n\n\nCode\nlibrary(ragg)\nlibrary(showtext)\nlibrary(tidyverse)\nlibrary(ggrepel)\n# Tell knitr to use ragg for graphics\nknitr::opts_chunk$set(dev = \"ragg_png\")\n# font_add_google is a function in sysfonts which is loaded by showtext\nfont_add_google(\"Rubik\", \"rubik\")\nfont_add_google(\"Roboto\", \"roboto\")\nfont_add_google(\"Fira Code\", \"firaCode\")\n# Adding a noticeable font just to make sure everything is working\nfont_add_google(\"Homemade Apple\", \"homemadeApple\")\nshowtext_auto()\ntheme_void_blog &lt;- function(...){\n  ggplot2::theme_void() + \n    ggplot2::theme(\n      text = ggplot2::element_text(\n        family = \"roboto\",\n        size = 11, \n        inherit.blank = FALSE),\n      title = ggplot2::element_text(\n        family = \"homemadeApple\",\n        face = \"bold\",\n        size = 18, \n        inherit.blank = FALSE,\n        hjust = .5),\n      plot.subtitle = ggplot2::element_text(\n        family = \"homemadeApple\",\n        size = 14, \n        inherit.blank = FALSE), \n    legend.title = ggplot2::element_text(\n      family = \"rubik\", \n      size = 12,\n      face = \"bold\",\n      inherit.blank = FALSE,\n      hjust = .5), \n    legend.text = ggplot2::element_text(\n      family = \"roboto\", \n      color = \"#33334D\", \n      size = 10, hjust = 0)\n    ) + \n    ggplot2::theme(...)\n}\n\n\nThis demo file uses the pie() function from base R. Nothing wrong with the function, but I’m not as familiar with the base R plotting tools so I’m going to use ggplot2 instead. The labels actually aren’t as simple with ggplot, but I just used ggrepel to make the labeling simpler. The new fonts should load and mostly the same visualization should render with your new fonts. \n\n\nCode\ntibble::tibble(\n  #value = c(280, 60, 20),\n  value = c(280, 60, 20),\n  name = c('Sky', 'Shady side of pyramid', 'Sunny side of pyramid'),\n  color = c('#0292D8', '#F7EA39', '#C4B632')\n) |&gt; \n  ggplot(aes(x = \"\", y = value, label = name, fill = color)) + \n  geom_col() + \n  coord_polar(theta = \"y\", start = pi / 1.275, direction = -1) +\n  scale_fill_identity() +\n  geom_text_repel(family = \"roboto\", size = 5, nudge_x = .6, show.legend = FALSE, segment.color = \"transparent\") +\n  labs(title = \"To make sure showtext is working\") +\n  theme_void_blog() \n\n\n\n\n\n\n\n\n\nStyle Vector Graphics\n\nR libraries don’t just produce raster graphics like plots from ggplot2, many libraries use the htmlwidgets framework which uses HTML, CSS, and JavaScript to produce interactive visualizations / widgets. Depending on exactly how the widget is set up, it will inherit CSS properties from your site. However, you may want to create new CSS properties or apply existing properties to specific parts of the widget.\n\nIf you add the code below to a new block inside ~/content/post/&lt;date&gt;-r-markdown/index.Rmd and run you’ll create a d3.js scatter plot from the metricsgraphics library.\n\n\n\nCode\nlibrary(metricsgraphics)\nmtcars %&gt;%\n  mjs_plot(x=wt, y=mpg, width=600, height=500) %&gt;%\n  mjs_point(color_accessor=carb, size_accessor=carb) %&gt;%\n  mjs_labs(x=\"Weight of Car\", y=\"Miles per Gallon\") \n\n\n\n\n\n\n\nIf you inspect the CSS carefully you can see the CSS properties we added in the apply new fonts section.\n\n\n\nThat’s likely the behavior we’d want. That said, if we wanted to modify the font of the axises, for instance, we could add CSS properties to the classes of the htmlwidget like so. Many htmlwidget libraries provide tools to do this directly, for instance metricsgraphics::mjs_add_css_rule(), but it’s good to know how to do it directly in the Rmarkdown document as well because sometimes that’s necessary or easier.\n\n\n\nCode\n.mg-x-axis {\n  font-family: Homemade Apple;\n}\n.mg-y-axis {\n  font-family: Homemade Apple;\n}\n\n\n\n\n\n\nCSS Conflicts\nSome htmlwidgets have generic CSS selectors that are in main.css, but no way to modify them through the library. An example I’ve bumped into in the past was the library skimr. These selectors were probably chosen intentionally to try to easily match site styles, but sometimes it doesn’t look great. To avoid those conflicts, throughout your entire main.css file you can use a prefix for the site. For example, you could change .header to .site-header.\n\n\nCode\n# Find and replace \".header {\"  to \".site-header {\"\nusethis::edit_file(main_css)\n\n\nIf you do that, you’ll have to find references of that class in your html partials. First, move the partial from the theme as we’ve mentioned before. WARNING, WE MODIFY THIS FILE SEVERAL PLACES. DO NOT RUN THIS CODE IF YOU ALREADY MOVED THIS FILE INTO YOUR PARTIALS FOLDER, IT WILL ERASE YOUR WORK!\n\n\nCode\nfs::file_copy(paste0(here::here(),'/themes/hugo-lithium/layouts/partials/header.html'), paste0(here::here(), '/layouts/partials/header.html'))\n\n\nYou’d need to modify header.html to the code below by changing the &lt;header class=\"header\"&gt; to &lt;header class=\"site-header\"&gt;.\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"{{ .Site.LanguageCode }}\"&gt;\n  &lt;head&gt;\n    {{ partial \"head.html\" . }}\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"wrapper\"&gt;\n      &lt;header class=\"site-header\"&gt;\n        {{ partial \"nav.html\" . }}\n      &lt;/header&gt;\n\n\n\nAdd Multimedia\n\nNow, let’s move onto the really hard hitting content like emojis! :joy: As you’ve seen throughout this post there’s a lot of multimedia you can use on a page. Hugo provides support for a number of things that blogdown wraps directly like emojis, tweets, youtube, etc. Beyond Hugo and blogdown, there are several R libraries that can provide additional content such as fontawesome.\n\nI’m going to modify the about page to demonstrate a few of these tools, content/about.md. Notice the about page is a .md file not an .Rmd file. Vanilla .md files can be used for simple pages with just text, images, emojis, but they will not have access to R. I generally use .Rmd files for everything, but .md are less complex and render much more quickly so keep that in mind.\n\nTo demonstrate all the multimedia functionality that comes to mind, I’ll need an .Rmd file so delete the .md version (optional) and create about.Rmd. In about.Rmd, first we setup our yaml header. There are a number of options that can be set in the yaml header, refer to the rmarkdown cookbook for more details - these yaml headers provide a lot of customization features out of the box.\n\n---\ntitle: \"About\"\n---\nUnder our header we can start to create our about page. We can use Hugo’s built in emojis in our sentences to insert emojis that are words wrapped in colons anywhere in the document. For these to work, we need to add a line to config.yaml.\n\n\n\n\nThe list of emojis supported by Hugo is small, but we can use the fontawesome package to insert fontawesome emojis. To find new fontawesome icons search here or use fontawesome::fa_metadata(). Note the heart in the code below is an emoji in the Hugo supported syntax, since blogdown parses the entire document it will add emojis in code blocks as well.\n\n\n\nCode\nThis is a blog I have created to share analytics with my team members. Although we have and :heart: Apache Superset & shiny, sometimes I just want to quickly share analysis I perform in `fontawesome::fa(\"r-project\", fill = \"steelblue\")`.\n\n\nHugo has a shortcode feature that allows users to pull in content from a number of platforms. The blogdown::shortcode() function is a wrapper to use it. It can be used to embed tweets, figures, youtube videos, gists, etc.\n\n\n\nCode\nblogdown::shortcode(\"youtube\", \"2xkNJL4gJ9E\")\n\n\n\nIt can also be used for syntax highlighting languages which may not be supported by knitr.\n\n\n\nCode\nblogdown::shortcode(\"highlight\", \"bash\", .content = \"echo Thanks for visiting our new site!;\")\n\n\nbash echo \"Thanks for visiting our new site!\"\nYou might be thinking, shortcode doesn’t support GIFs!?!@ Agreed, but don’t worrying GIFs are simple to add without using shortcode. They are an image and therefore work exactly like image files do. Add, ![&lt;my alt text&gt;](&lt;link to giph&gt;){&lt;add CSS properties}, to a new line in your document - not in a code block. In the CSS properties you can add anything you want, but I typically use it to specify width like {width=500px}. If you want to center the image, one simple way is to just wrap the line in &lt;center&gt; &lt;/center&gt; tags. You can save the giph in a folder in the post folder, or just link to a direct url. Giphy provides GIF Links if you hit the share button.\n\n\n\n\n\nMarkdown Formatting\nThere are so many guides on markdown formatting and I’m guessing most users will already know a little so I’m going to cut this section short. If you’re new to markdown, checkout this guide. If you’re like me, you probably know markdown pretty well but always bump into weird pesky things. I’m going to cover a few little tricks I’ve picked up that I find myself looking up a lot.\n\n\nSection Links\nSometimes you want to refer a user to a prior section. Do this the same way you add a hyperlink, but replace spaces in the section name with hyphens. For example, to refer someone back to the beginning of this entire section I would use [this](#build-a-site-from-scratch). which does this.\n\n\n\nBullets\n\nI often add bullets and the style looks terrible because markdown isn’t actually recognizing them as bullets. This is caused by two mistakes I commonly make: \n\nKeep one line between the paragraph above the bullets and the bullets themselves.\n\nMake sure you end the above paragraph and each bullet with a linebreak.\n\n\n\n\nLinebreaks\nAccording the markdown documentation:\n\n\nCommonMark and a few other lightweight markup languages let you type a backslash () at the end of the line, but not all Markdown applications support this, so it isn’t a great option from a compatibility perspective. The markdown docs instead recommend using two spaces (which is impossible to see in the IDE) or  at the end of a line. I’ve gotten into the habit of using backslash, but  seems like a good option as well."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#other-styling-components",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#other-styling-components",
    "title": "Building a blogdown site",
    "section": "Other Styling & Components\n",
    "text": "Other Styling & Components\n\n\nBrowser Tab Title & Image\n\nYou may notice when you preview the site there’s a little Li logo and a title A Hugo Website.\n\n\n\n\nWe can change the logo by creating a favicon logo and adding it. This site can convert images to icon files. Save the image in /static and add the appropriate reference to the config file. Also, change the text in the config file title.\ntitle: DocStation Analytics\nparams:\n  favicon: images/favicon.ico\n\n\nCode Folding\n\nThis section modifies the Hugo theme substantially more so than the other sections up to this point. It seems like a lot more work, but it’s very simple. Just make sure to follow along and do all of the steps unless I specifically say they’re optional because, if you miss any, the code show/hide button might not show up at all which makes it harder to troubleshoot. I never would have figured out how to do this without Sebastien Rochette’s work he published on his blog, so thanks so much to him! I also want to thank James Pustejovsky who also used Sebastian’s post and did his own incredibly helpful post.\n\nHigh Level Overview\nThe codefoling is made possible by a JS function, we’ll call it codefolding.js, which searches each page for code based on the HTML classes assigned by blogdown. When it finds code, it wraps it in an HTML div and adds a button. The JS function collapses the div when the button is clicked. That JavaScript function needs to be available to the pages, which means it needs to be in the header or footer of each HTML page. Further, that function relies on other code from bootstrap and custom CSS which need to be loaded first.\n\n\nAdd Dependencies\nSebastian wrote his codefolding blog post using Bootstrap 3 relying on collapse.js, dropdown.js, and transition.js, the last of which was deprecated in Bootstrap 4. I’m not enough of a Bootstrap guru to know where that functionality lives in Bootstrap 4 and frankly didn’t want to bother working through that for this one feature on my site - so I just stuck with Bootstrap 3, it works just fine. Those bootstrap functions rely on jquery so you need to download that as well. Finally, we’ll need popper.js to allow scrolling within our code boxes, but James’ post provided an API so we’ll just add it in our partials. For posterity, you can just download the bootstrap.min.js file, but I stuck with the three functions only since that’s all I needed.\n\nBootstrap 3 Latest Version\nJquery - I used 3.6.1\n\nAdd the JS files to the static/js folder. As we learned when changing the home button, the static/js files will override the files inside ~/themes/hugo-lithium/static. I this case, those files aren’t in ~/themes/hugo-lithium/static/images/logo.png so they’re simply additive. Again, you only need the three specific files or bootstrap.bundle.min.js, not all four as shown below.\n\n\n\n\nAside from JS, we’ll also need some CSS to format the div around the code block and the button that controls it. Ideally, we’d just use a recent Bootstrap distribution like this which does work, but it conflicts with other parts of the theme - for me it knocked things out of place in the navbar. We’d need to modify our main.css to manage the conflicts between that file and the boostrap file which could be a lot more work than just creating a custom CSS file for the code folding, which is what I decided to do. Copy the code below and save it at ~/static/css/codefolding.css. If you want to make your button look different (e.g. different color, hovering behavior, etc.), this is the place to do that.\n\n\nCode\n#code-folding-buttons {float: right;}\n#code-folding-buttons .pull-right &gt; .dropdown-menu {\n    right: 0;\n    left: auto;\n}\n#code-folding-buttons .btn.btn-default.btn-xs.dropdown-toggle {\n    float: right;\n}\n#code-folding-buttons.open &gt; .dropdown-menu {\n    display: block;\n}\n#code-folding-buttons .dropdown-menu li {\n    padding: 5px;\n}\n/* Each code block button */\n.code-folding-btn { \n  margin-right: 10px;\n  margin-bottom: 4px; \n  width: 125%;\n  background: #fff;\n  border-top: #252525 1px solid;\n  border-bottom: #252525 1px solid;\n  border-left: #252525 2px solid;\n  border-right: #252525 2px solid;\n  border-radius: 10px;\n  text-align: center;\n  color: black;\n  padding: 6px;\n  float: right;\n  font-family: Rubik;\n  font-size: .75rem;\n  font-weight: 400;\n}\n.code-folding-btn:hover { \n  background: #252525;\n  border-top: #fff 1px solid;\n  border-bottom: #fff 1px solid;\n  border-left: #fff 2px solid;\n  border-right: #fff 2px solid;\n  color: white;\n  font-family: Rubik;\n  font-size: .75rem;\n  font-weight: 600;\n}\n.row { \n    display: flex;\n    border-bottom: solid 1px #d7d7d7;\n}\n.col-md-12 {\n    margin: 0 0 0 auto;\n}\n.collapse {\n    display: none; \n}\n.in { \n    display: block ;\n    border: solid 1px #d7d7d7;\n    border-radius: 5px;\n}\n/* Main show/hide button */\n#code-folding-buttons {float: right;}\n#code-folding-buttons .dropdown-menu {\n    min-width: 80px;\n    padding: 5px 0;\n    margin: 2px 0 0;\n    font-size: 14px;\n}\n\n\n\n\nAdd Modify Partials\nNow that we have dependencies in place, we need to add the necessary HTML partials so that the dependencies are added when the site is rendered by Hugo. You can structure your partials however you want, but each theme generally has things structured a certain way that might be easy to piggy back off of. Below is some R code to copy files from the existing theme (hugo-lithium specific) and create new files we need.\n\n\nCode\nfs::file_touch(paste0(here::here(), '/layouts/partials/header_maincodefolding.html'))\nfs::file_copy(paste0(here::here(),'/themes/hugo-lithium/layouts/partials/head_custom.html'), paste0(here::here(), '/layouts/partials/head_custom.html'))\nfs::file_copy(paste0(here::here(),'/themes/hugo-lithium/layouts/partials/header.html'), paste0(here::here(), '/layouts/partials/header.html'))\n\n\nNotice partials have if statements in them, such as {{ if eq .RelPermalink \"/\" }} ... {{ else }}{{ if .Description }} ... {{ end }}{{ end }}. These if statements reference variables in config.yaml, it’s how the config.yaml is able to parameterize a Hugo site. When the site is rendered, Hugo reads the yaml and includes HTML partials based on the if statements. I actually don’t use parameters for my codefolding, but Sebastien set them up in his post and I left them in here for people who may want them - and to just better illustrate how Hugo works. You can completely remove all the if statements in the partials we create and leave the parameters off of the config file if you like. \n\nheader_alldoc_codefolding_button.html\nOptionally, Sebastien added a code folding button at the top of all posts which can show or hide code throughout the document. It looks like the screenshot below:\n\n\n\n\nI decided to leave this off of my site because I don’t like putting it in the navbar and it just felt clunky at the top of the doc. I would want to modify the navbar and use an icon in the top right, but didn’t seem worth the work. This is the html partial to do that though which I called header_alldoc_codefolding_button.html. You can copy it either way if you like, it won’t be rendered unless it’s added to another partial file.\n\n{{ if and (not .Site.Params.disable_codefolding) (not .Params.disable_codefolding) (in (string .Content) \"&lt;/pre&gt;\") }}\n&lt;div id=\"code-folding-buttons\" class=\"btn-group pull-right\"&gt;\n  &lt;button type=\"button\" class=\"btn btn-default btn-xs dropdown-toggle\" data-toggle=\"dropdown\" aria-haspopup=\"true\" aria-expanded=\"true\" data-_extension-text-contrast=\"\"&gt;\n    &lt;span&gt;Show/Hide all code&lt;/span&gt; \n    &lt;span class=\"caret\"&gt;&lt;/span&gt;\n  &lt;/button&gt;\n  &lt;ul class=\"dropdown-menu\" style=\"min-width: 50px;\"&gt;\n  &lt;li&gt;&lt;a id=\"rmd-show-all-code\" href=\"#\"&gt;Show All Code&lt;/a&gt;\n  &lt;/li&gt;&lt;li&gt;&lt;a id=\"rmd-hide-all-code\" href=\"#\"&gt;Hide All Code&lt;/a&gt;&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n{{ end }}\n\n\nhead_custom.html\nThe lithium template has a head_custom.html file by default, which is empty. It’s already loaded into the site so we don’t have to modify / create as many partials to implement our code folding by using it. We can use this to load dependencies into the site’s header. First, we’ll load the jquery, bootrap, and popper.js dependencies. Next, we load codefolding.js and codefolding.css. Finally, we can run codefolding.js (i.e. within $(document).ready). If you don’t plan to use config.yaml to disable code folding on certain posts you can omit the if statements.\n\n{{ if not .Site.Params.disable_codefolding }}\n&lt;script src=\"{{ \"js/jquery-3.6.0.min.js\" | relURL }}\"&gt;&lt;/script&gt;\n&lt;script src=\"{{ \"js/transition.js\" | relURL }}\"&gt;&lt;/script&gt;\n&lt;script src=\"{{ \"js/collapse.js\" | relURL }}\"&gt;&lt;/script&gt;\n&lt;script src=\"{{ \"js/dropdown.js\" | relURL }}\"&gt;&lt;/script&gt;\n  &lt;script src=\"https://cdn.jsdelivr.net/npm/popper.js@1.16.0/dist/umd/popper.min.js\"&gt;&lt;/script&gt;\n{{ end }}\n{{ if and (not .Site.Params.disable_codefolding) (not .Params.disable_codefolding) (in (string .Content) \"&lt;/pre&gt;\") }} \n&lt;script src=\"{{ \"js/codefolding.js\" | relURL }}\"&gt;&lt;/script&gt;\n&lt;link rel=\"stylesheet\" href=\"{{ \"css/codefolding.css\" | relURL }}\" /&gt;\n&lt;script&gt;\n$(document).ready(function () {\n  window.initializeCodeFolding(\"show\" === {{ if isset .Params \"codefolding_show\" }}{{ .Params.codefolding_show }}{{ else }}{{ default (\"hide\") .Site.Params.codefolding_show }}{{ end }});\n});\n&lt;/script&gt;\n{{ end }}\nThe head_custom.html partial is pulled into the ~/themes/hugo-lithium/layouts/partials/head.html partial, which we don’t need to modify or move out of themes. It’s loaded on the last line below with no if statement (i.e. it’s always loaded).\n\n\n\n\n\n\nheader.html\n\nThe aforementioned head.html, and by extension head_custom.html, are loaded into header.html (these names are a bit tough to keep straight). There’s no need to make changes to header.html unless you wanted to add the button to collapse and show all code, which is included below.\n\n&lt;!DOCTYPE html&gt;\n&lt;html lang=\"{{ .Site.LanguageCode }}\"&gt;\n  &lt;head&gt;\n    {{ partial \"head.html\" . }}\n  &lt;/head&gt;\n  &lt;body&gt;\n    &lt;div class=\"wrapper\"&gt;\n      &lt;header class=\"header\"&gt;\n        &lt;!-- Add this if you want the butto to callapse and show all code on a page --&gt; \n        {{ partial \"header_alldoc_codefolding_button.html\" . }}\n        {{ partial \"nav.html\" . }}\n      &lt;/header&gt;\n\n\n\nSetup Config\nIf you included the if statements in the partials, add these parameters to config.yaml at level 1 (i.e. not nested underneath anything in the file).\n\n# Set to true to disable code folding\ndisable_codefolding: false\n# Set to \"hide\" or \"show\" all codes by default\ncodefolding_show: hide\ncodefolding_nobutton: true\nAnd voila, you have code folding!\n\n\n\n\n\nPages with code should now have a button beside the code which will expand and collapse the code.\n\n\n\n\n\n\n\nAdd Links to Footer\nThe lithium template comes with a standard footer on every page. The footer is added into the bottom of every page from the HTML partial footer.html. We can modify the footer to include personal info that shows up at the bottom of all pages on our site.\n\n\n\n\nMove the footer file from the theme into our partials folder.\n\n\n\nCode\nfs::file_copy(paste0(here::here(),'/themes/hugo-lithium/layouts/partials/footer.html'), paste0(here::here(), '/layouts/partials/footer.html'))\n\n\n\nfooter.html\n\nI don’t use RSS feed at the moment, so I removed that. I might add it back at some point if I can get into a better groove with posting. I added an email, github, twitter, linkedin, and a credit to the blogdown repo. \n      &lt;footer class=\"footer\"&gt;\n        &lt;ul class=\"footer-links\"&gt;\n          {{ with .Site.Params.social }}\n          &lt;li&gt;\n            &lt;a href=\"mailto: mark.druffel@gmail.com\" class=\"fa fa-envelope \" aria-hidden=\"true\" title=\"Email\"&gt;&lt;/i&gt;&lt;span class=\"sr-only\"&gt;Github&lt;/span&gt;&lt;/a&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;a href=\"https://github.com/{{ .github }}\"&gt;&lt;i class=\"fa fa-github\" aria-hidden=\"true\" title=\"Github\"&gt;&lt;/i&gt;&lt;span class=\"sr-only\"&gt;Github&lt;/span&gt;&lt;/a&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;a href=\"https://twitter.com/{{ .twitter }}\"&gt;&lt;i class=\"fa fa-twitter\" aria-hidden=\"true\" title=\"Twitter\"&gt;&lt;/i&gt;&lt;span class=\"sr-only\"&gt;Twitter&lt;/span&gt;&lt;/a&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;a href=\"https://www.linkedin.com/in/{{ .linkedin }}\"&gt;&lt;i class=\"fa fa-linkedin\" aria-hidden=\"true\" title=\"LinkedIn\"&gt;&lt;/i&gt;&lt;span class=\"sr-only\"&gt;LinkedIn&lt;/span&gt;&lt;/a&gt;\n          &lt;/li&gt;\n          {{ end }}\n          &lt;li&gt;\n            &lt;a href=\"https://gohugo.io/\" class=\"footer-links-kudos\"&gt;Made with &lt;img src=\"{{ \"images/hugo-logo.png\" | relURL }}\" alt=\"Img link to Hugo website\" width=\"22\" height=\"22\"&gt;&lt;/a&gt;\n          &lt;/li&gt;\n          &lt;li&gt;\n            &lt;a href=\"https://bookdown.org/yihui/blogdown/\" class=\"footer-links-kudos\"&gt;Via &lt;img src=\"{{ \"images/blogdown-logo.png\" | relURL }}\" width=\"22\" height=\"22\"&gt;&lt;/a&gt;\n          &lt;/li&gt;\n        &lt;/ul&gt;\n      &lt;/footer&gt;\n    &lt;/div&gt;\n    {{ partial \"footer_highlightjs\" . }}\n    {{ partial \"footer_mathjax\" . }}\n    {{ template \"_internal/google_analytics.html\" . }}\n  &lt;/body&gt;\n&lt;/html&gt;\nIf you render the site with this partial you’ll end up with this footer, which is really ugly. The reason is we referenced font awesome logos in the footer.html partial so we need font awesome to render the logos. We need to add a font awesome dependency in the site header or footer to do that.\n\n\n\n\n\n\nhead_custom.html\n\nWARNING, WE MODIFY THIS FILE SEVERAL PLACES. DO NOT RUN THIS CODE IF YOU ALREADY MOVED THIS FILE INTO YOUR PARTIALS FOLDER, IT WILL ERASE YOUR WORK! The lithium template has a head_custom.html file by default, which is empty. It’s already loaded into the site so we don’t have to modify / create as many partials to implement our code folding by using it. We can use this to load dependencies into the site’s header.\n\n\n\nCode\nfs::file_copy(paste0(here::here(),'/themes/hugo-lithium/layouts/partials/head_custom.html'), paste0(here::here(), '/layouts/partials/head_custom.html'))\n\n\nAt the top of head_custom.html, add this line. The async part of the code keeps the site from loading slowly when font awesome doesn’t respond instantaneously.\n\n&lt;script async src=\"https://use.fontawesome.com/32c3d13def.js\"&gt;&lt;/script&gt;\nNow the footer looks better, but it’s still missing the image at the end. That’s the blogdown package logo which I added manually since it’s not in font awesome. Add the required image to ~/static/images and make sure the name matches the filename in the HTML partial footer.html.\n\n\n\n\nOnce you add the blogdown logo file, the footer should look like this. \n\n\n\n\n\n\nMisc Configs\n\nThere are so many configurations you can use in the lithium theme and even more throughout Hugo.\n\n\ndisqusShortname\n\nDisqus is a simple way to add a comments box on your site. Disqus has helped me engage with other data scientists through their sites which is a super simple way to connect with other people in the profession. It’s so simple, I highly recommend it. To use Disqus on your site, you need to create an account and then create a new site on that account. When you create a site, you give it a shortname which you can then add to the config.yaml.\n\nbaseurl: /\nlanguageCode: en-us\ntitle: DocStation Analytics\ntheme: hugo-lithium\ngoogleAnalytics: ''\ndisqusShortname: 'add the name here'\n\n\ncanonifyURLs\n\nTells Hugo to use canonical urls.\n\nbaseurl: /\nlanguageCode: en-us\ntitle: DocStation Analytics\ntheme: hugo-lithium\ngoogleAnalytics: ''\ncanonifyURLs: true\n\n\nenableRobotsTXT\n\nTells Hugo to use a custom robots.txt file, which tells bots how to interact with your site. Learn more here. To modify the robots.txt file, add the below code to the config.yaml.\n\nbaseurl: /\nlanguageCode: en-us\ntitle: DocStation Analytics\ntheme: hugo-lithium\ngoogleAnalytics: ''\nenableRobotsTXT: true\nThen create a new robots.txt file at ~/layouts/robots.txt."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#deploy-w-hosting-service",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#deploy-w-hosting-service",
    "title": "Building a blogdown site",
    "section": "Deploy w/ Hosting Service\n",
    "text": "Deploy w/ Hosting Service\n\nYou can deploy a blogdown site to an endless number of services including Netlify, Github Pages, Cloudflare, AWS S3, etc. I’m not going to document deployment because it’s so well documented elsewhere, but explain the first two options below. \n\nNetlify\n\nI get the impression that Netlify is the most popular place to host a blogdown. It’s to setup CI/CD with Github and Netlify has domain services so you can register your own custom domain in the same place. It’s what I use for my personal site. That said, if you want to set your site up with Netlify I’m going to hand you off to Alison because her up & running with blogdown in 2021 post provides a flawless step-by-step. There’s simply no reason to replicate here. \n\n\nGithub Pages\n\nGithub pages seems to be a close second in popularity from what I’ve seen. It’s even simpler than netlify, but you cannot use a custom domain without a premium account. Again, I’m going to outsource the guide on this approach because it’s so simple and already well documented in the blogdown book."
  },
  {
    "objectID": "posts/2022-01-05-building-blogdown-site/index.html#deploy-internally-w-docker",
    "href": "posts/2022-01-05-building-blogdown-site/index.html#deploy-internally-w-docker",
    "title": "Building a blogdown site",
    "section": "Deploy Internally w/ Docker ",
    "text": "Deploy Internally w/ Docker \nAs I mentioned, I host my personal site on Netlify - it’s been very cheap and easy to use. However, I want to publish an internal blog at work that needs to be secured. Netlify requires an enterprise account to use oauth, and their new pricing model would be cost prohibitive for a small startup. Further, because we work with health data we use a service called Aptible that makes HIPPA compliance and auditing much simpler. Given that, I need to host my blogdown site in a Docker container. Given the length of this post I’m going ot save that for another later post (if / when time allows)."
  },
  {
    "objectID": "posts/2023-04-05-web-scraping/index.html",
    "href": "posts/2023-04-05-web-scraping/index.html",
    "title": "Web Scraping w/ R",
    "section": "",
    "text": "I had a web scraping walk through on my old blog where I scraped Airbnb to find listings with king size beds, but Airbnb did major updates to their site and the post wouldn’t render when I did an overhaul to my website so I no longer have it. I was trying to do a little scraping while I was doing some research and found myself wanting my guide so I started writing a new one while I was working - I’m coming back to it now to finish it up as a post. I’m not going to bother covering some of the basic web development skills necessary because I coincidentally cover most of that in the prerequisites for my post on building a blogdown site. So without further delay, let’s get to scraping!"
  },
  {
    "objectID": "posts/2023-04-05-web-scraping/index.html#static-sites",
    "href": "posts/2023-04-05-web-scraping/index.html#static-sites",
    "title": "Web Scraping w/ R",
    "section": "Static Sites",
    "text": "Static Sites\nWhen scraping, as is true with many things done it code, it’s easiest to start small. Scraping static sites is significantly less complicated than dynamic sites so let’s start there. We can demonstrate this with the CRAN packages page - it’s structured in a way that’s ideal for scraping so it’s straight forward.\n\nParsing HTML\nThe rvest package has a suite of tools for parsing the HTML document, which is the core functionality required to scrape. The first thing to do when scraping a page is to figure out what we want to scrape and determine it’s HTML structure. We can do this using the browser’s web developer tools, but most of this can also be done inside RStudio. It probably goes without saying if you look at the CRAN packages page, but I’d like to scrape the packages table and make it a data frame.\n\n\n\nTo inspect the page, we first read the page using read_html(). This reads the HTML page as is into our rsession for processing. We can see that the cran_packages_html object is a list of length two and both objects inside the list are external pointers. In other words, the cran_packages_html document is not in the active rsession, rather a pointer which directs R to the documents created by libxml2 which are stored in RAM (at least this is my rough understanding of how it works). For more information, Bob Rudis provided a very detailed response about scraping which touches on this point, but the take away should be that this object does not contain the data from the HTML page - just pointers!\n\n\nCode\nlibrary(tidyverse) \nlibrary(rvest)\ncran_packages_html &lt;- read_html(\"https://cran.r-project.org/web/packages/available_packages_by_date.html\")\nstr(cran_packages_html)\n\n\nList of 2\n $ node:&lt;externalptr&gt; \n $ doc :&lt;externalptr&gt; \n - attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n\n\n\n\n\n\n\n\nxml2 not attached\n\n\n\n\n\nAn aside, if you open cran_packages_html with viewer and trying to inspect one of the pointers, you’ll get an error could not find function \"xml_child\". That’s because rvest depends on xml2, but does not attached it to the global environment.\n\n\n\nYou can simply load xml2 to fix the issue.\n\n\nCode\nlibrary(xml2)\nxml_child(cran_packages_html, 2)\n\n\n{html_node}\n&lt;body lang=\"en\"&gt;\n[1] &lt;div class=\"container\"&gt;\\n&lt;h1&gt;Available CRAN Packages By Date of Publicati ...\n\n\n\n\n\nThe rvest package has a suite of functions for parsing the HTML document starting with functions that help use understand the structure including html_children() & html_name(). We can use html_children() to climb down the page and html_name() to see the tag names of the HTML elements we want to parse. For this page, we used html_chidren() to see that the page has a  and a , which is pretty standard. We’ll want to scrape the &lt;body&gt; because that’s where the content of the page will be.\n\n\nCode\ncran_packages_html |&gt; \n  html_children() |&gt; \n  html_name()\n\n\n[1] \"head\" \"body\"\n\n\nTo further parse the &lt;body&gt;, we’ll use html_element() to clip the rest of the HTML document and look inside &lt;body&gt;. Within &lt;body&gt;, we can see there’s just a []](https://www.w3schools.com/tags/tag_div.asp).\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_children() |&gt; \n  html_name()\n\n\n[1] \"div\"\n\n\nWe can continue the process with the &lt;div&gt; and we see an an  and a . It’s fairly obvious we’ll want to the &lt;table&gt;, not &lt;h1&gt;, but just to illustrate if we look within &lt;h1&gt;, we’ll see no nodes exist beneath it.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h1\") |&gt; \n  html_children() |&gt; \n  html_name()\n\n\ncharacter(0)\n\n\nThat doesn’t mean &lt;h1&gt; has no data, it just means no HTML is a child of &lt;h1&gt;. Since &lt;h1&gt; a tag used on titles (text), we can use [html_text()] to extract the actual text inside. This isn’t particularly useful in this case, but html_text() can be very useful.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"h1\") |&gt; \n  html_text() \n\n\n[1] \"Available CRAN Packages By Date of Publication\"\n\n\nIf we use html_element(\"table\"), we can see it contains the data we’re looking for, but there’s a bit of HTML junk we’ll need to clean up for our data frame.\n\n\nCode\ncran_packages_html |&gt;  \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"table\") \n\n\n{html_node}\n&lt;table border=\"1\"&gt;\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/btergm/inde ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/campsis/ind ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cml/index.h ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/convertbond ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/ctrdata/ind ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/depigner/in ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/domir/index ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/fAssets/ind ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/fxl/index.h ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/ggblanket/i ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/grateful/in ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hbamr/index ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hdnom/index ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hierarchica ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/IDEATools/i ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/jacobi/inde ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/jdenticon/i ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Mapinguari/ ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/mapsRintera ...\n...\n\n\n\n\n\n\n\n\nhtml_element() will jump nodes\n\n\n\n\n\nIn the code above, we walked down the whole HTML tree body &gt; div &gt; table. The html_element() function will pickup HTML tags without providing the exact path, which is very convenient but can lead to unexpected results. The code below leads to the same results, but only because this page only has one HTML table. If it had multiple, it would only pick up the first one whether that was our intent or not. This point is very important to understand for more complicated web pages.\n\n\nCode\ncran_packages_html |&gt;   \n  # Skipped &lt;body&gt; & &lt;div&gt;\n  html_element(\"table\") \n\n\n{html_node}\n&lt;table border=\"1\"&gt;\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/btergm/inde ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/campsis/ind ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cml/index.h ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/convertbond ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/ctrdata/ind ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/depigner/in ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/domir/index ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/fAssets/ind ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/fxl/index.h ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/ggblanket/i ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/grateful/in ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hbamr/index ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hdnom/index ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hierarchica ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/IDEATools/i ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/jacobi/inde ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/jdenticon/i ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Mapinguari/ ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/mapsRintera ...\n...\n\n\n\n\n\nFortunately, rvest has a handy html_table() function that’s specifically for HTML tables and automatically coerces them into a list of tibbles. I used bind_rows() to coerce the list to a tibble. As you can see below, we end up with a table of packages with a date, package name, and title.\n\n\nCode\nlibrary(tidyverse) \nlibrary(reactable)\ncran_packages_df &lt;- cran_packages_html |&gt; \n  html_table() |&gt; \n  bind_rows()\n\ncran_packages_df |&gt; \n  reactable(\n    searchable = TRUE, \n    paginationType = \"jump\",  \n    showPageSizeOptions = TRUE,\n    pageSizeOptions = c(5, 10, 50, 100),\n    defaultPageSize = 5)\n\n\n\n\n\n\n\n\n\n\nThe table from above has the package names, but it doesn’t include most of the package metadata. Going back to the site, you can see the package name has a link to another page that contains all that data.\n\n\n\n\n\n\nPackages\n\n\n\n\n\n\n\nPackage Details\n\n\n\n\n\nIf we wanted to obtain the URL’s, we need to parse the &lt;table&gt;. Using html_children() again we can see that &lt;table&gt; contains &lt;tr&gt; tags, which is HTML table rows.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"table\") |&gt; \n  html_children()\n\n\n{xml_nodeset (19410)}\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/btergm/inde ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/campsis/ind ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cml/index.h ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/convertbond ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/ctrdata/ind ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/depigner/in ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/domir/index ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/fAssets/ind ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/fxl/index.h ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/ggblanket/i ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/grateful/in ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hbamr/index ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hdnom/index ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/hierarchica ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/IDEATools/i ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/jacobi/inde ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/jdenticon/i ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Mapinguari/ ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-04-24 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/mapsRintera ...\n...\n\n\nThen we can go a level lower and see all the elements in the rows. Notice, we use the html_elements() (plural) function instead of html_element(). That’s because each row has multiple elements and html_element() will only parse the first element. We can a &lt;td&gt; tag, which is an HTML data cell, and an &lt;a&gt; tag, which is a hyperlink. The &lt;a contains href=\"../../web/packages/.... An href is an HTML attribute for creating hyperlinks on a web page.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"table\") |&gt; \n  html_elements(\"tr\") |&gt; \n  html_children()\n\n\n{xml_nodeset (58230)}\n [1] &lt;th&gt; Date &lt;/th&gt;\n [2] &lt;th&gt; Package &lt;/th&gt;\n [3] &lt;th&gt; Title &lt;/th&gt;\n [4] &lt;td&gt; 2023-04-24 &lt;/td&gt;\n [5] &lt;td&gt; &lt;a href=\"../../web/packages/btergm/index.html\"&gt;&lt;span class=\"CRAN\"&gt;b ...\n [6] &lt;td&gt; Temporal Exponential Random Graph Models by Bootstrapped\\nPseudolik ...\n [7] &lt;td&gt; 2023-04-24 &lt;/td&gt;\n [8] &lt;td&gt; &lt;a href=\"../../web/packages/campsis/index.html\"&gt;&lt;span class=\"CRAN\"&gt; ...\n [9] &lt;td&gt; Generic PK/PD Simulation Platform CAMPSIS &lt;/td&gt;\n[10] &lt;td&gt; 2023-04-24 &lt;/td&gt;\n[11] &lt;td&gt; &lt;a href=\"../../web/packages/cml/index.html\"&gt;&lt;span class=\"CRAN\"&gt;cml&lt; ...\n[12] &lt;td&gt; Conditional Manifold Learning &lt;/td&gt;\n[13] &lt;td&gt; 2023-04-24 &lt;/td&gt;\n[14] &lt;td&gt; &lt;a href=\"../../web/packages/convertbonds/index.html\"&gt;&lt;span class=\"C ...\n[15] &lt;td&gt; Use the Given Parameters to Calculate the European Option Value &lt;/td&gt;\n[16] &lt;td&gt; 2023-04-24 &lt;/td&gt;\n[17] &lt;td&gt; &lt;a href=\"../../web/packages/ctrdata/index.html\"&gt;&lt;span class=\"CRAN\"&gt; ...\n[18] &lt;td&gt; Retrieve and Analyze Clinical Trials in Public Registers &lt;/td&gt;\n[19] &lt;td&gt; 2023-04-24 &lt;/td&gt;\n[20] &lt;td&gt; &lt;a href=\"../../web/packages/depigner/index.html\"&gt;&lt;span class=\"CRAN\" ...\n...\n\n\nWe can extract attributes using htm_attr() which will parse the text into a character vector.\n\n\nCode\ncran_packages_df$href &lt;- cran_packages_html |&gt; \n  html_elements(\"tr\") |&gt; \n  html_elements(\"td\") |&gt; \n  html_elements(\"a\") |&gt;\n  html_attr(\"href\")\n\n\nIn the code above, we walked down the whole HTML tree, but again we could’ve jumped down the tree like so. Just understand that this can lead to us picking up other hyperlinks that aren’t in our table (there are none of this page).\n\n\nCode\ncran_packages_html |&gt; \n  html_elements(\"a\") |&gt; \n  html_attr(\"href\")\n\n\nHowever, we do have to specify an HTML element before using html_attr() otherwise we’ll just get back NA.\n\n\nCode\ncran_packages_html |&gt; \n  html_attr(\"href\")\n\n\nOne thing that’s worth mentioning is the URL’s we collected are relative paths, which is why they have /../ in the path.\n\n\nCode\ncran_packages_df$href[1:3]\n\n\n[1] \"../../web/packages/btergm/index.html\" \n[2] \"../../web/packages/campsis/index.html\"\n[3] \"../../web/packages/cml/index.html\"    \n\n\nThis is a good point to take step way back to the beginning to introduce a few additional concepts, but if you made it this far you’ve learned enough to scrape a lot of websites with no additional tools. The html_ functions in rvest provide the core tools necessary to parse HTML which is what scraping is…"
  },
  {
    "objectID": "posts/2023-04-05-web-scraping/index.html#paging",
    "href": "posts/2023-04-05-web-scraping/index.html#paging",
    "title": "Web Scraping w/ R",
    "section": "Paging",
    "text": "Paging\nUp to this point, we’ve read HTML pages directly using read_html(). That would get pretty cumbersome if we want to read dozens of pages, let alone the 20K CRAN package pages.\nWhen we use read_html() we copy the page to RAM as discussed earlier, but if we want to read hundreds of pages of a site we want to open an active web connection, or a session if you will :smile: Yup, you guessed it, that’s where rvest’s session() function comes into the picture.\n\n\nCode\nsesh_cran_packages &lt;- rvest::session(\"https://cran.r-project.org/web/packages/available_packages_by_date.html\")\n\n\nWe can do everything we did in the prior session using a session object.\n\n\nCode\ncran_packages_html &lt;- sesh_cran_packages |&gt; \n  read_html()\n\ncran_packages_df &lt;- cran_packages_html |&gt; \n  html_table() |&gt; \n  bind_rows()\n\ncran_packages_df$href &lt;- cran_packages_html |&gt; \n  html_elements(\"tr\") |&gt; \n  html_elements(\"td\") |&gt; \n  html_elements(\"a\") |&gt;\n  html_attr(\"href\")\n\nglimpse(cran_packages_df)\n\n\nRows: 19,902\nColumns: 4\n$ Date    &lt;chr&gt; \"2023-08-10\", \"2023-08-09\", \"2023-08-09\", \"2023-08-09\", \"2023-…\n$ Package &lt;chr&gt; \"DrugSim2DR\", \"actxps\", \"AgroR\", \"aplot\", \"av\", \"basemodels\", …\n$ Title   &lt;chr&gt; \"Predict Drug Functional Similarity to Drug Repurposing\", \"Cre…\n$ href    &lt;chr&gt; \"../../web/packages/DrugSim2DR/index.html\", \"../../web/package…\n\n\nAnd we can use the session_ verbs such as session_jump_to() to go to relative pages on the site.\n\n\nCode\nrvest_details &lt;- sesh_cran_packages |&gt; \n  session_jump_to(url = cran_packages_df$href[cran_packages_df$Package == \"rvest\"]) |&gt;\n  read_html()\n\n\nWe could use session_jump_to() because we scraped all the package page URL’s earlier, but we could also use session_follow_link() to effectively click on the hyperlinks. The code below accomplishes the same thing as the prior block. However, the session_follow_link() function can be challenging to use with i because it only takes a session object and uses an internal function, find_href(), which uses html_elements(\"a\") to find all hyperlinks on the page and follows the ith hyperlink in that vector. It wraps session_jump_to(), which inherently provides you with significantly more control.\n\n\nCode\npage_title &lt;- sesh_cran_packages |&gt; \n  session_follow_link(i = which(cran_packages_df$Package == \"rvest\")) |&gt; \n  read_html() |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"h2\") |&gt; \n  html_text()\nprint(page_title)\n\n\n[1] \"rvest: Easily Harvest (Scrape) Web Pages\"\n\n\nNow that we have the detail page for rvest, we can structure parse and structure the package metadata using the same set of tools we’ve used along the way and a few stringr and janitor functions to clean text.\n\n\nCode\nlibrary(htmltools)\npackage &lt;- rvest_details |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h2\") |&gt; \n  html_text() |&gt; \n  str_split_i(pattern = \":\", i = 1)\n\ndescription &lt;- rvest_details |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h2\") |&gt; \n  html_text() |&gt; \n  str_split_i(pattern = \":\", i = 2) |&gt; \n  str_squish()\n\npackage_tables &lt;- rvest_details |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_elements(\"table\")\n\npackage_details &lt;- map(package_tables, function(x){\n  pack_table &lt;- x |&gt; \n    html_table()  \n}) |&gt;\n  bind_rows() |&gt; \n  rename(\"key\" = \"X1\", \"value\" = \"X2\") |&gt; \n  mutate(value = str_trunc(value, width = 300))\n\nrvest_table &lt;- reactable(package_details, filterable = T, pagination = FALSE)\n\ndiv(class = \"reactable-tbl-view\",\n    div(class = \"div-subtitle\",\n        div(class = \"div-title\", package),\n        description\n    ),\n    rvest_table\n)\n\n\n\n\n\nrvest\nEasily Harvest (Scrape) Web Pages\n\n\n\n\n\n\nThere’s very little additional work to do in order to scrape all package packages and bind them to our initial data frame. I’m not going to run the code for all packages because there’s a faster way to get this data, but I run it here for a subset to demonstrate.\n\n\nCode\nlibrary(janitor) \nsubset_cran_packages_df &lt;- cran_packages_df[1:15,]\npackage_details_df &lt;- map(\n  subset_cran_packages_df$href, \n  function(url){\n    package_page &lt;- sesh_cran_packages |&gt;\n      session_jump_to(url = url) |&gt;\n      read_html()\n    package_description &lt;- package_page |&gt; \n      html_element(\"body\") |&gt; \n      html_element(\"div\") |&gt; \n      html_element(\"h2\") |&gt; \n      html_text() |&gt; \n      str_split_i(pattern = \":\", i = 2) |&gt; \n      str_squish()\n    package_page_tables &lt;- package_page |&gt;\n      html_element(\"body\") |&gt;\n      html_element(\"div\") |&gt;\n      html_elements(\"table\")\n    package_details &lt;- map(package_page_tables, function(ppt){\n      ppt |&gt;\n        html_table() |&gt;\n        pivot_wider(names_from = \"X1\", values_from = \"X2\")\n    }) |&gt;\n      bind_cols() |&gt;\n      mutate(across(.cols = everything(), .fns = ~stringr::str_trunc(.x, width = 225))) |&gt; \n      mutate(description = package_description, .before = 1)\n    return(package_details)\n}) |&gt; \n  bind_rows() |&gt; \n  clean_names(\"snake\")\n\npackages_reactable &lt;- bind_cols(subset_cran_packages_df, package_details_df) |&gt; \n  reactable(filterable = TRUE, pagination = T, paginationType = \"numbers\", defaultPageSize = 5)\n\ndiv(class = \"reactable-tbl-view\",\n    packages_reactable\n)\n\n\n\n\n\n\n\n\n\n\nCRAN Packages File\nAbove, we were able to scrape the site so we could gather the data for all pacakges pretty easily. That said, thanks to a helpful post from Jeroen Ooms, we can download all the package data from those pages (and then some) with the below code. Much simpler than all that scraping, but the data frame is 67 columns so I’m going to hold of on printing the entire thing. On that note, it’s always worth a little research to see if the site offers their data or an API before doing any scraping.\n\n\nCode\nmytemp &lt;- tempfile()\ndownload.file(\"http://cran.r-project.org/web/packages/packages.rds\", mytemp)\ncran_packages_df &lt;- as.data.frame(readRDS(mytemp), row.names=NA) |&gt; \n  clean_names(\"snake\")"
  },
  {
    "objectID": "posts/2023-04-05-web-scraping/index.html#etiquitte",
    "href": "posts/2023-04-05-web-scraping/index.html#etiquitte",
    "title": "Web Scraping w/ R",
    "section": "Etiquitte",
    "text": "Etiquitte\nNow that we’ve learned how to scrape, it’s time to discuss etiquette. The web has some rules regarding how you can programmatically use web sites. Sites publish their rules in their robots.txt file. Most sites on the web have a robots.txt file and it lives directly in the site’s root directory. As an example, here’s Netflix’s. These files tell us what pages the site administrator does not want us accessing and using programmatically. CRAN’s robot.txt asks that we not scrape the DESCRIPTION page of each package, but they don’t mention the index.html pages we scraped in our example so we didn’t violate their rules.\n\n\n\nThe polite library implements a framework to help users follow site rules and use best practices when scraping with a set of verbs. Beyond making it painless to follow site rules, polite also wraps in a number of features that may save you a lot of time in the long-run including caching. The first verb to learn in polite is bow(). We can us bow in place of rvest’s session() function. The bow() function provides some nice features, but it’s main purpose is to create a session and introduce us to the host before we make a request.\n\n\nCode\nlibrary(polite)\nsesh_cran_packages &lt;- bow(\n  url = \"https://cran.r-project.org/web/packages/available_packages_by_date.html\",\n  user_agent = \"www.mark-druffel.com (mark.druffel@gmail.com)\",\n  delay = 1,\n  force = F,\n  verbose = T\n)\n\n\nAfter bowing, we can use scrape() in place of read_html(). The scrape() function reads in the HTML document in the same way that read_html() does, but also directly provides some useful parameters including query. The query parameter provides an easy way to add URL parameters for filtering certain pages.\n\n\nCode\ncran_packages_df &lt;- sesh_cran_packages |&gt; \n  scrape(accept = \"text/html\") |&gt; \n  html_table() |&gt; \n  bind_rows() \n\ncran_packages_df$href &lt;- sesh_cran_packages |&gt; \n  scrape(accept = \"text/html\") |&gt; \n  html_elements(\"tr\") |&gt; \n  html_elements(\"td\") |&gt; \n  html_elements(\"a\") |&gt;\n  html_attr(\"href\")\n\n\nWe can also use nod() to ask the host if we can modify the session path before scraping the URL paths and it will tell us if we’re violating the site rules.\n\n\nCode\nsesh_cran_packages |&gt; \n  nod(path= cran_packages_df$href[1])\n\n\n&lt;polite session&gt; https://cran.r-project.org/../../web/packages/DrugSim2DR/index.html\n    User-agent: www.mark-druffel.com (mark.druffel@gmail.com)\n    robots.txt: 19919 rules are defined for 1 bots\n   Crawl delay: 1 sec\n  The path is scrapable for this user-agent\n\n\nCode\nsesh_cran_packages |&gt; \n  nod(path= cran_packages_df$href[1]) |&gt; \n  scrape(accept = \"text/html\") |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h2\") |&gt; \n  html_text() |&gt; \n  str_split_i(pattern = \":\", i = 1)\n\n\n[1] \"DrugSim2DR\"\n\n\n\n\n\nIf we are violating the rules, polite tell us when we nod and will stop us from going further.\n\n\nCode\nsesh_cran_packages |&gt; \n  nod(path= \"/web/packages/rvest/DESCRIPTION\")\n\n\n&lt;polite session&gt; https://cran.r-project.org/web/packages/rvest/DESCRIPTION\n    User-agent: www.mark-druffel.com (mark.druffel@gmail.com)\n    robots.txt: 19919 rules are defined for 1 bots\n   Crawl delay: 1 sec\n  The path is not scrapable for this user-agent\n\n\nCode\nsesh_cran_packages |&gt; \n  nod(path= \"/web/packages/rvest/DESCRIPTION\") |&gt; \n  scrape(accept = \"text/html\") \n\n\nWarning: No scraping allowed here!\n\n\nNULL\n\n\nFinally, polite provides a the rip() function to download files politely.\n\n\nCode\nmytemp &lt;- cran_packages_df &lt;- sesh_cran_packages |&gt; \n  nod(\"web/packages/packages.rds\") |&gt; \n  rip() \ncran_packages_df &lt;- as.data.frame(readRDS(mytemp), row.names=NA)  |&gt;  \n  clean_names(\"snake\")"
  },
  {
    "objectID": "posts/2023-04-05-web-scraping/index.html#dynamic-sites",
    "href": "posts/2023-04-05-web-scraping/index.html#dynamic-sites",
    "title": "Web Scraping w/ R",
    "section": "Dynamic Sites",
    "text": "Dynamic Sites\nAs was aforementioned\nstart small –&gt; paging –&gt; consensual scraping –&gt; dynamic sites –&gt; getting werid"
  },
  {
    "objectID": "posts/2021-08-13-european-energy/index.html",
    "href": "posts/2021-08-13-european-energy/index.html",
    "title": "European Energry",
    "section": "",
    "text": "This Tidy Tuesday, I’m analyzing European energy data. I don’t know much about the data set, but it seems like it contains European energy production data. To download the data I use the tidytuesdayR package.\n\n\nCode\nknitr::opts_chunk$set(echo = T, message = F, warning = F, error = F, fig.width = 9)\nlibrary(knitr)\nlibrary(htmltools)\nlibrary(markUtils)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(magrittr)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(tidytext)\nlibrary(ggiraph)\nlibrary(ggflags)\nlibrary(ggrepel)\nlibrary(ragg)\nlibrary(colorspace)\nlibrary(flextable)\nlibrary(data.table)\nlibrary(officer)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(leaflet)\nenergy_types &lt;- tt |&gt; \n  pluck(\"energy_types\") |&gt; \n  clean_names(\"snake\")\ncountry_totals &lt;- tt |&gt; \n  pluck(\"country_totals\") |&gt; \n  clean_names(\"snake\")"
  },
  {
    "objectID": "posts/2021-08-13-european-energy/index.html#background",
    "href": "posts/2021-08-13-european-energy/index.html#background",
    "title": "European Energry",
    "section": "",
    "text": "This Tidy Tuesday, I’m analyzing European energy data. I don’t know much about the data set, but it seems like it contains European energy production data. To download the data I use the tidytuesdayR package.\n\n\nCode\nknitr::opts_chunk$set(echo = T, message = F, warning = F, error = F, fig.width = 9)\nlibrary(knitr)\nlibrary(htmltools)\nlibrary(markUtils)\nlibrary(tidyverse)\nlibrary(janitor)\nlibrary(skimr)\nlibrary(magrittr)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(tidytext)\nlibrary(ggiraph)\nlibrary(ggflags)\nlibrary(ggrepel)\nlibrary(ragg)\nlibrary(colorspace)\nlibrary(flextable)\nlibrary(data.table)\nlibrary(officer)\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(leaflet)\nenergy_types &lt;- tt |&gt; \n  pluck(\"energy_types\") |&gt; \n  clean_names(\"snake\")\ncountry_totals &lt;- tt |&gt; \n  pluck(\"country_totals\") |&gt; \n  clean_names(\"snake\")"
  },
  {
    "objectID": "posts/2021-08-13-european-energy/index.html#analysis",
    "href": "posts/2021-08-13-european-energy/index.html#analysis",
    "title": "European Energry",
    "section": "Analysis",
    "text": "Analysis\nThis data set came with two data frames. Both data frames go from 2016 to 2018. The first data frame, country totals, has the country’s energy total production, imported, exported, absorbed by pumping, and total supplied. The write-up provides the helpful hint that supplied = (energy produced + imported - exported - absorbed by pumping).\n\n\nCode\nskim(country_totals)\n\n\n\nData summary\n\n\nName\ncountry_totals\n\n\nNumber of rows\n185\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncountry\n0\n1.00\n2\n2\n0\n37\n0\n\n\ncountry_name\n5\n0.97\n5\n20\n0\n36\n0\n\n\ntype\n0\n1.00\n7\n26\n0\n5\n0\n\n\nlevel\n0\n1.00\n5\n5\n0\n1\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nx2016\n1\n0.99\n45207.06\n99814.70\n0\n1620.50\n8426.0\n29583.31\n614155.0\n▇▁▁▁▁\n\n\nx2017\n0\n1.00\n45413.37\n100337.16\n0\n2204.74\n8189.7\n30676.00\n619059.0\n▇▁▁▁▁\n\n\nx2018\n0\n1.00\n45062.34\n98010.77\n0\n2186.68\n8326.0\n31671.10\n571799.7\n▇▁▁▁▁\n\n\n\n\n\nThe second data frame, energy types, has the total energy production by power plant type. Unfortunately, the write-up didn’t provide more info regarding the types, but I think conventional thermal most likely includes the common fossil fuel power plants such as coal & natural gas.\n\n\nCode\nskim(energy_types)\n\n\n\nData summary\n\n\nName\nenergy_types\n\n\nNumber of rows\n296\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n4\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncountry\n0\n1.00\n2\n2\n0\n37\n0\n\n\ncountry_name\n8\n0.97\n5\n20\n0\n36\n0\n\n\ntype\n0\n1.00\n4\n20\n0\n8\n0\n\n\nlevel\n0\n1.00\n7\n7\n0\n2\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nx2016\n0\n1\n12783.36\n41066.36\n0\n0\n373.28\n5677.25\n390141.0\n▇▁▁▁▁\n\n\nx2017\n0\n1\n12910.96\n41029.50\n0\n0\n351.89\n5924.46\n379094.0\n▇▁▁▁▁\n\n\nx2018\n0\n1\n12796.20\n39423.36\n0\n0\n278.35\n6790.15\n393153.2\n▇▁▁▁▁\n\n\n\n\n\n\nUp-front data cleaning\nBased on a review of the data, we can filter to level=\"Level 1\" for most analysis we’d want to do. According to the write-up provided, level two is a subgroup of level one and therefore double counts production. There is only one level two subgroup in the data set so there’s not a ton to analyze. There were also some missing values in country_name that were easy to impute based on country. I changed country codes for the United Kingdom and Greece to match the country codes in ggflags (we’ll use that later).\n\n\nCode\nenergy_types &lt;- energy_types |&gt; \n  filter(level == \"Level 1\") |&gt; \n  pivot_longer(\n    cols = contains(\"201\"),\n    names_to = \"year\",\n    values_to = \"gwh\"\n  ) |&gt; \n  mutate(\n    country = case_when(\n      country == \"UK\" ~ \"gb\",\n      country == \"EL\" ~ \"gr\",\n      T ~ str_to_lower(country)),\n    country_name = if_else(is.na(country_name), \"United Kingdom\", country_name),\n    type = as_factor(type),\n    year = as.integer(stringr::str_remove(year, \"x\"))\n  )\ncountry_totals &lt;- country_totals |&gt; \n  mutate(country_name = if_else(is.na(country_name), \"United Kingdom\", country_name))\nenergy_level1_types_totals &lt;- energy_types |&gt; \n  filter(level == \"Level 1\") |&gt; \n  group_by(type) |&gt; \n  summarise(\n    gwh = sum(gwh, na.rm = T)\n  ) |&gt; \n  mutate(pct_of_total = gwh / sum(gwh)) |&gt; \n  ungroup()\nnet_imports &lt;- country_totals |&gt;\n  filter(type %in% c(\"Imports\", \"Exports\")) |&gt; \n  pivot_longer(cols = contains(\"201\"), names_to = \"year\", values_to = \"gwh\") |&gt;\n  filter(!is.na(gwh)) |&gt; \n  group_by(country, country_name, type) |&gt; \n  summarise(gwh = sum(gwh), .groups = \"drop\") |&gt; \n  pivot_wider(names_from = type, values_from = gwh) |&gt; \n  summarise(imports = sum(Imports),\n            exports = sum(Exports),\n  ) %$%\n  (imports - exports)\n\n\n\n\nCountry Totals\nFirst, we can look at the total power supplied by country to see if the total amount supplied changed over the time period. The changes in production will be interesting as well, but given imports / exports we’ll want to first look at whether the populations were receiving less power (or the excess was going to exports). Only Germany saw a notable change.\n\n\nCode\ncountry_totals |&gt; \n  filter(type == \"Energy supplied\") |&gt; \n  pivot_longer(cols = contains(\"201\"), names_to = \"year\", values_to = \"gwh\") |&gt;\n  mutate(year = as.integer(str_replace(year, \"x\", \"\"))) |&gt; \n  group_by(country) |&gt; \n  mutate(supplied_change = gwh - lag(gwh, n = 2, order_by = year),\n         supplied_change_abs = abs(gwh - lag(gwh, n = 2, order_by = year))) |&gt; \n  mutate(supplied_change = max(supplied_change, na.rm = T),\n         supplied_change_abs = max(supplied_change_abs, na.rm = T)) |&gt; \n  ungroup() |&gt; \n  mutate(\n    country_name = fct_lump(country_name, n = 11, w = supplied_change_abs)) |&gt; \n  filter(country_name != \"Other\") |&gt; \n  group_by(year, country_name) |&gt; \n  summarise(gwh = sum(gwh), .groups = \"drop\") |&gt; \n  ggplot(aes(x = year, y = gwh, color = country_name)) +\n  geom_line() +\n  scale_y_continuous(labels = label_number(scale = 1/1000, suffix = \"K\")) +\n  scale_x_continuous(breaks = c(2016, 2017, 2018)) +\n  labs(y = \"Power Supplied (GWh)\", x = \"Year\", title = \"Top 10 countries with change in power supplied (2016-2018)\",\n       subtitle = \"Most countries were static, but Germany reduced power supplied by 7.4%\") +\n  theme_blog() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\nNext, we can use analyze country totals to understand the impact of imports / exports. Overall, the group is a net importer by 1.2915092^{4} GWh, but the picture is different by country. Below are the largest net exporters, Germany & France by a wide margin, and the largest net importers, Italy by a wide margin.\n\n\nCode\ncountry_totals |&gt;\n  filter(type %in% c(\"Imports\", \"Exports\")) |&gt; \n  pivot_longer(cols = contains(\"201\"), names_to = \"year\", values_to = \"gwh\") |&gt;\n  filter(!is.na(gwh)) |&gt; \n  group_by(country, country_name, type) |&gt; \n  summarise(gwh = sum(gwh), .groups = \"drop\") |&gt; \n  pivot_wider(names_from = type, values_from = gwh) |&gt; \n  clean_names() |&gt; \n  filter(!(exports == 0 & imports == 0)) |&gt; \n  mutate(total = exports + imports, net = exports - imports, type = if_else(net &gt; 0, \"Net Exporters\", \"Net Importers\")) |&gt; \n  group_by(type) |&gt; \n  slice_max(n = 10, order_by = abs(net)) |&gt; \n  mutate(country_name = fct_reorder(country_name, abs(net))) |&gt; \n  ungroup() |&gt; \n  filter(country_name != \"Other\") |&gt; \n  ggplot(aes(y = country_name)) +\n  geom_col(aes(x = imports),  fill = \"#31a354\") + \n  geom_text_repel(\n    aes(x = imports, label = label_number(scale = 1/1000, suffix = \"K\")(imports)), \n    nudge_x = 1000, size = 3, fontface = \"bold\", direction = \"x\") +\n  geom_col(aes(x = -exports), fill = \"#de2d26\") +\n  geom_text_repel(\n    aes(x = -exports, label = label_number(scale = 1/1000, suffix = \"K\")(-exports)), \n    nudge_x = -1000, size = 3, fontface = \"bold\", direction = \"x\") +\n  scale_x_continuous(labels = label_number(scale = 1/1000, accuracy = .1, suffix = \"K\")) +\n  facet_wrap(~type, scales = \"free\") + \n  labs(title = \"European power imports / exports (GWh)\", y = NULL, x = \"Power (GWh)\",\n       subtitle = \"Germany & France are big net exporters, Italy is a big net importer \") +\n  theme_blog_facet() \n\n\n\n\n\nItaly’s net imports are larger than the next two countries (Finland & UK) combined!\n\n\n\n\n\nEnergy Types\nSwitching gears, we can get a better understanding of the country totals by looking at the production details in the energy types data. The majority of power produced came from conventional thermal sources (i.e. fossil fuels), but it did have a slight decline in 2018. A few of the renewable categories saw significant growth (e.g. wind, solar, geothermal), although the impact in total power produced for those categories was still small.\n\n\nCode\ntotals_by_type &lt;- energy_level1_types_totals |&gt; \n  mutate(type = fct_reorder(type, gwh)) |&gt;  \n  ggplot(aes(x = gwh, y = type)) +\n  geom_col() +\n  scale_x_continuous(labels = scales::label_number(scale = 1/1000000, suffix = \"M\")) +\n  geom_label(\n    aes(label = scales::label_percent()(pct_of_total)),\n    nudge_x = 290000,\n    size = 3,\n    label.size = .1,\n    label.padding = unit(0.2, \"lines\")\n  ) +\n  labs(\n    title = \"European power production\",\n    subtitle = \"Dirty engergy (convential thermal) accounts for nearly half of all power produced \",\n    y = \"Type\",\n    x = \"Total Power (gigawatt hours)\"\n  ) +\n  theme_blog()\ntotals_by_year &lt;- energy_types |&gt; \n  filter(level == \"Level 1\") |&gt; \n  group_by(year) |&gt;\n  group_by(year, type) |&gt; \n  summarise(\n    gwh = sum(gwh, na.rm = T),\n    .groups = \"drop\"\n  ) |&gt; \n  mutate(\n    type = fct_reorder(type, -gwh, sum)\n  ) |&gt; \n  ggplot(aes(x = year, y = gwh)) +\n  geom_col() +\n  scale_y_continuous(labels = comma) +\n  facet_wrap(~type, scales = \"free_y\") +\n  labs(\n    title = NULL,\n    subtitle = \"Wind, solar, & geothermal are slowly taking a small share of overall production\",\n    y = \"Power (GWh)\",\n    x = \"Year\",\n    fill = \"Type\"\n  ) +\n  theme_blog_facet() +\n  theme(legend.position = \"bottom\")\n(totals_by_type / plot_spacer() / totals_by_year) +\n  plot_layout(heights = c(1, .1, 4))\n\n\n\n\n\nTo better understand the changes we saw in the category totals, we can use a slope graph for each country. Given there are 37 countries and 7 types, it gets a bit crowded to print the country names across a faceted plot. We can use ggflags instead of spelled out names to plot the flags as points. It’s not perfect because most people won’t know all 37 flags, but it works. Additionally, given the skew in power production we can use a log10 scale to get some separation between countries (at least a bit more). The plot shows the top few producers in each type pretty nicely, but we can use ggiraph to make the graph a bit more readable by adding a tooltip and an inverse hover opacity on other countries. Unfortunately, I couldn’t figure out how to attach the tooltip to the flags in ggiraph so the hover only works on the slope line. Not perfect, but still pretty nifty. Germany accounts for a lot of the change because they lowered Conventional thermal and added Wind and Solar. Similar trends showed up in France, Ukraine, The Netherlands, and a few others. Turkey added production in all categories except hydro which they reduced. All these changes seem like they could reflect strategies related to global climate change.\n\n\nCode\np &lt;- energy_types |&gt; \n  filter(level == \"Level 1\") |&gt; \n  group_by(country_name, type) |&gt; \n  mutate(country_type_total = sum(gwh),\n         change = gwh - lag(gwh, n = 2L, order_by = year),\n         pct_change = (gwh - lag(gwh, n = 2L, order_by = year)) / lag(gwh, n = 2L, order_by = year)) |&gt; \n  ungroup() |&gt; \n  mutate(\n    label = glue::glue(\"{country_name}\\n{label_comma(accuracy = 1)(change)}{if_else(change &gt; 0, ' GWh increase', ' GWh decrease')} ({label_percent(accuracy = .1)(pct_change)})\")\n  ) |&gt;\n  group_by(country_name, type) |&gt; \n  mutate(total_gwh = sum(gwh)) |&gt; \n  ungroup() |&gt; \n  filter(year %in% c(2016, 2018), total_gwh &gt; 500) |&gt; \n  mutate(label_2016 = lead(label)) |&gt; \n  mutate(label = if_else(year == 2016, label_2016, label))  |&gt; \n  ggplot(aes(x = year, y = gwh, group = country, country = country, data_id = country)) +\n  geom_line_interactive(aes(tooltip = label)) +\n  geom_flag(size = 3.5) +\n  scale_y_log10(labels = comma) +\n  scale_x_continuous(breaks = c(2016, 2018),\n                     limits = c(2015, 2019)) +\n  labs(y = \"Power Produced (GWh)\", x = \"Year\") +\n  facet_wrap(~type, scales = \"free_y\") +\n  theme_blog_facet() +\n  theme(legend.position = \"bottom\")\ngirafe(\n  ggobj = p,\n  width_svg = 8,\n  height_svg = 9,\n  options = list(\n    opts_hover_inv(css = \"opacity:0.1;\"),\n    opts_hover(css = \"stroke-width:2; stroke:#ff0000;\")\n  )\n)\n\n\n\n\n\n\n\nCircular Lollipop Chart\nGiven the trends, it seems interesting to look at how much power is coming from clean sources versus dirty sources. The definition of clean and dirty is highly controversial and really a topic for a different blog (and data set), but we can try to group power sources based on their relative carbon footprint which basically puts everything but conventional thermal in a low carbon group (assuming other is probably biofuel or something like that, it’s so small it won’t make a difference). I understand this grouping is very crude. I’d like to approximate carbon footprint per GWh for the types and do some carbon estimations, but many of the groups would make that very difficult. Conventional thermal is a super-group of the major polluters (where much of the carbon comes from), but the different types within that group vary a great deal (i.e. natural gas vs. coal). We have no info about the Other group so we can only guess and it’s likely many types as well… anyhow, all this to say - please excuse the over simplicity of clean vs. dirty for the purposes of this post.\n\n\n\n\nCode explainer\nThere are many ways we could view this data. Typically, we’d want to look at this as a time series to understand the impacts of change, but three years of this data just wasn’t long enough with this data set for that to be useful. Initially, I thought it’d be interesting to do a rose chart 1 splitting up clean and dirty, but I decided to use a lollipop 2 instead because it ended up being a very interesting visual. Once I got the chart in place, I realized there was still a lot of context I wanted to surface so I embedded a flextable 3 into ggiraph’s tooltip. 4\n\n\nVisual Explainer\nEach country in the data set has a spoke on the wheel. The height of the point (lollipop) on each spoke represents the percent of total power produced that came from clean sources. Albania had 100 percent of their power come from clean sources and Malta had 0 percent come from clean sources. The order of the countries is by total power produced starting with Germany and running clockwise. The size of the point also provides this for scale. The color of the point represents the change in dirty power produced from 2016 through 2018. Red points increased overall production from dirty sources and blue dots reduced overall production from dirty sources. Finally, when hovering over the dot, a tooltip shows a table with a scaled bar chart of the data for that country. Each bar is a year of power production, if the bar goes up it means that year was larger relative to the other years and if it goes down it was lower relative to the other years. The fill (color) groups the bars based on their relative position (up or down compared to other years).\n\n\nCode\n# Setup df ####\nlow_carbon &lt;- c(\"hydro\", \"wind\", \"solar\", \"nuclear\", \"geothermal\", \"other\")\nhigh_carbon &lt;- \"conventional thermal\"\nlolli_df &lt;- energy_types |&gt; \n  mutate(carbon_level = case_when(\n    str_to_lower(type) %in% low_carbon ~ \"low\",\n    str_to_lower(type) %in% high_carbon ~ \"high\",\n    T ~ \"Error\")) |&gt; \n  group_by(country, country_name, year, carbon_level) |&gt; \n  summarise(gwh = sum(gwh), .groups = \"drop\") |&gt; \n  pivot_wider(id_cols = c(country, country_name), names_from = c(carbon_level, year), values_from = gwh) |&gt; \n  mutate(total_2016 = low_2016 + high_2018,\n         total_2017 = low_2017 + high_2017,\n         total_2018 = low_2018 + high_2018,\n         pct_low_2016 = low_2016 / total_2016,\n         pct_low_2018 = low_2018 / total_2018,\n         high_change = (high_2018 - high_2016) / high_2016,\n         country_name = fct_reorder(country_name, -total_2018, sum),\n         segment = row_number(-total_2018)) |&gt; \n  mutate(high_change = if_else(is.nan(high_change), 0, high_change))\n# Create tooltip ####\ngg_bars &lt;- function(z) {\n  z &lt;- scale(z)\n  z &lt;- na.omit(z)\n  z &lt;- data.frame(x = seq_along(z), z = z, w = z &lt; 0)\n  ggplot(z, aes(x = x, y = z, fill = w)) +\n    geom_col(show.legend = FALSE) +\n    geom_hline(aes(yintercept = 0), alpha = .9, color = \"#525252\", size = .25) +\n    scale_fill_discrete_diverging(palette = \"Green-Orange\",  rev = T) +\n    theme_void() \n}\nlolli_df$tooltip &lt;- map_chr(unique(lolli_df$country), function(x){\n  lolli_data &lt;- lolli_df |&gt; \n    filter(country == x) |&gt; \n    pivot_longer(cols = contains(\"201\"), names_to = \"type_year\", values_to = \"gwh\") |&gt; \n    select(-high_change, -segment) |&gt; \n    filter(!str_detect(type_year, \"pct\")) |&gt;\n    tidyr::separate(col = type_year, sep = \"_\", into = c(\"type\", \"year\")) |&gt; \n    mutate(year = as.integer(year)) |&gt; \n    mutate(type = case_when(\n      type == \"high\" ~ \"Dirty Sources\",\n      type == \"low\" ~ \"Clean Sources\",\n      T ~ \"Total\"\n    ))\n  bar_dt &lt;- energy_types |&gt; \n    select(-level) |&gt; \n    filter(country == x) |&gt; \n    bind_rows(lolli_data) |&gt; \n    mutate(type_order = case_when(\n      str_to_lower(type) %in% high_carbon ~ 1,\n      type == \"Dirty Sources\" ~ 2,\n      str_to_lower(type) %in% low_carbon ~ 3,\n      type == \"Clean Sources\" ~ 4,\n      type == \"Total\" ~ 5\n    )) |&gt; \n    mutate(Type = fct_reorder2(type, year, type_order, min)) |&gt; \n    rename(\"GWh\" = \"gwh\")\n  bar_dt &lt;- as.data.table(bar_dt, key = c(\"type_order\", \"year\"))\n  z &lt;- bar_dt[,\n              lapply(.SD, function(x) list(gg_bars(x))),\n              by = c(\"Type\"), .SDcols = c(\"GWh\")\n  ]\n  ft &lt;- flextable(z)\n  ft &lt;- compose(ft, \n                j =  c(\"GWh\"),\n                value = as_paragraph(gg_chunk(value = ., height = .25, width = 1)),\n                use_dot = TRUE\n  )\n  ft &lt;- compose(ft, \n                j =  c(\"GWh\"),\n                part = \"header\",\n                value = as_paragraph(\n                  \"GWh \",\n                  as_chunk(\n                    \" (2016-2018)\",\n                    props = fp_text_default(font.size = 9, vertical.align = \"superscript\")\n                  )\n                )\n  )\n  ft &lt;- add_header_lines(ft, values = \"Yearly Change in Production (GWh)\")\n  ft &lt;- bold(ft, part = \"header\", bold = TRUE)\n  ft &lt;- ft |&gt;\n    bold(~Type %in% c(\"Dirty Sources\", \"Clean Sources\", \"Total\")) |&gt; \n    hline(i = 2, border = fp_border(color = \"#525252\", width = 1)) |&gt; \n    hline(i = 9, border = fp_border(color = \"#525252\", width = 1)) |&gt; \n    hline(i = 10, border = fp_border(color = \"#525252\", width = 1)) \n  ft &lt;- set_table_properties(ft, layout = \"autofit\")\n  ft &lt;- as.character(htmltools_value(ft, ft.shadow = FALSE))\n  return(ft)\n})\n# Create Plot ####\nhigh_change_limits &lt;- c(min(lolli_df$high_change), max(lolli_df$high_change))\np &lt;- lolli_df |&gt; \n  ggplot(aes(x = segment))  +\n  geom_hline(aes(yintercept = 100), alpha = .9, color = \"#525252\") +\n  geom_hline(aes(yintercept = 50), alpha = .5, color = \"#858585\", linetype = \"twodash\") +\n  geom_hline(aes(yintercept = 0), alpha = .9, color = \"#525252\") +\n  geom_segment(\n    aes(xend = segment, y = 0, yend = pct_low_2018 * 100), \n    size = .25, color = \"#525252\", alpha = .9\n  ) +\n  geom_segment(\n    aes(xend = segment, y = pct_low_2018 * 100, yend = 110), \n    size = .25, color = \"#525252\", alpha = .25, linetype = \"twodash\"\n  ) +\n  geom_text(aes(x = segment, y = 112, label = country_name), family = \"Lato\", size = 5, fontface = \"bold\") +\n  annotate(\"text\", x = 0, y = 96, label = \"100%\", alpha = .75, size = 5, family = \"Lato\", fontface = \"bold\") +\n  annotate(\"text\", x = 0, y = 46, label = \"50%\", alpha = .75, size = 5, family = \"Lato\", fontface = \"bold\") +\n  geom_point_interactive(aes(y = pct_low_2018 * 100, size = total_2018, color = high_change, tooltip = tooltip, data_id = country)) +\n  scale_color_binned_diverging(n.breaks = 5, labels = c(\"-50%\", \"\", \"0%\", \"\", \"150%\"), c1 = 200, cmax = 200, palette = \"Blue-Red-2\", n_interp = 7, mid = 0) + \n  scale_size_continuous(labels = label_number(accuracy = 1, n.breaks = 7, scale = 1/1000, suffix = \"K\"), range = c(1, 12)) +\n  labs(title = \"Low carbon power production (percent of total)\",\n       subtitle = str_wrap(width = 125, \"Many countries produced &gt; 50% of power w/ clean sources, but many also added to their over overall dirty energy production\"),\n       size = \"2018 Power (GWh)\", color = \"Change in Dirty Power (GWh) 2016-2018\",\n       caption = \"Clean: hydro, wind, solar, nuclear, geomthermal, other\\nDirty: conventional thermal \"\n  ) +\n  coord_polar() +\n  ylim(-20, 115) +\n  theme_void() +\n  theme(\n    plot.title = element_text(family = \"Lato\", size = 24, face = \"bold\"),\n    plot.caption = element_text(family = \"Lato\", size = 10),\n    plot.subtitle = element_text(family = \"Lato\", size = 14),\n    legend.position = \"bottom\", \n    legend.title = element_text(family = \"Lato\", size = 12, face = \"bold\"),\n    legend.text = element_text(size = 12))\n# Print plot #### \ngirafe(ggobj = p, fonts = list(sans = \"Roboto\"), width_svg = 16, height_svg = 16,\n       options = list(\n         opts_tooltip(css = \"padding:5px;background:white;border-radius:2px 2px 2px 2px;\"),\n         opts_hover_inv(css = \"opacity:0.5;\"),\n         opts_hover(css = \"stroke-width:2;\")\n       ))\n\n\n\n\n\n\n\n\n\n\nNot another pie chart\nWhile building the above lollipop chart, I had the idea to create a shaded area in the negative space above the dot (i.e. the dirty energy) and I stumbled on another way to make a pie chart that’s h harder than simply using geom_col. It also has a small sensless pie chart in the center, I believe because coord_polar couldn’t fold a rectangle into those slices evenly? I decided not to finish the envisioned shaded area because the visual was already feeling a bit too complicated as it stood, but I wanted to keep this failed attempt because it was a good learning experience.\n\n\nCode\nenergy_types |&gt; \n  group_by(country, country_name, type) |&gt; \n  summarise(gwh = sum(gwh), .groups = \"drop\") |&gt; \n  pivot_wider(id_cols = c(country, country_name), names_from = type, values_from = gwh) |&gt; \n  clean_names() |&gt; \n  mutate(\n    total = hydro+wind+solar+geothermal+nuclear+other+conventional_thermal,\n    low_carbon = hydro+wind+solar+geothermal+nuclear+other, \n    high_carbon = conventional_thermal,\n    pct_low_carbon = low_carbon/(low_carbon+high_carbon)\n  ) |&gt; \n  mutate(\n    country_name = fct_reorder(country_name, -pct_low_carbon, sum),\n    segment = row_number(-pct_low_carbon)) |&gt; \n  ggplot(aes(x = segment, y = pct_low_carbon)) +\n  geom_rect(\n    aes(xmin = segment, xmax = segment + 1, fill = pct_low_carbon), \n    ymin = 0, ymax = 1, alpha = 1\n  ) + \n  geom_text(aes(y = 1.2, label = country_name), family = \"Lato\", size = 2, fontface = \"bold\") +\n  scale_fill_continuous_sequential(labels = label_percent(accuracy = 1)) +\n  labs(fill = \"Percent Clean Sources\") +\n  coord_polar() +\n  theme_void() +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nObligatory map\nA map because people love maps! Seriously though, mostly because as I was working on this I stumbled back across the rnaturalearth, which is an amazing package to keep in the toolkit for spatial data. It provides easy to use data frames with sf data, census data, reference data, etc. In this case, I used the ne_countries() function to get country data that I could join with my energy data frame. There’s a lot more analysis we could do with the data it provided, but below is a choropleth map of the percent of total power from clean sources. 5\n\n\nCode\nworld_sf &lt;- ne_countries(scale = 50, returnclass = 'sf') \nenergy_sf &lt;- inner_join(world_sf,\n           energy_types |&gt;\n             group_by(country, country_name, type) |&gt;\n             summarise(gwh = sum(gwh), .groups = \"drop\") |&gt; \n             pivot_wider(id_cols = c(country, country_name), names_from = type, values_from = gwh) |&gt; \n             clean_names() |&gt; \n             mutate(\n               total = hydro+wind+solar+geothermal+nuclear+other+conventional_thermal,\n               low_carbon = hydro+wind+solar+geothermal+nuclear+other, \n               high_carbon = conventional_thermal,\n               pct_low_carbon = low_carbon/(low_carbon+high_carbon)\n             ) |&gt; \n             mutate(\n               country_name = fct_reorder(country_name, -pct_low_carbon, sum),\n               segment = row_number(-pct_low_carbon),\n               country = str_to_upper(if_else(country == \"rs\", \"yf\", country)),\n               label = glue::glue(\"{label_percent(accuracy = .1)(pct_low_carbon)}\")),\n           by = c(\"wb_a2\" = \"country\")) |&gt; \n  sf::st_transform('+proj=longlat +datum=WGS84')\nfill_color &lt;- colorNumeric( palette=\"viridis\", domain=energy_sf$pct_low_carbon, na.color=\"transparent\", reverse = T)\nenergy_sf |&gt; \n  leaflet() |&gt; \n  addTiles() |&gt;\n  setView(lng = 9.8, lat = 53.41, zoom = 3) |&gt; \n  addPolygons(\n    label = ~label,\n    labelOptions = labelOptions( \n      style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"), \n      textsize = \"13px\", \n      direction = \"auto\"\n    ),\n    fillColor = ~fill_color(pct_low_carbon),\n    stroke = T, \n    color = \"grey\", \n    weight = .5) |&gt; \n  addLegend(\n    bins = 5,\n    pal = fill_color,\n    values = ~pct_low_carbon,\n    opacity = .5,\n    title = \"Clean Prouction\",\n    position = \"bottomleft\",\n    labFormat = labelFormat(\n      digits = 2,\n      suffix = \"%\",\n      transform = function(x){x * 100}))"
  },
  {
    "objectID": "posts/2021-08-13-european-energy/index.html#wrap-up",
    "href": "posts/2021-08-13-european-energy/index.html#wrap-up",
    "title": "European Energry",
    "section": "Wrap-up",
    "text": "Wrap-up\nWe could take this analysis a lot further, but I learned a few new things in this exercise. It was my first time using ggflags and I worked with polar coordinates in ggplot2 quite a bit. Based on our analysis, it looks like Europe (as of a couple years ago) has quite a lot of ground to cover in reducing their carbon impact - not unlike the rest of us."
  },
  {
    "objectID": "posts/2021-08-13-european-energy/index.html#footnotes",
    "href": "posts/2021-08-13-european-energy/index.html#footnotes",
    "title": "European Energry",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis stackoverflow post provides the gist of what’s needed to create a rose chart.↩︎\nThis stackoverflow post gave a stupid simple example for creating a lollipop chart.↩︎\nThe flextable book shows how to embed ggplot2 charts into the table using data.table .SD.↩︎\nThe ggiraph repo has a closed issue that explains how to embed a flextable into the tooltip.↩︎\nA quick example of a choropleth with leaflet↩︎"
  },
  {
    "objectID": "posts/2020-10-26-great-american-beer-festival/index.html#background",
    "href": "posts/2020-10-26-great-american-beer-festival/index.html#background",
    "title": "Great American Beer Festival",
    "section": "Background",
    "text": "Background\n\nTidy Tuesday\nThis is my first published of hopefully many Tidy Tuesday analysis posts. Tidy Tuesday is a weekly social data project in the R for Data Science community where R users explore a new dataset and share their findings. If you’re an R user (or aspiring) I highly recommend participating. A few specific Tidy Tuesday resources I’d recommend are David Robinson’s Tidy Tuesday screen casts, Twitter #TidyTuesday, and the TidyTuesday podcast with Jon Harmon. All Tidy Tuesday datasets are available on Github.\n\n\nThe Great American Beer Festival\nThis Tidy Tuesday, we’re analyzing data from The Great American Beer Festival. The Great American Beer Festival (GABF) is a three-day annual festival in Denver, Colorado. Judges evaluate several thousand beers entered by hundreds of breweries and award gold, silver, and bronze medals in 100+ categories - though not every medal is necessarily awarded in each category. GABF was founded in 1982 and had 22 participating breweries in the first year. To download the data I use the tidytuesdayR package."
  },
  {
    "objectID": "posts/2020-10-26-great-american-beer-festival/index.html#analysis",
    "href": "posts/2020-10-26-great-american-beer-festival/index.html#analysis",
    "title": "Great American Beer Festival",
    "section": "Analysis",
    "text": "Analysis\nThe GABF data set has an observation (row) for each beer that received an award for each year it received one. The obvious challenge with that is the dataset only includes beers awarded - it provides no data regarding participation. Without participation data many questions will go unanswered. We can’t infer the overall quality of beers based on total awards because we won’t know how many times they didn’t win. Huge bummer because many of the questions that came to mind when I saw the name of the dataset won’t be possible, c’est la vie.\nI summarized the data set with skimr. The output allowed me to quickly get an understanding of the dataset and identify some data cleaning tasks. The data types of the columns are all character, except year which is double. I converted medal to factor so I could more easily analyze it as an ordinal attribute. State has 52 unique values, but I spotted duplicate records due to casing (AK & Ak and WA & wa). I changed the casing in these observations bringing the number of states to 50, including Washington D.C. meaning one state has no awards. Medal is a character which makes sense, but I added a numeric version in case I want to do some weighted totals.\n\n\nCode\nskimr::skim(gabf)\n\n\n\nData summary\n\n\nName\ngabf\n\n\nNumber of rows\n4970\n\n\nNumber of columns\n7\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nmedal\n0\n1\n4\n6\n0\n3\n0\n\n\nbeer_name\n0\n1\n2\n89\n0\n3811\n0\n\n\nbrewery\n0\n1\n6\n58\n0\n1859\n0\n\n\ncity\n0\n1\n3\n44\n0\n803\n0\n\n\nstate\n0\n1\n2\n2\n0\n52\n0\n\n\ncategory\n0\n1\n4\n76\n0\n515\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nyear\n0\n1\n2007.88\n8.68\n1987\n2002\n2009\n2015\n2020\n▂▃▅▆▇\n\n\n\n\n\nCode\ngabf &lt;- gabf %&gt;% \n  mutate(medal = fct_relevel(medal, c(\"Bronze\", \"Silver\"))) %&gt;% \n  mutate(state = str_to_upper(state)) %&gt;%\n  mutate(medal_numeric = if_else(medal == \"Gold\", 3, if_else(medal == \"Silver\", 2, 1)))\n\n\nI visualized awards over time and saw the dataset starts in 1987 (27 awards) and ends in 2020 (218 awards). Growth appears to be linear with only a few years that ever decreased.\n\n\nCode\ngabf %&gt;% \n  group_by(year) %&gt;% \n  tally() %&gt;% \n  ggplot(aes(x = year, y = n)) +\n  geom_col(width = .75) +\n  labs(y = \"Awards\", x = \"Year\", title = \"Total Awards by Year\") +\n  theme_blog() \n\n\n\n\n\nThe annual growth in the number of awards appears to be similar for each medal class.\n\n\nCode\ngabf %&gt;% \n  group_by(year, medal) %&gt;% \n  tally() %&gt;% \n  ggplot(aes(x = year, y = n, color = medal)) +\n  geom_line() +\n  labs(x = \"Year\", y = \"Awards\", color = \"Medal\", title = \"Medals by Year\") + \n  theme_blog()  \n\n\n\n\n\nThe dataset has 515 different award categories, which appear to be different categories of beer. That’s more than I expected. Looking at the top 5 categories table, the category with the most awards is the Classic Irish-Style Dry Stout with 62. That accounts for 0.01 percent of the awards and less than two awards per year - meaning there aren’t categories that have been used throughout the 34 years. Given that, the category attribute will be difficult to use because it’s probably very inconsistent year to year.\n\nTop 5 Beer Categories\n\n\nCode\ncats &lt;- gabf %&gt;% \n  mutate(category = forcats::fct_lump(category, n = 5)) %&gt;% \n  group_by(category) %&gt;% \n  tally()\nother_cat &lt;- cats %&gt;%\n  filter(category == \"Other\")\n\ncats %&gt;% \n  filter(category != \"Other\") %&gt;% \n  arrange(dplyr::desc(n)) %&gt;% \n  bind_rows(., other_cat) %&gt;% \n  kableExtra::kable(col.names = c(\"Category\", \"Awards\")) %&gt;% \n  kableExtra::kable_styling(bootstrap_options = \"hover\") \n\n\n\n\n\nCategory\nAwards\n\n\n\n\nClassic Irish-Style Dry Stout\n62\n\n\nAmerican-Style Pale Ale\n61\n\n\nBock\n61\n\n\nRobust Porter\n61\n\n\nImperial Stout\n60\n\n\nOther\n4665\n\n\n\n\n\n\n\nThe other attributes in my summary table (beer_name, brewery, and city) all appear to be formatted correctly, but quite disperse I won’t inspect them any further at the moment.\nState is one attribute that seemed clean so I decided to take a closer look at that. The top 5 states account for almost 50 percent of all the awards. Two of the top five states are California and Texas, which are the two most populous states in the US.\n\n\nTop 5 States\n\n\nCode\nstates &lt;- gabf %&gt;%\n  mutate(state = forcats::fct_lump(state, n = 5)) %&gt;%\n  group_by(state) %&gt;%\n  tally()\nothers &lt;- states %&gt;%\n  filter(state == \"Other\")\n\nstates %&gt;% \n  filter(state != \"Other\") %&gt;% \n  arrange(dplyr::desc(n)) %&gt;%\n  bind_rows(., others) %&gt;% \n  kableExtra::kable(col.names = c(\"State\", \"Awards\")) %&gt;% \n  kableExtra::kable_styling(bootstrap_options = \"hover\")\n\n\n\n\n\nState\nAwards\n\n\n\n\nCA\n962\n\n\nCO\n659\n\n\nOR\n328\n\n\nTX\n249\n\n\nWI\n234\n\n\nOther\n2538\n\n\n\n\n\n\n\nAnalyzing the states effectively will require population data to control for or consider per capita. Any mapping will require some sort of GIS data. These attributes are often packaged together because they’re used together frequently. There are a number of R libraries we can use. The most robust one I know off hand is tidycensus, which is a wrapper around the US Census Bureau’s API. I mention it for reference, but for simplicity I used albersusa. It has simple features which is the GIS data used by R’s sf package.\n\n\nJoining albersusa data\nThe albersusa usa_sf() function pulls a state dataset from the package which includes population and simple features. I wrote a function that takes a data frame, which I can reuse easily. I know I’ll likely make more changes to gabf and changes after my awards_by_state function runs so create an ephemeral function just makes it a bit easier than managing multiple versions of data frames in my experience. I added some state level measurements that I assume we’d use later.\n\n\nCode\nawards_by_state &lt;- . %&gt;%\n  group_by(state) %&gt;%\n  summarise(state_total_awards = n(), \n            state_total_awards_weighted = sum(medal_numeric),\n            state_years_with_awards = n_distinct(year)) %&gt;% \n  ungroup() %&gt;% \n  right_join(tigris::shift_geometry(albersusa::usa_sf(\"laea\")), by = c(\"state\"=\"iso_3166_2\")) %&gt;%\n  rename(\"state_name\"=\"name\") %&gt;% \n  mutate(state_avg_award = state_total_awards_weighted / state_total_awards,\n         state_total_awards_per_cap = (state_total_awards / pop_2014) *100000,\n         state_percent_total_awards = state_total_awards / sum(state_total_awards, na.rm = T)) %&gt;% \n  replace_na(list(state_total_awards = 0, state_total_awards_weighted = 0, avg_award = 0, state_total_awards_per_cap = 0, state_percent_total_awards = 0, state_years_with_awards = 0)) \n\n\n\n\nState Rankings Table\nI wanted to view the state data with the populations and the new measures. I used a table view and some row-wise summaries to get more context for the top states and bottom states. I used the reactable pacakge because I could visualize row-wise summaries. It’s a lot of code, but most of it is pretty simple to adjust - I worked off of the package’s Twitter Followers demo and Women’s World Cup Predictions demo.\nThe table shows each state’s population, share of total awards, awards per capita (PC), and the average medal (Avg). The table is sorted by the awards per capita (PC). The table provides a few new insights. A few low population states (e.g. Wyoming, Alaska, & Delaware) have a disproportionate number of awards. Alternatively, a few high population states are not well represented including New York, Georgia, and most of all Florida (no shock). Most states have a centered distribution of medals (average silver) with the exceptions mostly being states that have a sparse number of awards.\n\n\n\nCode\nmake_color_pal &lt;- function(colors, bias = 1) {\n  get_color &lt;- colorRamp(colors, bias = bias)\n  function(x) rgb(get_color(x), maxColorValue = 255)\n}\noff_rating_color &lt;- make_color_pal(c(\"#f93014\", \"#f8fcf8\", \"#4DBE56\"), bias = 1.3)\nrating_column &lt;- function(maxWidth = 55, ...) {\n  colDef(maxWidth = maxWidth, align = \"center\", class = \"cell number\", ...)\n}\n\ntbl &lt;- gabf %&gt;% \n  awards_by_state() %&gt;% \n  select(state_name, pop_2014, state_percent_total_awards, state_total_awards_per_cap, state_avg_award) %&gt;%\n  reactable(\n    height = 550,\n    striped = TRUE,\n    defaultPageSize = 51,\n    defaultSorted = \"state_total_awards_per_cap\",\n    defaultColDef = colDef(headerClass = \"reactable-header\", align = \"left\"),\n    columns = list(\n      state_name = colDef( \n        name = \"State\",\n        width = 150\n      ),\n      pop_2014 = colDef(\n        name = \"Population (2014)\",\n        cell = function(value) {\n          width &lt;- paste0(value * 100 / max(.$pop_2014), \"%\")\n          value &lt;- format(value, big.mark = \",\")\n          value &lt;- format(value, width = 14, justify = \"right\")\n          bar &lt;- div(\n            class = \"reactable-bar-chart\",\n            style = list(marginRight = \"6px\"),\n            div(class = \"reactable-bar\", style = list(width = width, backgroundColor = \"#3fc1c9\"))\n          )\n          div(class = \"reactable-bar-cell\", span(class = \"reactable-number\", value), bar)\n        }\n      ),\n      state_percent_total_awards = colDef(\n        name = \"Percent of Total Awards\",\n        cell = JS(\"function(cellInfo) {\n          // Format as percentage\n          const pct = (cellInfo.value * 100).toFixed(1) + '%'\n          // Pad single-digit numbers\n          let value = pct.padStart(5)\n          // Render bar chart\n          return (\n            '&lt;div class=\\\"reactable-bar-cell\\\"&gt;' +\n              '&lt;span class=\\\"reactable-number\\\"&gt;' + value + '&lt;/span&gt;' +\n              '&lt;div class=\\\"reactable-bar-chart\\\" style=\\\"background-color: #e1e1e1\\\"&gt;' +\n                '&lt;div class=\\\"reactable-bar\\\" style=\\\"width: ' + pct + '; background-color: #fc5185\\\"&gt;&lt;/div&gt;' +\n              '&lt;/div&gt;' +\n            '&lt;/div&gt;'\n          )\n        }\"),\n        html = TRUE\n      ),\n      state_total_awards_per_cap = rating_column(\n        name = \"PC\",\n        defaultSortOrder = \"desc\",\n        cell = function(value) {\n          scaled &lt;- (value - min(.$state_total_awards_per_cap)) / (max(.$state_total_awards_per_cap) - min(.$state_total_awards_per_cap))\n          color &lt;- off_rating_color(scaled)\n          value &lt;- format(round(value, 1), nsmall = 1)\n          div(class = \"reactable-per-capita\", style = list(background = color), value)}\n      ),\n      state_avg_award = rating_column(\n        name = \"Avg\",\n        cell = function(value) format(round(value, digits = 2), nsmall = 2)\n      )\n    ),\n    compact = TRUE,\n    class = \"reactable-tbl\")\n\ndiv(class = \"reactable-tbl-view\",\n    div(class = \"div-subtitle\",\n        div(class = \"div-title\", \"Total GABF Awards by State, Population Adjusted (1987 - 2020)\"),\n        \"When reviewing total awards received per capita, Colorado, Wyoming, Alaska, Oregon, & Montana look like the big winners\"\n    ),\n    tbl\n)\n\n\n\n\n\nTotal GABF Awards by State, Population Adjusted (1987 - 2020)\nWhen reviewing total awards received per capita, Colorado, Wyoming, Alaska, Oregon, & Montana look like the big winners\n\n\n\n\n\n\n\nColorado is a great state for beer, but I would not expect it to be distinguished as a clear leader among the states. Similar story with Wyoming. Again, the data set does not provide information regarding participation. I suspect there is skew in the number of awards won by state because the festival is always held in Denver. Breweries located closer to Denver are more likely to participate.\nSeparately, time obviously plays a significant role that is not captured by the above table. The festival is older than most of the breweries that participated in 2020. The competition in 2020 was probably completely different, more crowded, than it was in 1987. When looking at total awards, we’re not accounting for that.\n\nBrief USA Beer History\nA little history on brewing in the US helps us better understand the time variable in the absencense of participation data. Jimmy Carter signed HR 1337 into law which made it explicitly legal to homebrew beer. When that law was passed, there were only ~50 breweries in the USA. Today, there are ~7k permitted breweries. You can read more in this interesting article on Vinepair. With that in mind, when the festival started breweries like Anheuser-Busch, Miller Brewing Company, and Coors Brewing Company controlled even more of the market share than they do today. The newcomers at that time were Boston Beer Company (i.e. Sam Adams) and Alaskan Brewing Co., founded in 1984 and 1986 respectively. That history plays a significant role in our analysis because the newer breweries of today are years behind these older breweries with regard to winning awards. \n\n\n\nAnimated Map\nI visualized the award data in a Choropleth map. To capture the year data, I used gganimate to loop through and create a map for each year in a gif. To scale the color of the Choropleth by the specific year, rather than by the entire 34 years, I updated the awards_by_state function to include year (awards_by_state_year). It’s a bit of code to calculate different sums and counts for each level of aggregation and to impute records for missing years, but dplyr and tidyr do the heavy lifting. Nesting the annual observations in a data frame column keeps the data frame at 51 rows and prevents duplicating the simple features data for each year. The ggplot aesthetics will not take the nested data, but it’s easier to work with the data frame that way for other purposes and the function will probably be handy later. Below is the function spelled out.\n\n\nCode\n# Creates a column with the year total, every row with the same year has the same value\nawards_by_state_year &lt;- . %&gt;%\n  add_count(year, name = \"year_total_awards\")\n\n# Aggregates by year, state, and year total and counts the number of rows\nawards_by_state_year &lt;- . %&gt;%\n  add_count(year, name = \"year_total_awards\") %&gt;% \n  group_by(state, year, year_total_awards) %&gt;% \n  summarise(state_year_total_awards = n()) %&gt;%\n  ungroup()\n\n# Joins missing states (from state.abb) and imputes the missing years and zero awards for each\nawards_by_state_year &lt;- . %&gt;%\n  add_count(year, name = \"year_total_awards\") %&gt;% \n  group_by(state, year, year_total_awards) %&gt;% \n  summarise(state_year_total_awards = n()) %&gt;%\n  ungroup() %&gt;% \n  full_join(tibble(state = state.abb), by = c(\"state\" = \"state\")) %&gt;% \n  replace_na(list(year = 1987)) %&gt;% \n  complete(state, nesting(year)) %&gt;% \n  replace_na(list(state_year_total_awards = 0))\n\n# Imputes year_total_awards for the new years that were imputed in the last step  \nawards_by_state_year &lt;- . %&gt;%\n  add_count(year, name = \"year_total_awards\") %&gt;% \n  group_by(state, year, year_total_awards) %&gt;% \n  summarise(state_year_total_awards = n()) %&gt;%\n  ungroup() %&gt;% \n  full_join(tibble(state = state.abb), by = c(\"state\" = \"state\")) %&gt;% \n  replace_na(list(year = 1987)) %&gt;% \n  complete(state, nesting(year)) %&gt;% \n  replace_na(list(state_year_total_awards = 0)) %&gt;% \n  group_by(year) %&gt;% \n  mutate(year_total_awards = max(year_total_awards, na.rm = T)) %&gt;% \n  ungroup()\n\n# Calculate the percent of year total for each record (Choropleth fill) \nawards_by_state_year &lt;- . %&gt;%\n  add_count(year, name = \"year_total_awards\") %&gt;% \n  group_by(state, year, year_total_awards) %&gt;% \n  summarise(state_year_total_awards = n()) %&gt;%\n  ungroup() %&gt;% \n  full_join(tibble(state = state.abb), by = c(\"state\" = \"state\")) %&gt;% \n  replace_na(list(year = 1987)) %&gt;% \n  complete(state, nesting(year)) %&gt;% \n  replace_na(list(state_year_total_awards = 0)) %&gt;% \n  group_by(year) %&gt;% \n  mutate(year_total_awards = max(year_total_awards, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  mutate(pct_of_year_total_awards = state_year_total_awards / year_total_awards)\n\n# Finally, nest the yearly data under the state and join the albersusa data set and create the function\nawards_by_state_year &lt;- . %&gt;%\n  add_count(year, name = \"year_total_awards\") %&gt;% \n  group_by(state, year, year_total_awards) %&gt;% \n  summarise(state_year_total_awards = n()) %&gt;%\n  ungroup() %&gt;% \n  full_join(tibble(state = state.abb), by = c(\"state\" = \"state\")) %&gt;% \n  replace_na(list(year = 1987)) %&gt;% \n  complete(state, nesting(year)) %&gt;% \n  replace_na(list(state_year_total_awards = 0)) %&gt;% \n  group_by(year) %&gt;% \n  mutate(year_total_awards = max(year_total_awards, na.rm = T)) %&gt;% \n  ungroup() %&gt;% \n  mutate(pct_of_year_total_awards = state_year_total_awards / year_total_awards) %&gt;% \n  group_by(state) %&gt;% \n  nest() %&gt;%\n  right_join(tigris::shift_geometry(albersusa::usa_sf(\"laea\")), by = c(\"state\"=\"iso_3166_2\"))\n\n\nThen we can plug the function and pipeline the data frame through and into a gganimate output.\n\n\nCode\nchorpleth &lt;- gabf %&gt;% \n  awards_by_state_year() %&gt;% \n  unnest(data) %&gt;% \n  ggplot(ggplot2::aes(geometry = geometry, fill = pct_of_year_total_awards, group = year)) +\n  geom_sf() +\n  scale_fill_viridis_c(option = \"magma\", alpha = .9, begin = .1, labels = scales::percent) + \n  labs(title = 'Percent of Total Awards by State, Year: {round(frame_time,0)}', fill = \"Awards\") +\n  theme_blog(axis.text = ggplot2::element_blank(),\n             axis.ticks = ggplot2::element_blank()) +\n  transition_time(year) +\n  ease_aes('linear')\nanimate(chorpleth, fps = 5)\n\n\n\n\n\n\n\nInteractive TIE Fighter Plot & Line Chart\nLooking at the animation, you can see Wisconsin has lost a proportion of total awards over the 34 years. This view does not provide clarity regarding where those proportions are going. To see the trend of awards for the state, I visualized a logistic regression model in a TIE fighter plot. A logistic regression model provides a useful summary for the 34 years of awards for each state, but I wanted to keep the annual context so I thought the TIE fighter plot would work better accompanied with a line chart. I decided to add in dynamic highlighting so that both visuals can be used interactively. I used crosstalk and plotly to build in the interactivity. Crosstalk enables htmlwidgets with cross-widget interactions (highlighting and filtering). Plotly builds interactive graphs and the author wrote a ggplotly() function that will convert ggplot2 graphics to plotly, brilliant! Now I can build interactive data visualizations without knowing much more than ggplot.\n\n\n\nI used a number of resources to figure out how to do this. David Robinson wrote intro to broom, which covers all the needed tools to build TIE fighter plots and more. Carson Servert wrote a plotly book which covers all plotly functionality with examples. Plotly mostly handles crosstalk for the user, but I have found it easier to learn crosstalk to know what’s happening under the hood. Crosstalk relies on it’s SharedData object. Carson’s book also has a client-side linking chapter which talks through how to use crosstalk in plotly. Finally, this Stackoverflow post also provided a quick, repeatable example.\nThe widgets need two SharedData objects from gabf to cross-link the visuals. One with an observation for each state and the years and awards nested as a tibble column (TIE fighter plot). One with an observation for each state for each year with awards won (line chart). The ephemeral functions now come in handy. I can join my state level awards data with my state & year level awards data in order to filter out states with that didn’t receive awards in more than one year and states that didn’t receive more than five total awards for both SharedData objects. Below are the two SharedData objects spelled out.\n\n\nCode\n###################\n# TIE Fighter Data\n###################\n# Filter states without enough awards and remove simple features (not needed and they are heavy)\ntie_fighter_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5)\n# Create a model for each state\ntie_fighter_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5) %&gt;% \n  mutate(model = map(data, ~ glm(cbind(state_year_total_awards, year_total_awards - state_year_total_awards) ~ year, data = .x , family = \"binomial\")))\n# Tidy model data \ntie_fighter_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5) %&gt;% \n  mutate(model = map(data, ~ glm(cbind(state_year_total_awards, year_total_awards - state_year_total_awards) ~ year, data = .x , family = \"binomial\"))) %&gt;% \n  mutate(results = map(model, broom::tidy, conf.int = TRUE)) %&gt;%\n  unnest(results) %&gt;%\n  ungroup() %&gt;% \n  filter(term == \"year\")\n# Reorder the data by estimate (states listed by trend)\ntie_fighter_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5) %&gt;% \n  mutate(model = map(data, ~ glm(cbind(state_year_total_awards, year_total_awards - state_year_total_awards) ~ year, data = .x , family = \"binomial\"))) %&gt;% \n  mutate(results = map(model, broom::tidy, conf.int = TRUE)) %&gt;%\n  unnest(results) %&gt;%\n  ungroup() %&gt;% \n  filter(term == \"year\") %&gt;% \n  mutate(state = fct_reorder(state, estimate))\n# Add attributes for plot formatting and tooltip\ntie_fighter_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5) %&gt;% \n  mutate(model = map(data, ~ glm(cbind(state_year_total_awards, year_total_awards - state_year_total_awards) ~ year, data = .x , family = \"binomial\"))) %&gt;% \n  mutate(results = map(model, broom::tidy, conf.int = TRUE)) %&gt;%\n  unnest(results) %&gt;%\n  ungroup() %&gt;% \n  filter(term == \"year\") %&gt;% \n  mutate(state = fct_reorder(state, estimate)) %&gt;% \n  mutate(p_value = case_when(\n    p.value &gt;= .075 ~ \"not confident\",\n    p.value &gt;= .025 ~ \"somewhat confident\",\n    p.value &lt; .025 ~ \"confident\"),\n    trend = case_when(\n      estimate &gt;= .0025 ~ \"trending up\",\n      estimate &lt; -.0025 ~ \"trending down\",\n      T  ~ \"flat\"\n      )) %&gt;%\n  mutate(p_value = fct_relevel(p_value, c(\"not confident\", \"somewhat confident\")))\n# Create the SharedData object\ntie_fighter_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5) %&gt;% \n  mutate(model = map(data, ~ glm(cbind(state_year_total_awards, year_total_awards - state_year_total_awards) ~ year, data = .x , family = \"binomial\"))) %&gt;% \n  mutate(results = map(model, broom::tidy, conf.int = TRUE)) %&gt;%\n  unnest(results) %&gt;%\n  ungroup() %&gt;% \n  filter(term == \"year\") %&gt;% \n  mutate(state = fct_reorder(state, estimate)) %&gt;% \n  mutate(p_value = case_when(\n    p.value &gt;= .075 ~ \"not confident\",\n    p.value &gt;= .025 ~ \"somewhat confident\",\n    p.value &lt; .025 ~ \"confident\"),\n    trend = case_when(\n      estimate &gt;= .0025 ~ \"trending up\",\n      estimate &lt; -.0025 ~ \"trending down\",\n      T  ~ \"flat\")) %&gt;%\n  mutate(p_value = fct_relevel(p_value, c(\"not confident\", \"somewhat confident\"))) %&gt;% \n  SharedData$new(key = ~state_name, group = \"Choose states (hold shift to select multiple):\")\n  \n###################\n# Line Chart Data\n###################\n# Filter states without enough awards and remove simple features (not needed and they are heavy)\nline_chart_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5)\n# Unnest the annual observations for ggplot aesthetics\nline_chart_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5) %&gt;% \n  unnest(data)\n# Create the SharedData object\nline_chart_data &lt;- . %&gt;% \n  awards_by_state_year() %&gt;% \n  left_join(y = gabf %&gt;% awards_by_state()) %&gt;%\n  select(-geometry) %&gt;% \n  filter(state_years_with_awards &gt; 1 & state_total_awards &gt; 5) %&gt;% \n  unnest(data) %&gt;% \n  SharedData$new(key = ~state_name, group = \"Choose states (hold shift to select multiple):\")\n\n\nAgain just use the functions in a pipeline into ggplot and pass the ggplot objects to ggplotly. Most of the ggplot code is straight forward if you’re a ggplot user, but the text aesthetic is a bit of a blob. That’s because ggplotly allows aesthetics to be set in the tool tip using tooltip = \"text\". So if you paste together data and html tags you can build a nice tool tip. It’s tedious, but useful. Finally, subplot ties the plots together in a view with highlighting. Just a heads up - if you use htmlwidgets in a blogdown site, css classes can start bumping into another. Objects can disappear simply because the css container is smaller the the htmlwidget, use your browser tools to reset sizing and debug.\n\n\n\nCode\ngreys &lt;- brewer.pal(n = 9, \"Greys\")\ngreys &lt;- c(greys[3], greys[6], greys[9])\ntie_fighter_plot &lt;- gabf %&gt;% \n  tie_fighter_data() %&gt;% \n  ggplot(aes(x = estimate, y = state, key = state_name, group = state_total_awards, color = p_value,\n             text = paste0(\"&lt;b&gt;\", state_name, \"&lt;/b&gt;\\n\", if_else(p_value == 'confident', glue('P-Value: {format(round(p.value, digits = 4), nsmall = 4)}\\nThe number of {state_name} awards is definitely {trend}.'), \n                                                                      if_else(p_value == 'somewhat confident', glue::glue(\"'P-Value: {format(round(p.value, digits = 4), nsmall = 4)}\\nThe number of {state_name} awards is {trend}, but there is some uncertainty.\"), glue::glue(\"P-Value: {format(round(p.value, digits = 4), nsmall = 4)}\\nThe number of {state_name} awards appears to be {trend}, but there is signficant uncertainty.\"))))), \n         guides = guides(color = NULL)) +\n  geom_point(size = 1.75) +\n  geom_errorbarh(aes(\n    xmin = conf.low,\n    xmax = conf.high),\n  height = .5,\n  size = 1,\n  show.legend = FALSE) +\n  geom_vline(xintercept = 0, lty = 2, color = \"#a0aace\") +\n  scale_colour_manual(values = greys) +\n  scale_x_continuous(limits = c(-0.15, 0.15), \n                     breaks = c(-.1, 0, .1),\n                     labels = c(\"Descreasing\", \"\", \"Increasing\")) +\n  labs(\n    x = \"Trend\",\n    title = NULL,\n    y = NULL,\n    color = NULL) +\n  theme_blog()\ntie_fighter_plot &lt;- plotly::ggplotly(tie_fighter_plot, tooltip = \"text\", height = 625)\nline_chart &lt;- gabf %&gt;% \n  line_chart_data() %&gt;% \n  ggplot(aes(x = year, y = state_year_total_awards, key = state_name, group = state_total_awards,\n             text = glue::glue(\"&lt;b&gt;{state_name} Awards&lt;/b&gt;\\n{state_year_total_awards} ({year})\\n{state_total_awards} (1987-2020)\\n\"))) +\n  geom_line() +\n  labs(\n    x = \"Year\",\n    title = NULL,\n    y = \"\") +\n  theme_blog(panel.grid.major.y = element_blank())\nline_chart &lt;- ggplotly(line_chart, tooltip = \"text\", height = 625)  \nlinked_plots &lt;- subplot(tie_fighter_plot,\n                        line_chart,\n                        widths = c(.35, .65)) %&gt;%\n  layout(showlegend = FALSE) %&gt;% \n  highlight(on = \"plotly_click\", selectize = TRUE, dynamic = T)\ndiv(class = \"subplot-view\",\n    div(class = \"div-subtitle\",\n        div(class = \"div-title\", \"State Trends\"),\n        \"Alaska & Wisconsin are losing their share of awards to North Carolina, Indiana, & Virginia with South Carolina Jumping in the mix. California, Oregon, & Colorado appear to be maintaining their hold on the lion's share of the awards.\"\n    ),\n    linked_plots\n)\n\n\n\n\nState Trends\nAlaska & Wisconsin are losing their share of awards to North Carolina, Indiana, & Virginia with South Carolina Jumping in the mix. California, Oregon, & Colorado appear to be maintaining their hold on the lion's share of the awards.\n\n\n\n\n\n\n\nLooking at the results (State Trends), the TIE fighter plot (on the left) shows the model results by state. The states with points further to the right are seeing an upward trend in the number of awards received, states to the left are seeing a downward trend. The lines (error bars) show the confidence intervals of the model. Darker colored TIE fighter’s have higher p-values (more certainty). The line chart (on the right) shows each state with the number of awards over the 34 years. Hover on a TIE fighter or line, the hover-link provides helpful text. Highlight any state by clicking on its TIE fighter or line, or just type the state in the search bar at the top. Multi-select states with shift, but it’s a bit shifty (pun intended). When selecting multiple states, change the brush color to make it easier to see which state is which.\n\n\nBrewery, Category, & Beer Table\nOur logistic regression model provides a great understanding of how states are preforming, what about breweries or beers? This is where the data set starts to get dirty, I’ll walk through a few of the issues using the first record in the table below (Beer Name == \"Oktoberfest\"). There are duplicate beer names across breweries. For example, there are 18 beers called Oktoberfest, which won 20 total medals. Since they aren’t unique, the data would need to be grouped on another attribute. Brewery would be the obvious choice, but drill down into the nested table and you will quickly spot duplicated brewery names with slight spelling variations (e.g. Stoudts Brewing Co. and Stoudt's Brewing Co.). This is pervasive upon inspecting other examples. The category on the other hand seems to have evolved rapidly or beers can enter multiple categories. The Oktoberfest beers have 10 different categories and Stoudt’s Oktoberfest (which is the exact same beer) has competed in two different categories. Categories also have some character issues, for example (the German-Style MC3=A4rzen is German-Style Mäerzen, the ä causes the characters). I can clean those up, but it’d be a bit of work.\n\n\n\nCode\ntbl &lt;- gabf %&gt;% \n  count(beer_name, brewery, category, medal) %&gt;% \n  mutate(beer_name = fct_reorder(beer_name, n, sum)) %&gt;% \n  pivot_wider(names_from = medal, values_from = n, values_fill = 0) %&gt;% \n  mutate(total_medals = Gold+Silver+Bronze) %&gt;% \n  clean_names(\"title\") %&gt;% \n  reactable(groupBy = \"Beer Name\",\n            defaultPageSize = 5,\n            filterable = TRUE, \n            searchable = TRUE,\n            defaultSorted = c(\"Total Medals\"),\n            defaultSortOrder = \"desc\",\n            showPageSizeOptions = TRUE,\n            class = \"reactable-tbl\",\n            defaultColDef = colDef(aggregate = \"sum\", headerClass = \"reactable-header\", align = \"left\"),\n            compact = TRUE,\n            columns =  list(\n              Brewery = colDef(\n                name = \"Breweries\",\n                aggregate = JS(\"\n                function(values, rows) \n                {\n                  let breweries = values.filter((e, i) =&gt; values.indexOf(e) === i);\n                  if(breweries.length &gt; 1){\n                    var left = '(';\n                    var right = ')';\n                    return breweries.length;\n                  } else {\n                    return breweries;\n                  }\n                }\")\n              ),\n              Category = colDef(\n                name = \"Categories\",\n                aggregate = JS(\"\n                function(values, rows) \n                {\n                  let categories = values.filter((e, i) =&gt; values.indexOf(e) === i);\n                  if(categories.length &gt; 1){\n                    return categories.length;\n                  } else {\n                    return categories;\n                  }\n                }\")\n              ),\n              `Category Total` = colDef(\n                aggregate = \"max\"\n              ),\n              Gold = colDef( \n                name = \"Total Golds\"\n              ),\n              Silver = colDef( \n                name = \"Total Silvers\"\n              ),\n              Bronze = colDef( \n                name = \"Total Bronzes\"\n              )))\ndiv(class = \"reactable-tbl-view\",\n    div(class = \"div-subtitle\",\n        div(class = \"div-title\", \"All GABF Award Winning Beers \"),\n        \"Data is nested on beers, exploring the data shows a number of data cleaning activities required\"\n    ),\n    tbl\n)\n\n\n\n\n\nAll GABF Award Winning Beers \nData is nested on beers, exploring the data shows a number of data cleaning activities required\n\n\n\n\n\n\n\n\n\nAward Winning Categories by State\nCleaning up some of the text data would be cumbersome and I don’t want to invest all that additional time at this point. Given that, I’m going to focus on getting insights from the category data. Although it’s shifted a lot over the years, it seems to be the cleanest by far. The tidylo package can measure the which categories are most likely to get an award by state even and it deals with the different states have different numbers of awards. I recommend Julia & Tyler’s article on the package site if you want to learn more.\nThe visual below shows which categories are most likely to get an award by state (bars that go to the right) and which categories are least likely (bars that go to the left) based on the historical data. I’m a big fan of stouts, porters, and sours which tend to be hard to find in certain states. Wisconsin and Illinois being less represented in those categories makes a lot of sense given all the love they give to German beer.\n\n\n\nCode\ngabf %&gt;% \n  filter(fct_lump(state, 9) != \"Other\",\n         fct_lump(category, 7) != \"Other\") %&gt;%\n  count(state, category) %&gt;% \n  complete(state, category, fill = list(n = 0)) %&gt;% \n  bind_log_odds(state, category, n) %&gt;% \n  mutate(category = reorder_within(category, log_odds_weighted, state),\n         state = fct_reorder(state, -n)) %&gt;% \n  arrange(desc(n)) %&gt;% \n  ggplot(aes(log_odds_weighted, category)) +\n  geom_col() +\n  labs(title = \"Most Award Winning Categories by State\", x = NULL, y = \"Category\") +\n  scale_y_reordered() +\n  facet_wrap(~state, scales = \"free_y\", nrow = 3) +\n  theme_blog_facet(axis.text.x = element_blank(), axis.ticks.x = element_blank())"
  },
  {
    "objectID": "posts/2020-10-26-great-american-beer-festival/index.html#wrapping-up",
    "href": "posts/2020-10-26-great-american-beer-festival/index.html#wrapping-up",
    "title": "Great American Beer Festival",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThis data set would have been a lot more interesting if we had the full participation list. That being said, there were some interesting findings.\nI went down a few rabbit holes that I did not write-up. I built a sunburst of the year, brewery, category, and beers just to explore the data more easily, but it was too difficult to read even after applying factor lumping - the data is just too disperse (i.e. too many unique values for each category). In order to make the sunburst usable I thought it would be helpful to build another cross-widget interaction to navigate the first few categories so they could be left off of the sunburst, but but it turns out sunburstR does not support crosstalk at the moment (side note, if you have not used sunburstR it makes beautiful d3.js sunbursts with very little effort). I had a quick conversation with one of the package maintainers and it sounds like it would be a significant effort to add that functionality - would be a super interesting project. I also tried to figure out how to clean up the character issues such as ä, but I couldn’t find any simple tools to use for that specific issue - would be interesting to research further.\nAside from those rabbit holes, the state rankings table provided interesting insights regarding awards per capita with Colorado way out in front. The TIE fighter plot showed that some lesser expected states including the Carolinas are catching up in the number of awards. The award winning categories by state log odds plot showed that, by in large, states win awards in different categories when comparing to each other.\nThanks for reading and if you have any feedback please do post it below or feel free to reach out to me directly (contact info is at the bottom)!"
  },
  {
    "objectID": "posts/2019-12-21-hello-world/index.html",
    "href": "posts/2019-12-21-hello-world/index.html",
    "title": "Hello World",
    "section": "",
    "text": "At this year’s RStudio Conf David Robinson discussed the importance of sharing work publicly in his talk “The unreasonable effectiveness of public work.”. He attributed much of his own personal, professional success to the habit of sharing his work and encouraged others to adopt the habit. I recommend the talk to anyone, data scientist or not. Anyhow, I have procrastinated writing a blog for long enough. I attended the Advanced RMarkdown Workshop at the conference and learned, among other things, the blogdown blogdown package so here’s to getting started :beers:"
  },
  {
    "objectID": "posts/2019-12-21-hello-world/index.html#hello-world",
    "href": "posts/2019-12-21-hello-world/index.html#hello-world",
    "title": "Hello World",
    "section": "",
    "text": "At this year’s RStudio Conf David Robinson discussed the importance of sharing work publicly in his talk “The unreasonable effectiveness of public work.”. He attributed much of his own personal, professional success to the habit of sharing his work and encouraged others to adopt the habit. I recommend the talk to anyone, data scientist or not. Anyhow, I have procrastinated writing a blog for long enough. I attended the Advanced RMarkdown Workshop at the conference and learned, among other things, the blogdown blogdown package so here’s to getting started :beers:"
  },
  {
    "objectID": "posts/2019-12-21-hello-world/index.html#blogdown-starter-kit",
    "href": "posts/2019-12-21-hello-world/index.html#blogdown-starter-kit",
    "title": "Hello World",
    "section": "Blogdown Starter Kit",
    "text": "Blogdown Starter Kit\nThe aforementioned Advanced RMarkdown Workshop is a great resource and per usual RStudio was kind enough to open source it. A few additional resource I used to get this site open and running were:\n\nRbind Github Repo\nUp & Running with blogdown\nMaking a Website Using Blogdown, Hugo, and GitHub Pages\nRstudio Conf: Making Websites in R Markdown\nblogdown: Creating Websites with R Markdown\nSetting up your blog with RStudio and blogdown:\n\nPart I: Creating the blog\nPart II: Workflow\nPart III: Modify Your Theme\n\nMarkdown Basics\nR Markdown the Definitive Guide\nHugo Emojify Cheatsheet\n\nAll the above resources should have anyone well informed on blogdown, but for posterity I recommend following Alison Hill’s blog as there are sure to be updates and she is an amazing resource. Happy blogging!"
  },
  {
    "objectID": "posts/2023-04-05-web-scraping/index.html#static-site",
    "href": "posts/2023-04-05-web-scraping/index.html#static-site",
    "title": "Web Scraping w/ R",
    "section": "Static Site",
    "text": "Static Site\nWhen scraping, as is true with many things done it code, it’s easiest to start small. Scraping static sites that do not heavily rely on JavaScript to function is significantly less complicated than scraping sites that rely on JavaScript or dynamic sites. With that in mind, we’ll start with a very simple static site, the CRAN packages page, because it’s structured in a way that’s ideal for scraping so it’s straight forward.\n\nParsing HTML\nThe rvest package has a suite of tools for parsing the HTML document, which is the core functionality required to scrape. The first thing to do when scraping a page is to figure out what we want to scrape and determine it’s HTML structure. We can do this using the browser’s web developer tools, but most of this can also be done inside RStudio. It probably goes without saying if you look at the CRAN packages page, but I’d like to scrape the packages table and make it a data frame.\n\n\n\nTo inspect the page, we first read the page using read_html(). This reads the HTML page as is into our rsession for processing. We can see that the cran_packages_html object is a list of length two and both objects inside the list are external pointers. In other words, the cran_packages_html document is not in the active rsession, rather a pointer which directs R to the documents created by libxml2 which are stored in RAM (at least this is my rough understanding of how it works). For more information, Bob Rudis provided a very detailed response about scraping which touches on this point, but the take away should be that this object does not contain the data from the HTML page - just pointers!\n\n\nCode\nlibrary(tidyverse) \nlibrary(rvest)\ncran_packages_html &lt;- read_html(\"https://cran.r-project.org/web/packages/available_packages_by_date.html\")\nstr(cran_packages_html)\n\n\nList of 2\n $ node:&lt;externalptr&gt; \n $ doc :&lt;externalptr&gt; \n - attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n\n\n\n\n\n\n\n\nxml2 not attached\n\n\n\n\n\nAn aside, if you open cran_packages_html with viewer and trying to inspect one of the pointers, you’ll get an error could not find function \"xml_child\". That’s because rvest depends on xml2, but does not attached it to the global environment.\n\n\n\nYou can simply load xml2 to fix the issue.\n\n\nCode\nlibrary(xml2)\nxml_child(cran_packages_html, 2)\n\n\n{html_node}\n&lt;body lang=\"en\"&gt;\n[1] &lt;div class=\"container\"&gt;\\n&lt;h1&gt;Available CRAN Packages By Date of Publicati ...\n\n\n\n\n\nThe rvest package has a suite of functions for parsing the HTML document starting with functions that help use understand the structure including html_children() & html_name(). We can use html_children() to climb down the page and html_name() to see the tag names of the HTML elements we want to parse. For this page, we used html_chidren() to see that the page has a and a , which is pretty standard. We’ll want to scrape the &lt;body&gt; because that’s where the content of the page will be.\n\n\nCode\ncran_packages_html |&gt; \n  html_children() |&gt; \n  html_name()\n\n\n[1] \"head\" \"body\"\n\n\nTo further parse the &lt;body&gt;, we’ll use html_element() to clip the rest of the HTML document and look inside &lt;body&gt;. Within &lt;body&gt;, we can see there’s just a []](https://www.w3schools.com/tags/tag_div.asp).\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_children() |&gt; \n  html_name()\n\n\n[1] \"div\"\n\n\nWe can continue the process with the &lt;div&gt; and we see an an and a . It’s fairly obvious we’ll want to the &lt;table&gt;, not &lt;h1&gt;, but just to illustrate if we look within &lt;h1&gt;, we’ll see no nodes exist beneath it.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h1\") |&gt; \n  html_children() |&gt; \n  html_name()\n\n\ncharacter(0)\n\n\nThat doesn’t mean &lt;h1&gt; has no data, it just means no HTML is a child of &lt;h1&gt;. Since &lt;h1&gt; a tag used on titles text), we can use [html_text()] to extract the actual text inside. This isn’t particularly useful in this case, but html_text() can be very useful.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"h1\") |&gt; \n  html_text() \n\n\n[1] \"Available CRAN Packages By Date of Publication\"\n\n\nIf we use html_element(\"table\"), we can see it contains the data we’re looking for, but there’s a bit of HTML junk we’ll need to clean up for our data frame.\n\n\nCode\ncran_packages_html |&gt;  \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"table\") \n\n\n{html_node}\n&lt;table border=\"1\"&gt;\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DrugSim2DR/ ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/actxps/inde ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/AgroR/index ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/aplot/index ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/av/index.ht ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/basemodels/ ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bayesPop/in ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Bayesrel/in ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/beanz/index ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bookdown/in ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bruceR/inde ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/canvasXpres ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CARBayes/in ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/clinDR/inde ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cmm/index.h ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/complexlm/i ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CPC/index.h ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/dfoliatR/in ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DR.SC/index ...\n...\n\n\n\n\n\n\n\n\nhtml_element() will jump nodes\n\n\n\n\n\nIn the code above, we walked down the whole HTML tree body &gt; div &gt; table. The html_element() function will pickup HTML tags without providing the exact path, which is very convenient but can lead to unexpected results. The code below leads to the same results, but only because this page only has one HTML table. If it had multiple, it would only pick up the first one whether that was our intent or not. This point is very important to understand for more complicated web pages.\n\n\nCode\ncran_packages_html |&gt;   \n  # Skipped &lt;body&gt; & &lt;div&gt;\n  html_element(\"table\") \n\n\n{html_node}\n&lt;table border=\"1\"&gt;\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DrugSim2DR/ ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/actxps/inde ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/AgroR/index ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/aplot/index ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/av/index.ht ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/basemodels/ ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bayesPop/in ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Bayesrel/in ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/beanz/index ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bookdown/in ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bruceR/inde ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/canvasXpres ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CARBayes/in ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/clinDR/inde ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cmm/index.h ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/complexlm/i ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CPC/index.h ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/dfoliatR/in ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DR.SC/index ...\n...\n\n\n\n\n\nFortunately, rvest has a handy html_table() function that’s specifically for HTML tables and automatically coerces them into a list of tibbles. I used bind_rows() to coerce the list to a tibble. As you can see below, we end up with a table of packages with a date, package name, and title.\n\n\nCode\nlibrary(tidyverse) \nlibrary(reactable)\ncran_packages_df &lt;- cran_packages_html |&gt; \n  html_table() |&gt; \n  bind_rows()\n\ncran_packages_df |&gt; \n  reactable(\n    searchable = TRUE, \n    paginationType = \"jump\",  \n    showPageSizeOptions = TRUE,\n    pageSizeOptions = c(5, 10, 50, 100),\n    defaultPageSize = 5)\n\n\n\n\n\n\n\n\n\n\nThe table from above has the package names, but it doesn’t include most of the package metadata. Going back to the site, you can see the package name has a link to another page that contains all that data.\n\n\n\n\n\n\nPackages\n\n\n\n\n\n\n\nPackage Details\n\n\n\n\n\nIf we wanted to obtain the URL’s, we need to parse the &lt;table&gt;. Using html_children() again we can see that &lt;table&gt; contains &lt;tr&gt; tags, which is HTML table rows.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"table\") |&gt; \n  html_children()\n\n\n{xml_nodeset (19903)}\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DrugSim2DR/ ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/actxps/inde ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/AgroR/index ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/aplot/index ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/av/index.ht ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/basemodels/ ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bayesPop/in ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Bayesrel/in ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/beanz/index ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bookdown/in ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bruceR/inde ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/canvasXpres ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CARBayes/in ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/clinDR/inde ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cmm/index.h ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/complexlm/i ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CPC/index.h ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/dfoliatR/in ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DR.SC/index ...\n...\n\n\nThen we can go a level lower and see all the elements in the rows. Notice, we use the html_elements() (plural) function instead of html_element(). That’s because each row has multiple elements and html_element() will only parse the first element. We can a &lt;td&gt; tag, which is an HTML data cell, and an &lt;a&gt; tag, which is a hyperlink. The &lt;a contains href=\"../../web/packages/.... An href is an HTML attribute for creating hyperlinks on a web page.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"table\") |&gt; \n  html_elements(\"tr\") |&gt; \n  html_children()\n\n\n{xml_nodeset (59709)}\n [1] &lt;th&gt; Date &lt;/th&gt;\n [2] &lt;th&gt; Package &lt;/th&gt;\n [3] &lt;th&gt; Title &lt;/th&gt;\n [4] &lt;td&gt; 2023-08-10 &lt;/td&gt;\n [5] &lt;td&gt; &lt;a href=\"../../web/packages/DrugSim2DR/index.html\"&gt;&lt;span class=\"CRA ...\n [6] &lt;td&gt; Predict Drug Functional Similarity to Drug Repurposing &lt;/td&gt;\n [7] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n [8] &lt;td&gt; &lt;a href=\"../../web/packages/actxps/index.html\"&gt;&lt;span class=\"CRAN\"&gt;a ...\n [9] &lt;td&gt; Create Actuarial Experience Studies: Prepare Data, Summarize\\nResul ...\n[10] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n[11] &lt;td&gt; &lt;a href=\"../../web/packages/AgroR/index.html\"&gt;&lt;span class=\"CRAN\"&gt;Ag ...\n[12] &lt;td&gt; Experimental Statistics and Graphics for Agricultural Sciences &lt;/td&gt;\n[13] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n[14] &lt;td&gt; &lt;a href=\"../../web/packages/aplot/index.html\"&gt;&lt;span class=\"CRAN\"&gt;ap ...\n[15] &lt;td&gt; Decorate a 'ggplot' with Associated Information &lt;/td&gt;\n[16] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n[17] &lt;td&gt; &lt;a href=\"../../web/packages/av/index.html\"&gt;&lt;span class=\"CRAN\"&gt;av&lt;/s ...\n[18] &lt;td&gt; Working with Audio and Video in R &lt;/td&gt;\n[19] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n[20] &lt;td&gt; &lt;a href=\"../../web/packages/basemodels/index.html\"&gt;&lt;span class=\"CRA ...\n...\n\n\nWe can extract attributes using htm_attr() which will parse the text into a character vector.\n\n\nCode\ncran_packages_df$href &lt;- cran_packages_html |&gt; \n  html_elements(\"tr\") |&gt; \n  html_elements(\"td\") |&gt; \n  html_elements(\"a\") |&gt;\n  html_attr(\"href\")\n\n\nIn the code above, we walked down the whole HTML tree, but again we could’ve jumped down the tree like so. Just understand that this can lead to us picking up other hyperlinks that aren’t in our table (there are none of this page).\n\n\nCode\ncran_packages_html |&gt; \n  html_elements(\"a\") |&gt; \n  html_attr(\"href\")\n\n\nHowever, we do have to specify an HTML element before using html_attr() otherwise we’ll just get back NA.\n\n\nCode\ncran_packages_html |&gt; \n  html_attr(\"href\")\n\n\nOne thing that’s worth mentioning is the URL’s we collected are relative paths, which is why they have /../ in the path.\n\n\nCode\ncran_packages_df$href[1:3]\n\n\n[1] \"../../web/packages/DrugSim2DR/index.html\"\n[2] \"../../web/packages/actxps/index.html\"    \n[3] \"../../web/packages/AgroR/index.html\"     \n\n\nThis is a good point to take step way back to the beginning to introduce a few additional concepts, but if you made it this far you’ve learned enough to scrape a lot of websites with no additional tools. The html_ functions in rvest provide the core tools necessary to parse HTML which is what scraping is…"
  },
  {
    "objectID": "posts/2023-04-05-web-scraping/index.html#dynamic-site",
    "href": "posts/2023-04-05-web-scraping/index.html#dynamic-site",
    "title": "Web Scraping w/ R",
    "section": "Dynamic Site",
    "text": "Dynamic Site\nWe’ve learned how to scrape, to page through a website, and to do all this with proper decorum, but we learned all of these skills on a completely static site. We didn’t have to interact with any buttons, sign in pages, on page filters, or hit any servers once we were on the page to see the data we wanted to scrape. These on-screen actions can be a bit more complicated and require the use of additional software, which I’ll demonstrate.\n\nHiQ Labs v. LinkedIn\nAwhile back, I was job searching and built a scraper that helped me search LinkedIn for jobs that met my criteria. LinkedIn’s job search filters just don’t do enough. Plus, LinkedIn has so many duplicated posts and all the sponsored posts are always at the top. Job searching basically becomes a full-time job because none of the tools are built for the users’ benefit, they’re built for the business model. So yeah, I didn’t want to endlessly scroll through the dystopian hellscape that is the LinkedIn experience to try to find jobs… so that’s why I built a scraper! At that time, screen scraping was frowned upon, but Linkedin had unsuccessfully sued HiQ Labs for scraping and lost so it seemed like the worst case scenario was LinkedIn would close my account. That would suck, but it wouldn’t be that big of a deal. As I’m writing this post, I’m realizing they were recently able to get that ruling reversed and win in court. Many of the articles I’m seeing from the business journalists mention “LinkedIn winning in a suit to protect user data and privacy…” Thank goodness Microsoft is protecting our data from nefarious start-ups so that only they can benefit from it… 🤢\n\n\n\nAnyhow, all this to say, I wrote this scraper before the suit and I’m not suggesting it be used by anyone. I’m only publishing this work to demonstrate how to use these tools, I’d encourage you use the polite framework in order to avoid any legal issues.\nFor fun, I tried to rerun my code using polite and build on our prior examples and bow() responded with….. 🤣\n\n\nCode\nsesh_li &lt;- bow(\n  url = \"https://www.linkedin.com/\",\n  user_agent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\",\n  delay = 1,\n  force = F,\n  verbose = T\n)\n\n\nWarning: Psst!...It's not a good idea to scrape here!\n\n\n\n\n\n\n\nHTML Forms\nYou may have noticed rvest’s _form functions. Pages can have forms, the most common of which is a sign in. We cam use these to fill out forms and hit submit without using a full-fledged web driver like RSelenium. LinkedIn has a few pages that can be used to sign-in, but I worked from this one specifically because of it’s simplicity. You can inspect it with your web developer tools to easily see the structure - for larger sites like this I find it’s much easier to start there and then go to RStudio to use the tools we have up to this point.\n\n\n\nWhen I tried to use the rvest tools, I had a tough time getting them to work for this page. I’m not entirely sure, but it looks like LinkedIn’s submit button doesn’t have a name in the form when I scrape it. That said, in my browser tools I can see the name show up as Sign In. I tried using Sign In in the submit parameter in html_form_set(), but it didn’t work either. Not sure what I’m doing wrong, but that’s ok because we’ll need RSelenium if we go any further so we can just jump into it.\n\n\nCode\nlibrary(glue)\nli &lt;- session(\"https://www.linkedin.com/login?fromSignIn\")\nlogin_forms &lt;- li |&gt; \n  read_html() |&gt; \n  html_form() \nli &lt;- login_forms[[2]] |&gt; \n  html_form_set(session_key = Sys.getenv(\"LINKEDIN_UID\"), session_password = Sys.getenv(\"LINKEDIN_PWD\")) |&gt; \n  html_form_submit() \n\nglue(\"I requested `{li$request$url}` and was redirected to `{li$url}\") \n\n\n\n\nRSelenium\nAs I mentioned in the intro, RSelenium allows you to drive web browsers from R. RSelenium is an R package that provides bindings to Selenium, which is what actually drives the web browser. To use RSelenium, we have a couple options. Option one, run a server either on your machine or in docker and connect to it. Option two, run Selenium locally. For posterity, I’ll cover how to setup and connect to a docker instance running Selenium, but for the demonstration I’ll just us Selenium locally.\n\nDocker\nIf you’re not familiar with docker, I recommend Colin Faye’s An Introduction to docker for R Users. To get docker setup for Selenium, I heavily reference RSelenium’s docker documentation and the SeleniumHQ docker documentation. If you don’t have docker on your machine, you can install it here. You can also install docker desktop here if you like. The docker installation is required, but docker desktop is just helpful. Kind of like R is required and RStudio is just helpful.\n\nDocker Pull\nOnce you’ve installed docker, you need to pull an image with Selenium (or create one). To start, I followed the RSelenium docs. RSelenium refers to this image, which you can pull the image (i.e. get a copy of it to run on your machine) in terminal with the below code.\n\n\nCode\ndocker pull selenium/standalone-firefox:2.53.0\n\n\nThat standalone image works just fine for scraping, but if you want to be able to observe your code working, you’ll need to download the debug version.\n\n\nCode\ndocker pull selenium/standalone-firefox-debug:2.53.0\n\n\nRegardless, firefox:2.53.0 is pretty out of date. It uses Firefox version 47.0.1, which was released in 2016. The newest version on Selenium’s docker docs at the time of writing this is firefox:4.9.0-20230421, which uses Firefox version 112.0.1 (released in 2023). This later version also has debugging capbailities built in so there is no need to pull a -debug version. We can pull whatever is the latest version (right now 4.9.0-20230421) by removing the version or explicitly using latest. However, I couldn’t get version 4.9.0-20230421 and see in this issue that other users bumped into the same roadblock.\n\n\nCode\ndocker pull selenium/standalone-firefox:latest\n\n\nIn the Github issue comments, the fvalenduc said he tested the versions and the newest working version with RSelenium was 3.141.59.\n\n\nCode\ndocker pull selenium/standalone-firefox-debug:3.141.59\n\n\n\n\nStart Docker\nYou can start the docker container with the code below. The first port (i.e. -p 4445) is the Selenium server, the second is used to connect to a web server to observe the Selenium.\n\n\nCode\ndocker run -d -p 4445:4445 -p 5900:5900 selenium/standalone-firefox-debug:3.141.59\ndocker run -d -p 4444:4444 -p 7900:7900 --shm-size=\"2g\" selenium/standalone-firefox:latest\n\n\nWe can also start our Selenium server from R using the system command. This is helpful if you want to create a script or package your code. If you’re using Linux and docker is owned by root, you can either add your user to the docker group or pipe your sudo password into your command (shown in second system() call below).\n\n\nCode\nsystem(\n  command = \"docker run -d -p 4445:4445 -p 5900:5900 selenium/standalone-firefox-debug:3.141.59\",\n  intern = T\n)\nsystem(\n  command = glue::glue(\"echo {Sys.getenv('SUDO_PWD')} | sudo -S docker run -d -p 4445:4445 -p 5900:5900 selenium/standalone-firefox-debug:3.141.59\"),\n  intern = T\n)\n\n\n\n\nConnect to Docker\nWe can connect to the docker container using the remoteDriver() class. As I mentioned before, the latest version would not work for me… It would hang if I ran latest$open(). Once you open a connection, you can drive the browser from there. I’ll cover driving the web driver section.\n\n\nCode\nlibrary(RSelenium)\n# Connect to 3.141.59\nrdriver &lt;- remoteDriver(\n  remoteServerAddr = \"0.0.0.0\",\n  port = 4445L,\n  browserName = \"firefox\",\n  version = \"47.0.1\",\n  platform = \"LINUX\",\n  javascript = T\n)\n# Connect to latest\nlatest &lt;- remoteDriver(\n  remoteServerAddr = \"172.17.0.2\",\n  port = 4444L,\n  browserName = \"firefox\",\n  version = \"112.0\",\n  platform = \"LINUX\",\n  javascript = T\n)\n# Open the connection and navigate to a website\nrdriver$open()\nrdriver$navigate(\"www.google.com\")\n\n\nIf you don’t know the remoteServerAddr, there are several ways to get that info. First, you can run docker ps. The output can look a little confusing, but you should be able to figure out what the server’s IP address is.SDM\n\n\nCode\ndocker ps\n\n\nYou can also run docker ps from R. I couldn’t find a library that simplified interacting with Docker from R, but I’m sure one exists. I’ve written a few wrappers around ps, but never tried to do anything\n\n\nCode\nlibrary(tidyverse)\nimages &lt;- system(glue::glue(\"echo {Sys.getenv('SUDO_PWD')} | sudo -S docker ps\"), ignore.stdout = F, intern = T)\nips &lt;- as_tibble(str_locate(images, pattern = \"[0-9]??\\\\.[0-9]??\\\\.[0-9]??\\\\.[0-9]??:\"))\nips &lt;- pmap_dfr(list(i = images[2:5], s = ips[2:5, ]$start, e = ips[2:5, ]$end), function(i, s, e){\n  ip &lt;- str_sub(i, start = s, end = e-1)\n  tibble(ip = ip)\n})\nnames &lt;- as_tibble(str_locate(images, pattern = \"selenium.*firefox:[a-z0-9]+(?:[._-][a-z0-9]+)*|selenium.*firefox-debug:[a-z0-9]+(?:[._-][a-z0-9]+)*\"))\nnames &lt;- pmap_dfr(list(i = images[2:5], s = names[2:5, ]$start, e = names[2:5, ]$end), function(i, s, e){\n  name &lt;- str_sub(i, start = s, end = e)\n  tibble(name = name)\n}) \ndocker &lt;- bind_cols(names, ips) |&gt; \n  bind_cols(images[2:5]) |&gt; \n  rename(\"ps\" = 3L)\ndocker |&gt; \n  filter(str_detect(name, pattern = \"latest|firefox:4.\"))\n\n\n# A tibble: 0 × 3\n# ℹ 3 variables: name &lt;chr&gt;, ip &lt;chr&gt;, ps &lt;chr&gt;\n\n\nAnother option is to look on docker hub to see the environment variables set in the docker image. The NODE_HOST variable is the remote server address you should use. Alternatively, you can log into the web server, right click on the desktop, launch bash, and run hostname -I.\n\n\n\nIf you’re using a newer version of Selenium, once you have the IP, you can connect to Selenium Grid via endpoints using httr to get other info. This won’t work for the older versions that don’t use Selenium Grid.\n\n\nCode\nlibrary(httr)\nstatus &lt;- GET('http://localhost:4444/status')\ncontent(status)\n\n\n\n\nConnect to Web Server\nYou can log into the web server to get info from the machine, install updates, etc. if you want to, but the most useful aspect of doing so imho is the ability to watch your scraping program work in the browser while it’s running. You can make sure it’s doing what you intended so it doesn’t fly off the rails. To log into the web server, you’ll need a remote desktop app. The latest Selenium version comes with noVNC, but you’ll need to install your own when using the older -debug versions. I used Vinagre. You will be prompted for a password, I couldn’t figure out where that’s set in docker but found in the documentation that it is secret.\n \n\nWhen using the latest versuib you can simply access the web sever through your browser by going to http://localhost:7900/?autoconnect=1&resize=scale&password=secret.\n\n\n\nLocal\nRSelenium allows us to install binaries and run Selenium locally in our browser of chance through the wdman package with the rsDriver function which creates a remoteDriver locally (i.e. you can use it the same way you would use remoteDriver() once the connection is established). I had some issues with firefox on Ubuntu 22, but I was able to get Chrome working by updating Chrome and turning off check.\n\nSign-in\nFirst, we need to sign into LinkedIn so we can view all the pages. The findElement() method works similarly to html_element() from rvest, but we can access other methods to interact with the HTML element. Below, we used sendKeysToElement() in order to enter our credentials, the clickElement() to submit, and the getCurrentUrl() method to confirm the sign-in worked properly.\n\n\nCode\ndriver$client$navigate(\"https://www.linkedin.com/login?fromSignIn\")\nSys.sleep(3)\ndriver$client$screenshot(file = \"images/sign_in_page.png\")\ndriver$client$findElement(\"name\", \"session_key\")$sendKeysToElement(list(Sys.getenv(\"LINKEDIN_UID\")))\ndriver$client$findElement(\"name\", \"session_password\")$sendKeysToElement(list(Sys.getenv(\"LINKEDIN_PWD\")))\ndriver$client$findElement(\"tag name\", \"BUTTON\")$clickElement()\ndriver$client$maxWindowSize()\n\n\n  \n\n\nJob Search\nOnce we’ve signed in, we can interact with the site under our username. We could fill in the search forms, but I want to do a few different job searches and the parameters all go in the URL as query parameters so I’m just going to build a URL using glue and URLencode.\n\n\n\n\n\n\nMinimize Messaging\n\n\n\n\n\nYou may notice the use of minimize_messaging() in the job search code. I created a function to minimize personal messaging just so it wouldn’t be open for screen shots. This wouldn’t have been nessary if I wasn’t publishing automatically generated screenshots for this post…\n\n\nCode\nminimize_messaging &lt;- function(driver){\n  html &lt;- xml2::read_html(driver$client$getPageSource()[[1]], options = \"HUGE\")\n  df_messaging &lt;- tibble(\n    id = html |&gt; html_element(\"aside\") |&gt; html_elements(\"button\") |&gt; html_attr(\"id\"),\n    icon_type = html |&gt; html_element(\"aside\") |&gt; html_elements(\"button\") |&gt; html_element(\"li-icon\") |&gt; html_attr(\"type\")\n  ) |&gt; \n    filter(icon_type == \"chevron-down\")\n  if(nrow(df_messaging) == 1){\n    id &lt;- df_messaging$id\n    driver$client$findElement(\"id\", id)$clickElement()\n  }\n}\n\n\n\n\n\n\n\nCode\nremote_and_hybrid &lt;- \"f_WT=2,3\"\nkeywords &lt;- \"keywords=data analyst | data scientist\"\nlocation &lt;- \"Portland, Oregon Metropolitan Area\"\ngeo &lt;- \"geoId=90000079\"\njobs_url &lt;- URLencode(glue::glue(\"https://www.linkedin.com/jobs/search/?{remote_and_hybrid}&{geo}&{keywords}&{location}&refresh=T\"))\ndriver$client$navigate(jobs_url)\nSys.sleep(3)\nminimize_messaging(driver)\nSys.sleep(1)\ndriver$client$screenshot(file = \"images/job_search.png\")\njobs_html &lt;- xml2::read_html(driver$client$getPageSource()[[1]], options = \"HUGE\")\n\n\n  \n\n\nJob Listings\nThe search returns a bunch of jobs based on our search parameters in a number of pages. The list of jobs just has the role title, company name, and a few other basic job facts. The details of each job are in the detailed pages that we have to click into. We can extract the href attribute for each job listing to get a direct URL or the button id attribute so we can click the button through Selenium to open the job details pane. We can extract both of these things using the URL page links using html_attr. Remember, an &lt;a&gt; tag is used for hyperlinks, the href stores the URL and the id uniquely identifies the tag. Since each hyperlink will have one href and one id, I just extracted the data into a tibble to make it easier to filter to only the buttons we want. That said, if you look at the list of job links you may notice there are less jobs than are actually in the web page… the reason for that is you have to scroll down the page to load all the jobs.\n\n\nCode\ndf_jobs &lt;- tibble(\n  href = jobs_html |&gt; html_elements(\"body\") |&gt; html_elements(\"a\") |&gt; html_attr(\"href\"),\n  id = jobs_html |&gt; html_elements(\"body\") |&gt; html_elements(\"a\") |&gt; html_attr(\"id\")\n) |&gt; \n  filter(str_detect(href, pattern = \"/jobs/view/\")) |&gt; \n  mutate(href = paste0(\"https://www.linkedin.com\", href))\nglue::glue(\"df_jobs has {nrow(df_jobs)} hrefs\")\n\n\ndf_jobs has 9 hrefs\n\n\nTo scroll down, we need to find an anchor point to tell Selenium to scroll to. I chose to use the paging buttons at the bottom of the list because I’ll need those later for paging through the list.\n \n\nAfter you scroll down and retry scraping the jobs you’ll still be missing jobs from the list 😱 The site cleverly only loads as you scroll down the page. I tried a few more elegant ways to load the whole list including the down arrow (which is disabled in the jobs list view) and using a custom JavaScript script to send a mouse wheel event. Unfortunately, I couldn’t get those approaches to work and I was stuck with brute force…\n\n\nCode\njobs_html &lt;- xml2::read_html(driver$client$getPageSource()[[1]], options = \"HUGE\")\ndf_jobs &lt;- tibble(\n  href = jobs_html |&gt; html_elements(\"body\") |&gt; html_elements(\"a\") |&gt; html_attr(\"href\"),\n  id = jobs_html |&gt; html_elements(\"body\") |&gt; html_elements(\"a\") |&gt; html_attr(\"id\")\n) |&gt; \n  filter(str_detect(href, pattern = \"/jobs/view/\")) |&gt; \n  mutate(href = paste0(\"https://www.linkedin.com\", href))\nglue::glue(\"df_jobs has {nrow(df_jobs)} hrefs\")\n\n\ndf_jobs has 14 hrefs\n\n\nMy brute force approach was basically to repeatedly use the getElementLocationInView() method on a random walk over the job HTML id’s. This approach causes Selenium to scroll the browser to get the job into the view, and by randomly ordering the jobs the scroll goes in both directions getting jobs to load. I wrote this process in a recursive function then scrapes the jobs until the random walk doesn’t get any additional jobs to load.\n\n\nCode\nscrape_html &lt;- function(driver){\n  html &lt;- xml2::read_html(driver$client$getPageSource()[[1]], options = \"HUGE\")\n  return(html)\n}\nscrape_pages_df &lt;- function(html){\n  df_pages &lt;- tibble(\n    pagination_btn = html |&gt; html_element(\"body\") |&gt; html_elements(\"li\") |&gt; html_attr(\"data-test-pagination-page-btn\"),\n    id = html |&gt; html_elements(\"body\") |&gt; html_elements(\"li\") |&gt; html_attr(\"id\")) |&gt; \n    mutate(pagination_btn = as.numeric(pagination_btn)) |&gt; \n    mutate(pagination_btn_ellipsis = lag(pagination_btn) +1) |&gt; \n    mutate(pagination_btn = coalesce(pagination_btn, pagination_btn_ellipsis),   .keep = \"unused\") |&gt; \n    filter(str_detect(id, \"ember\") & !is.na(pagination_btn))\n  return(df_pages)\n}\nrandwalk_jobs &lt;- function(driver, ids){\n  ids &lt;- ids |&gt; \n    sort() |&gt; \n    sample()\n  purrr::walk(ids, function(id){\n    job_element &lt;- driver$client$findElement(\"id\", id)\n    job_element$getElementLocationInView()\n    naptime::naptime(.25)\n  })\n}\nscrape_jobs_df &lt;- function(driver, n_jobs = 0){\n  if(n_jobs == 0){\n    html &lt;- scrape_html(driver)\n    df_pages &lt;- scrape_pages_df(html)\n    first_page_button_webElem &lt;- driver$client$findElement(\"id\", df_pages$id[1])\n    first_page_button_webElem$getElementLocationInView()\n    naptime::naptime(1)\n  }\n  html &lt;- scrape_html(driver)\n  df_jobs &lt;- tibble(\n    url = html |&gt; html_elements(\"body\") |&gt;  html_elements(\"a\") |&gt; html_attr(\"href\"),\n    class = html |&gt; html_elements(\"body\") |&gt;  html_elements(\"a\") |&gt; html_attr(\"class\"),\n    id = html |&gt; html_elements(\"body\") |&gt;  html_elements(\"a\") |&gt; html_attr(\"id\"),\n    text = html |&gt; html_elements(\"body\") |&gt;  html_elements(\"a\") |&gt; html_text()\n  ) |&gt; \n    filter(str_detect(url, pattern = \"/jobs/view/\")) |&gt; \n    filter(str_detect(class, patter = \"job-card-list__title\"))\n  if(n_jobs &lt; nrow(df_jobs)){\n    randwalk_jobs(driver, df_jobs$id)\n    df_jobs &lt;- scrape_jobs_df(driver, n_jobs = nrow(df_jobs))\n  } else{\n    return(df_jobs)\n  }\n}\ndf_jobs &lt;- scrape_jobs_df(driver)\nglimpse(df_jobs)\n\n\nRows: 25\nColumns: 4\n$ url   &lt;chr&gt; \"/jobs/view/3643896946/?eBP=CwEAAAGJ3eaV7OiDcVSfcoI4Yh_BhaGHnPtE…\n$ class &lt;chr&gt; \"disabled ember-view job-card-container__link job-card-list__tit…\n$ id    &lt;chr&gt; \"ember210\", \"ember222\", \"ember233\", \"ember245\", \"ember256\", \"emb…\n$ text  &lt;chr&gt; \"\\n                  Data Analyst - Machine Learning\\n          …\n\n\n\n\nJob Details\nNow that we have all the jobs loaded with href’s and id’s, we can open the job details to scrape out the data we want. We can either directly navigate to the job URL’s or open the job details pane. The former option is much easier, but it will cause another page load (slower) and it’s a user behavior that would not resemble a user on the site (i.e. it would look like a scraper). We’ve already covered all the methods required to do use either method (direct navigation or button clicking) in Job Search and Sign-in, respectively. I used the latter method to open the jobs using the id’s and the clickElement method.\n\n\nCode\ndriver$client$findElement(\"id\", df_jobs$id[1])$clickElement()\ndriver$client$screenshot(file = \"images/job_1_view.png\")\n\n\n\nThe job details pane was a total pain 😅 It was broken into a sections and I scraped a few of them - a top card which has some details about the company and role, the job description (text), salary data, and skills data. The skills data is inside a modal view. I’m not going to walk through all this code line by line because we’ve already walked through most these functions and methods, but I put everything in functions with long names to try to make it as easy to follow as possible. The main function, scrape_job_details() can pull all the data from the aforementioned sections (top card, description, salary, skills) and puts it all in a data frame. The one new idea that shows up here that didn’t in prior sections in this post is error handling. Error handling deserves its own separate post so I’m not going to dive into it here, but if you write a scraper that does interactive things (clicking, log-ins, paging) you will need error handling. Sometimes your code will be slightly faster than the browser and it causes random glitches. Also, sometimes if you scrape enough data some parts of the site might not be structured the same way due to edge cases, errors, etc. There are several libraries and base functions available for error handling, but I used purrr because I thought possibly() and insistently would be the easiest / best for what I was doing. If it wasn’t apparent up to now, you can see screen scraping can be a lot of work. Further, if the page changes some of the classes and structures may change and break our code 😅\n\n\nCode\n# pane is what we're scraping, but I wrote my functions to work for page if you click into the job's url (whole page job listing)\nscrape_job_details_df &lt;- function(driver, id = NULL, view = c(\"pane\", \"page\")){\n  html &lt;- scrape_html(driver)\n  html_job_details &lt;- extract_job_details_html(html, view)\n  html_top_card &lt;- extract_top_card_html(html_job_details, view)\n  df_job_details &lt;- scrape_top_card_df(html_top_card)\n  html_job_desc_card &lt;- extract_job_desc_card_html(html_job_details)\n  df_job_desc &lt;- scrape_job_desc_df(html_job_desc_card)\n  df_job_details &lt;- dplyr::bind_cols(df_job_details, df_job_desc)\n  html_salary_card &lt;- extract_salary_card_html(html_job_details)\n  df_job_details &lt;- dplyr::bind_cols(\n    df_job_details,\n    scrape_salary_df(html_salary_card))\n  open_skills_button_id &lt;- get_skills_open_button_id(html_job_details)\n  open_skills_click_result &lt;- click_button(driver, \"id\", open_skills_button_id, nap_length = \"long\")\n  if(open_skills_click_result == \"Success\"){\n    html &lt;- scrape_html(driver)\n    html_skills_modal &lt;- extract_skills_modal_html(html)\n    close_skills_button_id &lt;- get_skills_close_button_id(html_skills_modal)\n    close_skills_click_result &lt;- click_button(driver, \"id\", close_skills_button_id, nap_length = \"short\")\n    if(close_skills_click_result != \"Success\"){driver$client$goBack()}\n    skills &lt;- scrape_skills_chr(html_skills_modal)\n    df_job_details$skills &lt;- list(skills)\n  } else {\n    skills &lt;- NA_character_\n    df_job_details$skills &lt;- list(skills)\n  }\n  df_job_details$id &lt;- id\n  return(df_job_details)\n}\nextract_job_details_html &lt;- function(html, view = c(\"pane\", \"page\")){\n  view &lt;- match.arg(view, choices = c(\"pane\", \"page\"), several.ok = F)\n  if(view == \"pane\"){\n    class_pattern &lt;- \"scaffold-layout__detail\"\n  } else if(view == \"page\"){\n    class_pattern &lt;- \"job-view-layout jobs-details\"\n  } else{\n    class_pattern &lt;- \"jobs-details\"\n  }\n  job_details_html_index &lt;- tibble::tibble(class = html |&gt;  rvest::html_elements(\"body\") |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(class, pattern = class_pattern)) |&gt;\n    pull(rn)\n  html_job_details &lt;- html |&gt;  rvest::html_elements(\"body\") |&gt;  rvest::html_elements(\"div\") |&gt; _[job_details_html_index]\n  return(html_job_details)\n}\nextract_top_card_html &lt;- function(html_job_details, view = c(\"pane\", \"page\")){\n  view &lt;- match.arg(view, choices = c(\"pane\", \"page\"), several.ok = F)\n  if(view == \"pane\"){\n    top_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n      dplyr::mutate(rn = dplyr::row_number()) |&gt;\n      filter(class == \"jobs-unified-top-card__content--two-pane\") |&gt;\n      pull(rn)\n    html_top_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[top_card_html_index]\n  } else if(view == \"page\"){\n    top_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n      dplyr::mutate(rn = dplyr::row_number()) |&gt;\n      filter(stringr::str_detect(class, \"jobs-unified-top-card*.*artdeco-card\")) |&gt;\n      pull(rn)\n    html_top_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[top_card_html_index]\n  }\n  return(html_top_card)\n}\nscrape_top_card_df &lt;- function(html_top_card){\n  title &lt;- html_top_card |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_text() |&gt; str_squish() |&gt; _[1]\n  subline &lt;- html_top_card |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_text() |&gt; str_squish() |&gt; _[2]\n  company_name &lt;- str_split_i(subline, pattern = \"·\", i = 1) |&gt; str_squish()\n  location &lt;- str_extract(subline, pattern = \"([A-Z][a-z]+\\\\s?)+,\\\\s[A-Z]{2}\")\n  workplace_type &lt;- if_else(\n    str_detect(str_to_lower(subline), pattern = \"remote\"),\n    \"remote\",\n    if_else(\n      str_detect(str_to_lower(subline), pattern = \"hybrid\"),\n      \"hybrid\",\n      if_else(\n        str_detect(str_to_lower(subline), pattern = \"on-site\"),\n        \"on-site\",\n        \"not provided\")))\n  df_list_items &lt;- tibble::tibble(\n    class = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_attr(\"class\"),\n    text = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_text(),\n    icon_type = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_element(\"li-icon\") |&gt; html_attr(\"type\")\n  )\n  employment_type &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"job\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"Full-time|Part-time|Contract|Volunteer|Temporary|Internship|Other\")\n  job_level &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"job\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"Internship|Entry Level|Associate|Mid-Senior level|Director|Executive\")\n  company_size &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"company\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"[0-9]{1,},?[0-9]*(-|\\\\+)([0-9]{1,},?[0-9]*)?\")\n  company_industry &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"company\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_split_i(pattern = \"·\", i = 2) |&gt;\n    str_squish()\n  df_job_details &lt;- tibble::tibble(\n    job_title = if_length0_NA_character(title),\n    job_level = if_length0_NA_character(job_level),\n    company_name = if_length0_NA_character(company_name),\n    company_industry = if_length0_NA_character(company_industry),\n    company_size = if_length0_NA_character(company_size),\n    location = if_length0_NA_character(location),\n    workplace_type = if_length0_NA_character(workplace_type),\n    employment_type = if_length0_NA_character(employment_type)\n  )\n  return(df_job_details)\n}\nextract_job_desc_card_html &lt;- function(html_job_details){\n  job_desc_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(stringr::str_detect(class, \"jobs-description-content\")) |&gt;\n    pull(rn)\n  html_job_desc_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_elements(\"div\") |&gt;  _[job_desc_card_html_index]\n  return(html_job_desc_card)\n}\nscrape_job_desc_df &lt;- function(html_job_desc_card){\n  job_desc &lt;- html_job_desc_card |&gt; html_text() |&gt; paste(collapse = (\" \")) |&gt;  stringr::str_squish()\n  df_job_desc &lt;- tibble::tibble(job_desc = job_desc)\n  return(df_job_desc)\n}\nextract_salary_card_html &lt;- function(html_job_details){\n  salary_card_html_index &lt;- tibble::tibble(id = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"id\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(id == \"SALARY\") |&gt;\n    pull(rn)\n  html_salary_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[salary_card_html_index]\n  return(html_salary_card)\n}\nscrape_salary_df &lt;- function(html_salary_card){\n  salary_text_index &lt;- html_salary_card |&gt; rvest::html_elements(\"p\") |&gt; rvest::html_text() |&gt; stringr::str_detect(pattern = \"(from job description)|salary\")\n  salary_text &lt;- html_salary_card |&gt; rvest::html_elements(\"p\") |&gt; rvest::html_text() |&gt; _[salary_text_index] |&gt; stringr::str_squish()\n  if(length(salary_text) != 0){\n    salary_interval &lt;- str_extract_all(salary_text, pattern = \"[1-9][0-9]?[0-9]?,?[0-9]?[0-9]?[0-9]?\") |&gt; purrr::map(str_remove, pattern = \",\") |&gt; purrr::map(as.double)\n    df_salary &lt;- tibble(salary_min = min(unlist(salary_interval)), salary_max = max(unlist(salary_interval)))\n  } else {\n    df_salary &lt;- tibble(salary_min = NA_real_, salary_max = NA_real_)\n  }\n  return(df_salary)\n}\nget_skills_open_button_id &lt;- function(html_job_details){\n  skills_box_html_index &lt;- tibble::tibble(\n    class = html_job_details |&gt;   html_elements(\"div\")  |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(str_to_lower(class), pattern = \"job-details-how-you-match-card__container\")) |&gt;\n    pull(rn)\n  html_skills_box &lt;-html_job_details |&gt; html_elements(\"div\") |&gt; _[skills_box_html_index]\n  button_id &lt;- html_skills_box |&gt; html_elements(\"button\") |&gt; html_attr(\"id\")\n  return(button_id)\n}\nget_skills_close_button_id &lt;- function(html_skills_modal){\n  html_x_button_index &lt;- tibble::tibble(\n    aria_label = html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"aria-label\"),\n    id = html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"id\")\n  ) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_to_lower(aria_label) == \"dismiss\") |&gt;\n    pull(rn)\n  x_button_id &lt;- html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"id\") |&gt; _[html_x_button_index]\n}\nextract_skills_modal_html &lt;- function(html){\n  skills_modal_index &lt;- tibble::tibble(\n    role = html |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"role\"),\n    class = html |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")\n    ) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(class, \"job-details-skill-match-modal\") & role == \"dialog\") |&gt;\n    pull(rn)\n  html_skills_modal &lt;- html |&gt;  rvest::html_elements(\"div\") |&gt; _[skills_modal_index]\n  return(html_skills_modal)\n}\nscrape_skills_chr &lt;- function(html_skills_modal){\n  skills &lt;- html_skills_modal |&gt;\n    html_elements(\"li\") |&gt;\n    html_text() |&gt;\n    str_squish() |&gt;\n    str_remove(pattern = \"Add$\") |&gt;\n    str_squish()\n  skills &lt;- skills[skills != \"\"]\n  return(skills)\n}\nif_length0_NA_character &lt;- function(var){\n  if(length(var) == 0){\n    x &lt;- NA_character_\n  } else {\n    x &lt;- var\n  }\n  return(x)\n}\nclick_button &lt;- function(driver, using = c(\"xpath\", \"css selector\", \"id\", \"name\", \"tag name\", \"class name\", \"link text\", \"partial link text\"),  value, nap_length = \"short\"){\n  url &lt;- driver$client$getCurrentUrl()\n  click &lt;- function(driver, using, value, nap_length){\n    driver$client$findElement(using = using, value = value)$getElementLocationInView()\n    driver$client$findElement(using = using, value = value)$clickElement()\n    nap_rnorm(nap_length)\n    message &lt;- \"Success\"\n    return(message)\n  }\n  possibly_click &lt;- possibly(insistently(click, rate = rate_backoff(pause_base = 5, pause_cap = 45, max_times = 3, jitter = T), quiet = FALSE), otherwise = url)\n  click_result &lt;- possibly_click(driver = driver, using = using, value = value, nap_length = nap_length)\n  return(click_result)\n}\nnap_rnorm &lt;- function(length = c(\"short\", \"moderate\", \"long\")){\n  if(length[1] == \"short\"){\n    wait &lt;- abs(rnorm(n = 1, mean = 1.25, sd = .25))\n  } else if(length[1] == \"moderate\"){\n    wait &lt;- abs(rnorm(n = 1, mean = 3, sd = .5))\n  } else if(length[1] == \"long\") {\n    wait &lt;- abs(rnorm(n = 1, mean = 6, sd = 1.5))\n  } else{\n    wait &lt;- length\n  }\n  naptime::naptime(time = wait)\n}\n\ndf_job_details &lt;- scrape_job_details_df(driver)\nglimpse(df_job_details)\n\n\nRows: 1\nColumns: 12\n$ job_title        &lt;chr&gt; \"Data Analyst - Machine Learning\"\n$ job_level        &lt;chr&gt; \"Mid-Senior level\"\n$ company_name     &lt;chr&gt; \"AltaSource Group\"\n$ company_industry &lt;chr&gt; \"IT Services and IT Consulting\"\n$ company_size     &lt;chr&gt; \"51-200\"\n$ location         &lt;chr&gt; NA\n$ workplace_type   &lt;chr&gt; \"remote\"\n$ employment_type  &lt;chr&gt; \"Contract\"\n$ job_desc         &lt;chr&gt; \"About the job Data Analyst - Machine Learning AltaSo…\n$ salary_min       &lt;dbl&gt; NA\n$ salary_max       &lt;dbl&gt; NA\n$ skills           &lt;list&gt; &lt;\"Data Analysis\", \"Data Mining\", \"Data Science\", \"Mac…\n\n\nAll that code scraped details for one job, but we’ll want to do the same for all the jobs in df_jobs. We can create\n\n\nPaging\ndf_jobs only has the jobs on the first page, which means we’ll have to navigate through all the pages of job lists if we want data for all the jobs returned by the search. In order to do that, we first need to figure out how many times we have to page - which we can find at the bottom of the jobs list page with the page numbers. We can use developer tools to find the HTML tags and attributes of those page numbers. The HTML tag is li (list), which is what the page numbers at the bottom are in. We can use a similar approach as before to scrape this list, and again, filter to the items that match what we want. This gives use a list of ID’s we can use to page through. That said, you might notice that we don’t have all the pages. That’s because the site only displays some pages, a …, and then the last page number. I used lag() from dplyr to find the ID that is the ellipsis because it doesn’t have a data-test-pagination-page-btn attribute. That makes it simple to keep that HTML id in my data frame so I can easily click on the ellipsis and continue paging rather than skipping over all the pages inside the ellipsis. I’ll still need to pull the HTML id’s for additional pages inside the ellipsis, but we can do that as we page through…\n\n\nCode\nscrape_html &lt;- function(driver){\n  html &lt;- rvest::read_html(driver$client$getPageSource()[[1]])\n  return(html)\n}\nscrape_pages_df &lt;- function(html){\n  df_pages &lt;- tibble::tibble(\n    pagination_btn = html |&gt; rvest::html_element(\"body\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"data-test-pagination-page-btn\"),\n    id = html |&gt; rvest::html_elements(\"body\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"id\")\n  ) |&gt;\n    dplyr::mutate(pagination_btn = as.numeric(pagination_btn)) |&gt;\n    dplyr::mutate(pagination_btn_ellipsis = lag(pagination_btn) +1) |&gt;\n    dplyr::mutate(pagination_btn = coalesce(pagination_btn, pagination_btn_ellipsis),   .keep = \"unused\") |&gt;\n    dplyr::filter(stringr::str_detect(id, \"ember\") & !is.na(pagination_btn))\n  return(df_pages)\n}\nhtml &lt;- scrape_html(driver)\ndf_pages &lt;- scrape_pages_df(html)\npages &lt;- seq.int(from = 1, to = max(df_pages$pagination_btn), by = 1)\nprint(pages)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21\n\n\nWe can use the info in df_page and walk() to click the next page button.\n\n\nCode\npurrr::walk(pages[1:3], function(i){\n    df_pages &lt;- scrape_pages_df(scrape_html(driver)) |&gt; dplyr::filter(pagination_btn == i + 1)\n    click_button(driver, \"id\", df_pages$id, nap_length = \"long\")\n    print(glue::glue(\"Selenium is now on page {df_pages$pagination_btn} at {driver$client$getCurrentUrl()}\"))\n})\n\n\nSelenium is now on page 2 at https://www.linkedin.com/jobs/search/?currentJobId=3616925357&f_WT=2%2C3&geoId=90000079&keywords=data%20analyst%20%7C%20data%20scientist&refresh=T&start=25\nSelenium is now on page 3 at https://www.linkedin.com/jobs/search/?currentJobId=3634448847&f_WT=2%2C3&geoId=90000079&keywords=data%20analyst%20%7C%20data%20scientist&refresh=T&start=50\nSelenium is now on page 4 at https://www.linkedin.com/jobs/search/?currentJobId=3679326687&f_WT=2%2C3&geoId=90000079&keywords=data%20analyst%20%7C%20data%20scientist&refresh=T&start=75\n\n\n\n\nFunctionalize\nIf you’re still with me you’re a trooper. I put everything we’ve done so far in Selenium into functions so that I can run our scraper with a few function calls.\n\n\nCode\n# Driver ####\nopen_driver &lt;- function(port = 1443L, browser = \"chrome\", check = F, verbose = F){\n  driver &lt;- rsDriver(port = port, browser = browser, check = check, verbose = F)\n  driver$client$maxWindowSize()\n  return(driver)\n}\nkill_driver &lt;- function(driver){\n  driver$server$process$kill()\n  rm(driver, envir = rlang::global_env())\n}\nnap_rnorm &lt;- function(length = c(\"short\", \"moderate\", \"long\")){\n  if(length[1] == \"short\"){\n    wait &lt;- abs(rnorm(n = 1, mean = 1.25, sd = .25))\n  } else if(length[1] == \"moderate\"){\n    wait &lt;- abs(rnorm(n = 1, mean = 3, sd = .5))\n  } else if(length[1] == \"long\") {\n    wait &lt;- abs(rnorm(n = 1, mean = 6, sd = 1.5))\n  } else{\n    wait &lt;- length\n  }\n  naptime::naptime(time = wait)\n}\n# Interact ####\nsignin &lt;- function(driver, session_key = Sys.getenv(\"LINKEDIN_UID\"), session_password = Sys.getenv(\"LINKEDIN_PWD\")){\n  url &lt;- \"https://www.linkedin.com/login?fromSignIn\"\n  driver$client$navigate(url)\n  nap_rnorm(\"moderate\")\n  driver$client$findElement(\"name\", \"session_key\")$sendKeysToElement(list(session_key))\n  driver$client$findElement(\"name\", \"session_password\")$sendKeysToElement(list(session_password))\n  driver$client$findElement(\"tag name\", \"BUTTON\")$clickElement()\n  nap_rnorm(\"moderate\")\n  message(glue::glue(\"Selenium is now at {driver$client$getCurrentUrl()}\"))\n}\nminimize_messaging &lt;- function(driver){\n  html &lt;- scrape_html(driver)\n  aside_index &lt;- html |&gt; rvest::html_elements(\"aside\") |&gt; rvest::html_attr(\"class\") |&gt; stringr::str_detect(pattern = \"msg-overlay\") |&gt; which()\n  html_aside &lt;- html |&gt; rvest::html_elements(\"aside\") |&gt; _[aside_index]\n  df_messaging &lt;- tibble::tibble(\n    id = html_aside |&gt; rvest::html_elements(\"button\") |&gt; rvest::html_attr(\"id\"),\n    icon_type = html_aside |&gt; rvest::html_elements(\"button\") |&gt; rvest::html_element(\"li-icon\") |&gt; rvest::html_attr(\"type\")\n  ) |&gt;\n    dplyr::filter(icon_type == \"chevron-down\")\n  if(nrow(df_messaging) == 1){\n    id &lt;- df_messaging$id\n    click_minimize &lt;- function(driver, id){\n      driver$client$findElement(\"id\", id)$clickElement()\n      return(\"Sucessefully minimized\")\n    }\n    possibly_click_minimize &lt;- possibly(click_minimize)\n    minimized &lt;- possibly_click_minimize(driver = driver, id = id)\n    nap_rnorm(\"short\")\n  }\n}\nsearch_jobs &lt;- function(driver, keywords = NULL, date_posted = c(\"anytime\", \"month\", \"week\", \"24hrs\"), workplace_types = c(\"on-site\", \"remote\", \"hybrid\"), location = NULL, distance_mi = 5){\n  assertthat::assert_that(!is.null(keywords))\n  keywords = glue::glue(\"&keywords={keywords}\")\n  date_posted &lt;- get_datePosted(date_posted)\n  workplace_types &lt;- get_workPlace_types(workplace_types)\n  if(!is.null(location)){\n    location &lt;- glue::glue(\"&location={location}\")\n  } else{\n    location &lt;- \"\"\n  }\n  if((\"on-site\" %in% workplace_types | \"hybrid\" %in% workplace_types) & !is.null(distance_mi)){\n    distance &lt;- glue::glue(\"&distance={distance_mi}\")\n  } else{\n    distance &lt;- \"\"\n  }\n  jobs_url &lt;- URLencode(glue::glue(\"https://www.linkedin.com/jobs/search/?{date_posted}{workplace_types}{distance}{location}{keywords}&refresh=T\"))\n  driver$client$navigate(jobs_url)\n  nap_rnorm(\"moderate\")\n  message(glue::glue(\"Selenium is now at {jobs_url}\"))\n  return(jobs_url)\n}\nget_datePosted &lt;- function(arg){\n  choices &lt;- c(\"anytime\", \"month\", \"week\", \"24hrs\")\n  arg &lt;- match.arg(arg, choices = choices, several.ok = F)\n  if(arg == \"anytime\"){\n    date_posted = \"\"\n  } else if(arg == \"month\"){\n    date_posted = \"f_TPR=r604800\"\n  } else if(arg == \"week\"){\n    date_posted = \"f_TPR=r2592000\"\n  } else if(arg == \"24hrs\"){\n    date_posted = \"f_TPR=r86400\"\n  } else {\n    message(\"Something went wrong with get_datePosted()\")\n  }\n  return(date_posted)\n}\nget_workPlace_types &lt;- function(args){\n  choices &lt;- c(\"on-site\", \"remote\", \"hybrid\")\n  args &lt;- match.arg(args, choices = choices, several.ok = T)\n  args &lt;- ifelse(args == \"on-site\"\n                            , \"1\", ifelse(args == \"remote\", \"2\", ifelse(args == \"hybrid\", \"3\", NA_character_)))\n  args &lt;- paste0(\"&f_WT=\", paste(args, collapse = \",\"))\n  return(args)\n}\nscrollto_element &lt;- function(driver, using = c(\"xpath\", \"css selector\", \"id\", \"name\", \"tag name\", \"class name\", \"link text\", \"partial link text\"), value, nap_length = \"short\"){\n  html &lt;- scrape_html(driver)\n  df_pages &lt;- scrape_pages_df(html)\n  scrollto &lt;- function(driver, using, value, nap_length){\n    webElem &lt;- driver$client$findElement(using, value)\n    webElem$getElementLocationInView()\n    nap_rnorm(nap_length)\n    return(\"Success\")\n  }\n  possibly_scrollto &lt;- possibly(insistently(scrollto, rate = rate_backoff(pause_base = 5, pause_cap = 45, max_times = 3, jitter = T), quiet = FALSE), otherwise = \"Fail\")\n  scrollto_result &lt;- possibly_scrollto(driver = driver, using = using, value = value, nap_length = nap_length)\n  return(scrollto_result)\n}\nscrollto_paging &lt;- function(driver){\n  html &lt;- scrape_html(driver)\n  df_pages &lt;- scrape_pages_df(html)\n  scrollto_element(driver, \"id\", df_pages$id[1])\n}\nload_jobs_list &lt;- function(driver, n_jobs = 0){\n  if(n_jobs == 0){\n    scrollto_paging(driver)\n  }\n  df_jobs_list &lt;- scrape_jobs_list_df(driver)\n  if(n_jobs &lt; nrow(df_jobs_list)){\n    randwalk_jobs(driver, df_jobs_list$id)\n    load_jobs_list(driver, n_jobs = nrow(df_jobs_list))\n  } else {\n    message(glue::glue(\"Loaded {n_jobs} jobs\"))\n  }\n}\nrandwalk_jobs &lt;- function(driver, ids){\n  ids &lt;- ids |&gt;\n    sort() |&gt;\n    sample()\n  purrr::walk(ids, function(id){\n    scrollto_element(driver, \"id\", id, nap_length = .25)\n  })\n}\nclick_button &lt;- function(driver, using = c(\"xpath\", \"css selector\", \"id\", \"name\", \"tag name\", \"class name\", \"link text\", \"partial link text\"),  value, nap_length = \"short\"){\n  url &lt;- driver$client$getCurrentUrl()\n  click &lt;- function(driver, using, value, nap_length){\n    driver$client$findElement(using = using, value = value)$getElementLocationInView()\n    driver$client$findElement(using = using, value = value)$clickElement()\n    nap_rnorm(nap_length)\n    message &lt;- \"Success\"\n    return(message)\n  }\n  possibly_click &lt;- possibly(insistently(click, rate = rate_backoff(pause_base = 5, pause_cap = 45, max_times = 3, jitter = T), quiet = FALSE), otherwise = url)\n  click_result &lt;- possibly_click(driver = driver, using = using, value = value, nap_length = nap_length)\n  return(click_result)\n}\n\n# Scrape #### \nscrape_html &lt;- function(driver){\n  html &lt;- rvest::read_html(driver$client$getPageSource()[[1]])\n  return(html)\n}\nif_length0_NA_character &lt;- function(var){\n  if(length(var) == 0){\n    x &lt;- NA_character_\n  } else {\n    x &lt;- var\n  }\n  return(x)\n}\nscrape_pages_df &lt;- function(html){\n  df_pages &lt;- tibble::tibble(\n    pagination_btn = html |&gt; rvest::html_element(\"body\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"data-test-pagination-page-btn\"),\n    id = html |&gt; rvest::html_elements(\"body\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"id\")\n  ) |&gt;\n    dplyr::mutate(pagination_btn = as.numeric(pagination_btn)) |&gt;\n    dplyr::mutate(pagination_btn_ellipsis = lag(pagination_btn) +1) |&gt;\n    dplyr::mutate(pagination_btn = coalesce(pagination_btn, pagination_btn_ellipsis),   .keep = \"unused\") |&gt;\n    dplyr::filter(stringr::str_detect(id, \"ember\") & !is.na(pagination_btn))\n  return(df_pages)\n}\nscrape_jobs_list_df &lt;- function(driver){\n  html &lt;- scrape_html(driver = driver)\n  job_li_ids &lt;- get_li_ids(html) |&gt; remove_blank_ids()\n  df_jobs_list &lt;- purrr::map(job_li_ids, function(li_id){\n    li_index &lt;- which(get_li_ids(html) == li_id)\n    li_element &lt;- get_li_elements(html, li_index)\n    id &lt;- li_element |&gt; rvest::html_elements(\"a\") |&gt; rvest::html_attr(\"id\")\n    class &lt;- li_element |&gt; rvest::html_elements(\"a\") |&gt; rvest::html_attr(\"class\")\n    href &lt;- li_element |&gt; rvest::html_elements(\"a\") |&gt; rvest::html_attr(\"href\")\n    url &lt;- paste0(\"https://www.linkedin.com/\", href)\n    img &lt;- li_element |&gt; rvest::html_element(\"div\") |&gt;rvest::html_element(\"img\") |&gt; rvest::html_attr(\"src\")\n    alt &lt;- li_element |&gt; rvest::html_elements(\"img\") |&gt; rvest::html_attr(\"alt\") |&gt; stringr::str_squish() |&gt; _[1]\n    text &lt;- li_element |&gt; rvest::html_elements(\"a\") |&gt; rvest::html_text() |&gt; stringr::str_squish()\n    tibble::tibble(\n      li_id = li_id,\n      id = id,\n      class = class,\n      url = url,\n      img = img,\n      alt = alt,\n      text = text,\n    )\n  }) |&gt;\n    purrr::list_rbind() |&gt;\n    dplyr::filter(stringr::str_detect(url, pattern = \"/jobs/view/\")) |&gt;\n    dplyr::filter(stringr::str_detect(class, pattern = \"job-card-list__title\"))\n  return(df_jobs_list)\n}\nget_li_ids &lt;- function(html){\n  li_ids &lt;- html |&gt; rvest::html_element(\"main\") |&gt; rvest::html_element(\"ul\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"id\")\n  return(li_ids)\n}\nremove_blank_ids &lt;- function(ids){\n  keep_ids &lt;- ids[!is.na(ids)]\n  return(keep_ids)\n}\nget_li_elements &lt;- function(html, index){\n  li_elements &lt;- html |&gt; rvest::html_element(\"main\") |&gt; rvest::html_element(\"ul\") |&gt; rvest::html_elements(\"li\")\n  li_element &lt;- li_elements[index]\n  return(li_element)\n}\nscrape_job_details_df &lt;- function(driver, id = NULL, view = c(\"pane\", \"page\")){\n  html &lt;- scrape_html(driver)\n  html_job_details &lt;- extract_job_details_html(html, view)\n  html_top_card &lt;- extract_top_card_html(html_job_details, view)\n  df_job_details &lt;- scrape_top_card_df(html_top_card)\n  html_job_desc_card &lt;- extract_job_desc_card_html(html_job_details)\n  df_job_desc &lt;- scrape_job_desc_df(html_job_desc_card)\n  df_job_details &lt;- dplyr::bind_cols(df_job_details, df_job_desc)\n  html_salary_card &lt;- extract_salary_card_html(html_job_details)\n  df_job_details &lt;- dplyr::bind_cols(\n    df_job_details,\n    scrape_salary_df(html_salary_card))\n  open_skills_button_id &lt;- get_skills_open_button_id(html_job_details)\n  open_skills_click_result &lt;- click_button(driver, \"id\", open_skills_button_id, nap_length = \"long\")\n  if(open_skills_click_result == \"Success\"){\n    html &lt;- scrape_html(driver)\n    html_skills_modal &lt;- extract_skills_modal_html(html)\n    close_skills_button_id &lt;- get_skills_close_button_id(html_skills_modal)\n    close_skills_click_result &lt;- click_button(driver, \"id\", close_skills_button_id, nap_length = \"short\")\n    if(close_skills_click_result != \"Success\"){driver$client$goBack()}\n    skills &lt;- scrape_skills_chr(html_skills_modal)\n    df_job_details$skills &lt;- list(skills)\n  } else {\n    skills &lt;- NA_character_\n    df_job_details$skills &lt;- list(skills)\n  }\n  df_job_details$id &lt;- id\n  return(df_job_details)\n}\n\nextract_job_details_html &lt;- function(html, view = c(\"pane\", \"page\")){\n  view &lt;- match.arg(view, choices = c(\"pane\", \"page\"), several.ok = F)\n  if(view == \"pane\"){\n    class_pattern &lt;- \"scaffold-layout__detail\"\n  } else if(view == \"page\"){\n    class_pattern &lt;- \"job-view-layout jobs-details\"\n  } else{\n    class_pattern &lt;- \"jobs-details\"\n  }\n  job_details_html_index &lt;- tibble::tibble(class = html |&gt;  rvest::html_elements(\"body\") |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(class, pattern = class_pattern)) |&gt;\n    pull(rn)\n  html_job_details &lt;- html |&gt;  rvest::html_elements(\"body\") |&gt;  rvest::html_elements(\"div\") |&gt; _[job_details_html_index]\n  return(html_job_details)\n}\nextract_top_card_html &lt;- function(html_job_details, view = c(\"pane\", \"page\")){\n  view &lt;- match.arg(view, choices = c(\"pane\", \"page\"), several.ok = F)\n  if(view == \"pane\"){\n    top_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n      dplyr::mutate(rn = dplyr::row_number()) |&gt;\n      filter(class == \"jobs-unified-top-card__content--two-pane\") |&gt;\n      pull(rn)\n    html_top_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[top_card_html_index]\n  } else if(view == \"page\"){\n    top_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n      dplyr::mutate(rn = dplyr::row_number()) |&gt;\n      filter(stringr::str_detect(class, \"jobs-unified-top-card*.*artdeco-card\")) |&gt;\n      pull(rn)\n    html_top_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[top_card_html_index]\n  }\n  return(html_top_card)\n}\nscrape_top_card_df &lt;- function(html_top_card){\n  title &lt;- html_top_card |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_text() |&gt; str_squish() |&gt; _[1]\n  subline &lt;- html_top_card |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_text() |&gt; str_squish() |&gt; _[2]\n  company_name &lt;- str_split_i(subline, pattern = \"·\", i = 1) |&gt; str_squish()\n  location &lt;- str_extract(subline, pattern = \"([A-Z][a-z]+\\\\s?)+,\\\\s[A-Z]{2}\")\n  workplace_type &lt;- if_else(\n    str_detect(str_to_lower(subline), pattern = \"remote\"),\n    \"remote\",\n    if_else(\n      str_detect(str_to_lower(subline), pattern = \"hybrid\"),\n      \"hybrid\",\n      if_else(\n        str_detect(str_to_lower(subline), pattern = \"on-site\"),\n        \"on-site\",\n        \"not provided\")))\n  df_list_items &lt;- tibble::tibble(\n    class = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_attr(\"class\"),\n    text = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_text(),\n    icon_type = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_element(\"li-icon\") |&gt; html_attr(\"type\")\n  )\n  employment_type &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"job\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"Full-time|Part-time|Contract|Volunteer|Temporary|Internship|Other\")\n  job_level &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"job\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"Internship|Entry Level|Associate|Mid-Senior level|Director|Executive\")\n  company_size &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"company\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"[0-9]{1,},?[0-9]*(-|\\\\+)([0-9]{1,},?[0-9]*)?\")\n  company_industry &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"company\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_split_i(pattern = \"·\", i = 2) |&gt;\n    str_squish()\n  df_job_details &lt;- tibble::tibble(\n    job_title = if_length0_NA_character(title),\n    job_level = if_length0_NA_character(job_level),\n    company_name = if_length0_NA_character(company_name),\n    company_industry = if_length0_NA_character(company_industry),\n    company_size = if_length0_NA_character(company_size),\n    location = if_length0_NA_character(location),\n    workplace_type = if_length0_NA_character(workplace_type),\n    employment_type = if_length0_NA_character(employment_type)\n  )\n  return(df_job_details)\n}\nextract_job_desc_card_html &lt;- function(html_job_details){\n  job_desc_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(stringr::str_detect(class, \"jobs-description-content\")) |&gt;\n    pull(rn)\n  html_job_desc_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_elements(\"div\") |&gt;  _[job_desc_card_html_index]\n  return(html_job_desc_card)\n}\nscrape_job_desc_df &lt;- function(html_job_desc_card){\n  job_desc &lt;- html_job_desc_card |&gt; html_text() |&gt; paste(collapse = (\" \")) |&gt;  stringr::str_squish()\n  df_job_desc &lt;- tibble::tibble(job_desc = job_desc)\n  return(df_job_desc)\n}\nextract_salary_card_html &lt;- function(html_job_details){\n  salary_card_html_index &lt;- tibble::tibble(id = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"id\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(id == \"SALARY\") |&gt;\n    pull(rn)\n  html_salary_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[salary_card_html_index]\n  return(html_salary_card)\n}\nscrape_salary_df &lt;- function(html_salary_card){\n  salary_text_index &lt;- html_salary_card |&gt; rvest::html_elements(\"p\") |&gt; rvest::html_text() |&gt; stringr::str_detect(pattern = \"(from job description)|salary\")\n  salary_text &lt;- html_salary_card |&gt; rvest::html_elements(\"p\") |&gt; rvest::html_text() |&gt; _[salary_text_index] |&gt; stringr::str_squish()\n  if(length(salary_text) != 0){\n    salary_interval &lt;- str_extract_all(salary_text, pattern = \"[1-9][0-9]?[0-9]?,?[0-9]?[0-9]?[0-9]?\") |&gt; purrr::map(str_remove, pattern = \",\") |&gt; purrr::map(as.double)\n    df_salary &lt;- tibble(salary_min = min(unlist(salary_interval)), salary_max = max(unlist(salary_interval)))\n  } else {\n    df_salary &lt;- tibble(salary_min = NA_real_, salary_max = NA_real_)\n  }\n  return(df_salary)\n}\nget_skills_open_button_id &lt;- function(html_job_details){\n  skills_box_html_index &lt;- tibble::tibble(\n    class = html_job_details |&gt;   html_elements(\"div\")  |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(str_to_lower(class), pattern = \"job-details-how-you-match-card__container\")) |&gt;\n    pull(rn)\n  html_skills_box &lt;-html_job_details |&gt; html_elements(\"div\") |&gt; _[skills_box_html_index]\n  button_id &lt;- html_skills_box |&gt; html_elements(\"button\") |&gt; html_attr(\"id\")\n  return(button_id)\n}\nget_skills_close_button_id &lt;- function(html_skills_modal){\n  html_x_button_index &lt;- tibble::tibble(\n    aria_label = html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"aria-label\"),\n    id = html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"id\")\n  ) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_to_lower(aria_label) == \"dismiss\") |&gt;\n    pull(rn)\n  x_button_id &lt;- html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"id\") |&gt; _[html_x_button_index]\n}\nextract_skills_modal_html &lt;- function(html){\n  skills_modal_index &lt;- tibble::tibble(\n    role = html |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"role\"),\n    class = html |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")\n    ) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(class, \"job-details-skill-match-modal\") & role == \"dialog\") |&gt;\n    pull(rn)\n  html_skills_modal &lt;- html |&gt;  rvest::html_elements(\"div\") |&gt; _[skills_modal_index]\n  return(html_skills_modal)\n}\nscrape_skills_chr &lt;- function(html_skills_modal){\n  skills &lt;- html_skills_modal |&gt;\n    html_elements(\"li\") |&gt;\n    html_text() |&gt;\n    str_squish() |&gt;\n    str_remove(pattern = \"Add$\") |&gt;\n    str_squish()\n  skills &lt;- skills[skills != \"\"]\n  return(skills)\n}\nscrape_jobs_df &lt;- function(driver){\n  load_jobs_list(driver)\n  df_job_list &lt;- scrape_jobs_list_df(driver)\n  df_job_details &lt;- map(df_job_list$id, function(id){\n    click_job_result &lt;- click_button(driver, \"id\", id, nap_length = \"moderate\")\n    if(click_job_result == \"Success\"){\n      df_job_details &lt;- scrape_job_details_df(driver, id, view = \"pane\")\n    } else {\n      df_job_details &lt;- get_df_job_details_schema(id = id)\n    }\n    return(df_job_details)\n  }) |&gt; bind_rows()\n  df_jobs &lt;- dplyr::left_join(df_job_list, df_job_details, by = c(\"id\" = \"id\"))\n  details_cols &lt;- get_df_job_details_schema() |&gt; select(-id) |&gt; names()\n  filter_blank_details &lt;- . %&gt;%\n    dplyr::filter(dplyr::if_all(.cols = details_cols, ~is.na(.)))\n  remove_details_columns &lt;- . %&gt;%\n    select(-tidyselect::all_of(details_cols))\n  df_missing_details &lt;- df_jobs |&gt;\n    filter_blank_details() |&gt;\n    remove_details_columns()\n  if(nrow(df_missing_details)&gt;0){\n    df_jobs &lt;- df_jobs |&gt; filter(id %in% df_missing_details$id)\n    df_retry_details &lt;-purrr::map2(df_missing_details$id, df_missing_details$url, function(id, url){\n      click_job_result &lt;- click_button(driver, \"id\", id, nap_length = \"moderate\")\n      driver$client$navigate(url)\n      nap_rnorm(\"long\")\n      df_retry &lt;- scrape_job_details_df(driver, id = id, view = \"page\")\n      return(df_retry)\n    })\n    df_missing_details &lt;- dplyr::left_join(df_missing_details, df_retry_details, by = c(\"id\" = \"id\"))\n    df_jobs &lt;- dplyr::bind_rows(df_jobs, df_missing_details)\n  }\n  return(df_jobs)\n}\nscrape_job_search &lt;- function(driver, page_cap = NULL){\n  df_pages &lt;- scrape_pages_df(scrape_html(driver))\n  pages &lt;- seq.int(from = 1, to = min(max(df_pages$pagination_btn), page_cap), by = 1)\n  message(glue::glue(\"Preparing to scrape {max(pages)} pages...\"))\n  df_job_search &lt;- purrr::map(pages, function(i){\n    df_jobs &lt;- scrape_jobs_df(driver)\n    df_pages &lt;- scrape_pages_df(scrape_html(driver)) |&gt; dplyr::filter(pagination_btn == i + 1)\n    if(nrow(df_pages) == 1){\n      page_click_result &lt;- click_button(driver, \"id\", df_pages$id, nap_length = \"long\")\n      if(page_click_result == \"Success\"){\n        message(glue::glue(\"Selenium is now on page {df_pages$pagination_btn} at {driver$client$getCurrentUrl()}\"))\n      } else {\n        next_page_url &lt;- modify_job_search_url_page_n(driver, n = i + 1)\n        driver$client$navigate(next_page_url)\n        message(glue::glue(\"scrape_pages_df() did not get a matching page, using alternative navigation to go to page {i + 1} at {driver$client$getCurrentUrl()}\"))\n      }\n    } else{\n      message(\"Done\")\n    }\n    return(df_jobs)\n  }) |&gt; dplyr::bind_rows()\n  return(df_job_search)\n}\n\n# Utils ####\nget_df_job_details_schema &lt;- function(id = \"\"){\n  tibble::tibble(\n    job_title = NA_character_,\n    job_level = NA_character_,\n    company_name = NA_character_,\n    company_industry = NA_character_,\n    company_size = NA_character_,\n    location = NA_character_,\n    workplace_type = NA_character_,\n    employment_type = NA_character_,\n    salary_min = NA_character_,\n    salary_max = NA_character_,\n    skills = NA_character_,\n    id = id\n  )\n}\nget_job_search_page_n &lt;- function(driver){\n  n &lt;- as.numeric(stringr::str_remove(stringr::str_extract(driver$client$getCurrentUrl(), \"start=[1-9][0-9][0-9]?[0-9]?[0-9]?\"), \"start=\")) / 25\n  return(n)\n}\nmodify_job_search_url_page_n &lt;- function(driver, n = 1){\n  if(n &lt;= 1){\n    start &lt;- 1\n  } else {\n    start &lt;- n * 25 - 25\n  }\n  url &lt;- stringr::str_remove(driver$client$getCurrentUrl(), \"&start=[1-9][0-9][0-9]?[0-9]?[0-9]?\")\n  url &lt;- glue::glue(\"{url}&start={start}\")\n  return(url)\n}\nkill_driver(driver)\n\n\nWhich we can test out for a few pages of jobs with the script below to get all the data in one data frame 😎\n\n\nCode\ndriver &lt;- open_driver(port = 1446L)\nsignin(driver)\nminimize_messaging(driver)\nsearch_jobs(driver, keywords = \"data scientist\", date_posted = \"anytime\", location = \"United States\", workplace_types = c(\"remote\"))\ndf_jobs &lt;- scrape_job_search(driver, page_cap = 2)\nkill_driver(driver)\nglimpse(df_jobs)\n\n\n\n\nRows: 50\nColumns: 19\n$ li_id            &lt;chr&gt; \"ember200\", \"ember212\", \"ember225\", \"ember238\", \"embe…\n$ id               &lt;chr&gt; \"ember206\", \"ember218\", \"ember231\", \"ember244\", \"embe…\n$ class            &lt;chr&gt; \"disabled ember-view job-card-container__link job-car…\n$ url              &lt;chr&gt; \"https://www.linkedin.com//jobs/view/3636496441/?eBP=…\n$ img              &lt;chr&gt; \"https://media.licdn.com/dms/image/C4E0BAQGk-xkrdUoBp…\n$ alt              &lt;chr&gt; \"TRM Labs logo\", \"innoVet Health (SDVOSB) logo\", \"Hea…\n$ text             &lt;chr&gt; \"Senior Data Scientist\", \"Senior Data Scientist\", \"Se…\n$ job_title        &lt;chr&gt; \"Senior Data Scientist\", \"Senior Data Scientist\", \"Se…\n$ job_level        &lt;chr&gt; \"Mid-Senior level\", NA, NA, \"Mid-Senior level\", NA, N…\n$ company_name     &lt;chr&gt; \"TRM Labs\", \"innoVet Health (SDVOSB)\", \"HealthRhythms…\n$ company_industry &lt;chr&gt; \"Information Services\", NA, NA, \"Technology, Informat…\n$ company_size     &lt;chr&gt; \"51-200\", \"11-50\", \"11-50\", \"201-500\", \"51-200\", \"11-…\n$ location         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Los Angeles,…\n$ workplace_type   &lt;chr&gt; \"remote\", \"remote\", \"remote\", \"remote\", \"remote\", \"re…\n$ employment_type  &lt;chr&gt; \"Full-time\", \"Full-time\", \"Full-time\", \"Full-time\", \"…\n$ job_desc         &lt;chr&gt; \"About the job At TRM, we're on a mission to build a …\n$ salary_min       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 144000, NA, NA, 88, 12000…\n$ salary_max       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 157000, NA, NA, 98, 14000…\n$ skills           &lt;list&gt; &lt;\"Analytics\", \"Data Science\", \"Data Visualization\", …"
  },
  {
    "objectID": "posts/2023-08-08-web-scraping/index.html",
    "href": "posts/2023-08-08-web-scraping/index.html",
    "title": "Web Scraping w/ R",
    "section": "",
    "text": "I had a web scraping walk through on my old blog where I scraped Airbnb to find listings with king size beds, but Airbnb did major updates to their site and the post wouldn’t render when I did an overhaul to my website so I no longer have it. I was trying to do a little scraping while I was doing some research and found myself wanting my guide so I started writing a new one while I was working - I’m coming back to it now to finish it up as a post. I’m not going to bother covering some of the basic web development skills necessary because I coincidentally cover most of that in the prerequisites for my post on building a blogdown site. So without further delay, let’s get to scraping!"
  },
  {
    "objectID": "posts/2023-08-08-web-scraping/index.html#static-site",
    "href": "posts/2023-08-08-web-scraping/index.html#static-site",
    "title": "Web Scraping w/ R",
    "section": "Static Site",
    "text": "Static Site\nWhen scraping, as is true with many things done it code, it’s easiest to start small. Scraping static sites that do not heavily rely on JavaScript to function is significantly less complicated than scraping sites that rely on JavaScript or dynamic sites. With that in mind, we’ll start with a very simple static site, the CRAN packages page, because it’s structured in a way that’s ideal for scraping so it’s straight forward.\n\nParsing HTML\nThe rvest package has a suite of tools for parsing the HTML document, which is the core functionality required to scrape. The first thing to do when scraping a page is to figure out what we want to scrape and determine it’s HTML structure. We can do this using the browser’s web developer tools, but most of this can also be done inside RStudio. It probably goes without saying if you look at the CRAN packages page, but I’d like to scrape the packages table and make it a data frame.\n\n\n\nTo inspect the page, we first read the page using read_html(). This reads the HTML page as is into our rsession for processing. We can see that the cran_packages_html object is a list of length two and both objects inside the list are external pointers. In other words, the cran_packages_html document is not in the active rsession, rather a pointer which directs R to the documents created by libxml2 which are stored in RAM (at least this is my rough understanding of how it works). For more information, Bob Rudis provided a very detailed response about scraping which touches on this point, but the take away should be that this object does not contain the data from the HTML page - just pointers!\n\n\nCode\nlibrary(tidyverse) \nlibrary(rvest)\ncran_packages_html &lt;- read_html(\"https://cran.r-project.org/web/packages/available_packages_by_date.html\")\nstr(cran_packages_html)\n\n\nList of 2\n $ node:&lt;externalptr&gt; \n $ doc :&lt;externalptr&gt; \n - attr(*, \"class\")= chr [1:2] \"xml_document\" \"xml_node\"\n\n\n\n\n\n\n\n\nxml2 not attached\n\n\n\n\n\nAn aside, if you open cran_packages_html with viewer and trying to inspect one of the pointers, you’ll get an error could not find function \"xml_child\". That’s because rvest depends on xml2, but does not attached it to the global environment.\n\n\n\nYou can simply load xml2 to fix the issue.\n\n\nCode\nlibrary(xml2)\nxml_child(cran_packages_html, 2)\n\n\n{html_node}\n&lt;body lang=\"en\"&gt;\n[1] &lt;div class=\"container\"&gt;\\n&lt;h1&gt;Available CRAN Packages By Date of Publicati ...\n\n\n\n\n\nThe rvest package has a suite of functions for parsing the HTML document starting with functions that help use understand the structure including html_children() & html_name(). We can use html_children() to climb down the page and html_name() to see the tag names of the HTML elements we want to parse. For this page, we used html_chidren() to see that the page has a and a , which is pretty standard. We’ll want to scrape the &lt;body&gt; because that’s where the content of the page will be.\n\n\nCode\ncran_packages_html |&gt; \n  html_children() |&gt; \n  html_name()\n\n\n[1] \"head\" \"body\"\n\n\nTo further parse the &lt;body&gt;, we’ll use html_element() to clip the rest of the HTML document and look inside &lt;body&gt;. Within &lt;body&gt;, we can see there’s just a []](https://www.w3schools.com/tags/tag_div.asp).\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_children() |&gt; \n  html_name()\n\n\n[1] \"div\"\n\n\nWe can continue the process with the &lt;div&gt; and we see an an and a . It’s fairly obvious we’ll want to the &lt;table&gt;, not &lt;h1&gt;, but just to illustrate if we look within &lt;h1&gt;, we’ll see no nodes exist beneath it.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h1\") |&gt; \n  html_children() |&gt; \n  html_name()\n\n\ncharacter(0)\n\n\nThat doesn’t mean &lt;h1&gt; has no data, it just means no HTML is a child of &lt;h1&gt;. Since &lt;h1&gt; a tag used on titles text), we can use [html_text()] to extract the actual text inside. This isn’t particularly useful in this case, but html_text() can be very useful.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"h1\") |&gt; \n  html_text() \n\n\n[1] \"Available CRAN Packages By Date of Publication\"\n\n\nIf we use html_element(\"table\"), we can see it contains the data we’re looking for, but there’s a bit of HTML junk we’ll need to clean up for our data frame.\n\n\nCode\ncran_packages_html |&gt;  \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"table\") \n\n\n{html_node}\n&lt;table border=\"1\"&gt;\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DrugSim2DR/ ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/gmapsdistan ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/actxps/inde ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/AgroR/index ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/aplot/index ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/av/index.ht ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/basemodels/ ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bayesPop/in ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Bayesrel/in ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/beanz/index ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bookdown/in ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bruceR/inde ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/canvasXpres ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CARBayes/in ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/clinDR/inde ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cmm/index.h ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/complexlm/i ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CPC/index.h ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/dfoliatR/in ...\n...\n\n\n\n\n\n\n\n\nhtml_element() will jump nodes\n\n\n\n\n\nIn the code above, we walked down the whole HTML tree body &gt; div &gt; table. The html_element() function will pickup HTML tags without providing the exact path, which is very convenient but can lead to unexpected results. The code below leads to the same results, but only because this page only has one HTML table. If it had multiple, it would only pick up the first one whether that was our intent or not. This point is very important to understand for more complicated web pages.\n\n\nCode\ncran_packages_html |&gt;   \n  # Skipped &lt;body&gt; & &lt;div&gt;\n  html_element(\"table\") \n\n\n{html_node}\n&lt;table border=\"1\"&gt;\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DrugSim2DR/ ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/gmapsdistan ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/actxps/inde ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/AgroR/index ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/aplot/index ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/av/index.ht ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/basemodels/ ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bayesPop/in ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Bayesrel/in ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/beanz/index ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bookdown/in ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bruceR/inde ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/canvasXpres ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CARBayes/in ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/clinDR/inde ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cmm/index.h ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/complexlm/i ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CPC/index.h ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/dfoliatR/in ...\n...\n\n\n\n\n\nFortunately, rvest has a handy html_table() function that’s specifically for HTML tables and automatically coerces them into a list of tibbles. I used bind_rows() to coerce the list to a tibble. As you can see below, we end up with a table of packages with a date, package name, and title.\n\n\nCode\nlibrary(tidyverse) \nlibrary(reactable)\ncran_packages_df &lt;- cran_packages_html |&gt; \n  html_table() |&gt; \n  bind_rows()\n\ncran_packages_df |&gt; \n  reactable(\n    searchable = TRUE, \n    paginationType = \"jump\",  \n    showPageSizeOptions = TRUE,\n    pageSizeOptions = c(5, 10, 50, 100),\n    defaultPageSize = 5)\n\n\n\n\n\n\n\n\n\n\nThe table from above has the package names, but it doesn’t include most of the package metadata. Going back to the site, you can see the package name has a link to another page that contains all that data.\n\n\n\n\n\n\nPackages\n\n\n\n\n\n\n\nPackage Details\n\n\n\n\n\nIf we wanted to obtain the URL’s, we need to parse the &lt;table&gt;. Using html_children() again we can see that &lt;table&gt; contains &lt;tr&gt; tags, which is HTML table rows.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"table\") |&gt; \n  html_children()\n\n\n{xml_nodeset (19903)}\n [1] &lt;tr&gt;\\n&lt;th&gt; Date &lt;/th&gt; &lt;th&gt; Package &lt;/th&gt; &lt;th&gt; Title &lt;/th&gt; &lt;/tr&gt;\\n\n [2] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/DrugSim2DR/ ...\n [3] &lt;tr&gt;\\n&lt;td&gt; 2023-08-10 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/gmapsdistan ...\n [4] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/actxps/inde ...\n [5] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/AgroR/index ...\n [6] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/aplot/index ...\n [7] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/av/index.ht ...\n [8] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/basemodels/ ...\n [9] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bayesPop/in ...\n[10] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/Bayesrel/in ...\n[11] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/beanz/index ...\n[12] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bookdown/in ...\n[13] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/bruceR/inde ...\n[14] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/canvasXpres ...\n[15] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CARBayes/in ...\n[16] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/clinDR/inde ...\n[17] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/cmm/index.h ...\n[18] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/complexlm/i ...\n[19] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/CPC/index.h ...\n[20] &lt;tr&gt;\\n&lt;td&gt; 2023-08-09 &lt;/td&gt; &lt;td&gt; &lt;a href=\"../../web/packages/dfoliatR/in ...\n...\n\n\nThen we can go a level lower and see all the elements in the rows. Notice, we use the html_elements() (plural) function instead of html_element(). That’s because each row has multiple elements and html_element() will only parse the first element. We can a &lt;td&gt; tag, which is an HTML data cell, and an &lt;a&gt; tag, which is a hyperlink. The &lt;a contains href=\"../../web/packages/.... An href is an HTML attribute for creating hyperlinks on a web page.\n\n\nCode\ncran_packages_html |&gt; \n  html_element(\"table\") |&gt; \n  html_elements(\"tr\") |&gt; \n  html_children()\n\n\n{xml_nodeset (59709)}\n [1] &lt;th&gt; Date &lt;/th&gt;\n [2] &lt;th&gt; Package &lt;/th&gt;\n [3] &lt;th&gt; Title &lt;/th&gt;\n [4] &lt;td&gt; 2023-08-10 &lt;/td&gt;\n [5] &lt;td&gt; &lt;a href=\"../../web/packages/DrugSim2DR/index.html\"&gt;&lt;span class=\"CRA ...\n [6] &lt;td&gt; Predict Drug Functional Similarity to Drug Repurposing &lt;/td&gt;\n [7] &lt;td&gt; 2023-08-10 &lt;/td&gt;\n [8] &lt;td&gt; &lt;a href=\"../../web/packages/gmapsdistance/index.html\"&gt;&lt;span class=\" ...\n [9] &lt;td&gt; Distance and Travel Time Between Two Points from Google Maps &lt;/td&gt;\n[10] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n[11] &lt;td&gt; &lt;a href=\"../../web/packages/actxps/index.html\"&gt;&lt;span class=\"CRAN\"&gt;a ...\n[12] &lt;td&gt; Create Actuarial Experience Studies: Prepare Data, Summarize\\nResul ...\n[13] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n[14] &lt;td&gt; &lt;a href=\"../../web/packages/AgroR/index.html\"&gt;&lt;span class=\"CRAN\"&gt;Ag ...\n[15] &lt;td&gt; Experimental Statistics and Graphics for Agricultural Sciences &lt;/td&gt;\n[16] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n[17] &lt;td&gt; &lt;a href=\"../../web/packages/aplot/index.html\"&gt;&lt;span class=\"CRAN\"&gt;ap ...\n[18] &lt;td&gt; Decorate a 'ggplot' with Associated Information &lt;/td&gt;\n[19] &lt;td&gt; 2023-08-09 &lt;/td&gt;\n[20] &lt;td&gt; &lt;a href=\"../../web/packages/av/index.html\"&gt;&lt;span class=\"CRAN\"&gt;av&lt;/s ...\n...\n\n\nWe can extract attributes using htm_attr() which will parse the text into a character vector.\n\n\nCode\ncran_packages_df$href &lt;- cran_packages_html |&gt; \n  html_elements(\"tr\") |&gt; \n  html_elements(\"td\") |&gt; \n  html_elements(\"a\") |&gt;\n  html_attr(\"href\")\n\n\nIn the code above, we walked down the whole HTML tree, but again we could’ve jumped down the tree like so. Just understand that this can lead to us picking up other hyperlinks that aren’t in our table (there are none of this page).\n\n\nCode\ncran_packages_html |&gt; \n  html_elements(\"a\") |&gt; \n  html_attr(\"href\")\n\n\nHowever, we do have to specify an HTML element before using html_attr() otherwise we’ll just get back NA.\n\n\nCode\ncran_packages_html |&gt; \n  html_attr(\"href\")\n\n\nOne thing that’s worth mentioning is the URL’s we collected are relative paths, which is why they have /../ in the path.\n\n\nCode\ncran_packages_df$href[1:3]\n\n\n[1] \"../../web/packages/DrugSim2DR/index.html\"   \n[2] \"../../web/packages/gmapsdistance/index.html\"\n[3] \"../../web/packages/actxps/index.html\"       \n\n\nThis is a good point to take step way back to the beginning to introduce a few additional concepts, but if you made it this far you’ve learned enough to scrape a lot of websites with no additional tools. The html_ functions in rvest provide the core tools necessary to parse HTML which is what scraping is…"
  },
  {
    "objectID": "posts/2023-08-08-web-scraping/index.html#paging",
    "href": "posts/2023-08-08-web-scraping/index.html#paging",
    "title": "Web Scraping w/ R",
    "section": "Paging",
    "text": "Paging\nUp to this point, we’ve read HTML pages directly using read_html(). That would get pretty cumbersome if we want to read dozens of pages, let alone the 20K CRAN package pages.\nWhen we use read_html() we copy the page to RAM as discussed earlier, but if we want to read hundreds of pages of a site we want to open an active web connection, or a session if you will :smile: Yup, you guessed it, that’s where rvest’s session() function comes into the picture.\n\n\nCode\nsesh_cran_packages &lt;- rvest::session(\"https://cran.r-project.org/web/packages/available_packages_by_date.html\")\n\n\nWe can do everything we did in the prior session using a session object.\n\n\nCode\ncran_packages_html &lt;- sesh_cran_packages |&gt; \n  read_html()\n\ncran_packages_df &lt;- cran_packages_html |&gt; \n  html_table() |&gt; \n  bind_rows()\n\ncran_packages_df$href &lt;- cran_packages_html |&gt; \n  html_elements(\"tr\") |&gt; \n  html_elements(\"td\") |&gt; \n  html_elements(\"a\") |&gt;\n  html_attr(\"href\")\n\nglimpse(cran_packages_df)\n\n\nRows: 19,902\nColumns: 4\n$ Date    &lt;chr&gt; \"2023-08-10\", \"2023-08-10\", \"2023-08-09\", \"2023-08-09\", \"2023-…\n$ Package &lt;chr&gt; \"DrugSim2DR\", \"gmapsdistance\", \"actxps\", \"AgroR\", \"aplot\", \"av…\n$ Title   &lt;chr&gt; \"Predict Drug Functional Similarity to Drug Repurposing\", \"Dis…\n$ href    &lt;chr&gt; \"../../web/packages/DrugSim2DR/index.html\", \"../../web/package…\n\n\nAnd we can use the session_ verbs such as session_jump_to() to go to relative pages on the site.\n\n\nCode\nrvest_details &lt;- sesh_cran_packages |&gt; \n  session_jump_to(url = cran_packages_df$href[cran_packages_df$Package == \"rvest\"]) |&gt;\n  read_html()\n\n\nWe could use session_jump_to() because we scraped all the package page URL’s earlier, but we could also use session_follow_link() to effectively click on the hyperlinks. The code below accomplishes the same thing as the prior block. However, the session_follow_link() function can be challenging to use with i because it only takes a session object and uses an internal function, find_href(), which uses html_elements(\"a\") to find all hyperlinks on the page and follows the ith hyperlink in that vector. It wraps session_jump_to(), which inherently provides you with significantly more control.\n\n\nCode\npage_title &lt;- sesh_cran_packages |&gt; \n  session_follow_link(i = which(cran_packages_df$Package == \"rvest\")) |&gt; \n  read_html() |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"h2\") |&gt; \n  html_text()\nprint(page_title)\n\n\n[1] \"rvest: Easily Harvest (Scrape) Web Pages\"\n\n\nNow that we have the detail page for rvest, we can structure parse and structure the package metadata using the same set of tools we’ve used along the way and a few stringr and janitor functions to clean text.\n\n\nCode\nlibrary(htmltools)\npackage &lt;- rvest_details |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h2\") |&gt; \n  html_text() |&gt; \n  str_split_i(pattern = \":\", i = 1)\n\ndescription &lt;- rvest_details |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h2\") |&gt; \n  html_text() |&gt; \n  str_split_i(pattern = \":\", i = 2) |&gt; \n  str_squish()\n\npackage_tables &lt;- rvest_details |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_elements(\"table\")\n\npackage_details &lt;- map(package_tables, function(x){\n  pack_table &lt;- x |&gt; \n    html_table()  \n}) |&gt;\n  bind_rows() |&gt; \n  rename(\"key\" = \"X1\", \"value\" = \"X2\") |&gt; \n  mutate(value = str_trunc(value, width = 300))\n\nrvest_table &lt;- reactable(package_details, filterable = T, pagination = FALSE)\n\ndiv(class = \"reactable-tbl-view\",\n    div(class = \"div-subtitle\",\n        div(class = \"div-title\", package),\n        description\n    ),\n    rvest_table\n)\n\n\n\n\n\nrvest\nEasily Harvest (Scrape) Web Pages\n\n\n\n\n\n\nThere’s very little additional work to do in order to scrape all package packages and bind them to our initial data frame. I’m not going to run the code for all packages because there’s a faster way to get this data, but I run it here for a subset to demonstrate.\n\n\nCode\nlibrary(janitor) \nsubset_cran_packages_df &lt;- cran_packages_df[1:15,]\npackage_details_df &lt;- map(\n  subset_cran_packages_df$href, \n  function(url){\n    package_page &lt;- sesh_cran_packages |&gt;\n      session_jump_to(url = url) |&gt;\n      read_html()\n    package_description &lt;- package_page |&gt; \n      html_element(\"body\") |&gt; \n      html_element(\"div\") |&gt; \n      html_element(\"h2\") |&gt; \n      html_text() |&gt; \n      str_split_i(pattern = \":\", i = 2) |&gt; \n      str_squish()\n    package_page_tables &lt;- package_page |&gt;\n      html_element(\"body\") |&gt;\n      html_element(\"div\") |&gt;\n      html_elements(\"table\")\n    package_details &lt;- map(package_page_tables, function(ppt){\n      ppt |&gt;\n        html_table() |&gt;\n        pivot_wider(names_from = \"X1\", values_from = \"X2\")\n    }) |&gt;\n      bind_cols() |&gt;\n      mutate(across(.cols = everything(), .fns = ~stringr::str_trunc(.x, width = 225))) |&gt; \n      mutate(description = package_description, .before = 1)\n    return(package_details)\n}) |&gt; \n  bind_rows() |&gt; \n  clean_names(\"snake\")\n\npackages_reactable &lt;- bind_cols(subset_cran_packages_df, package_details_df) |&gt; \n  reactable(filterable = TRUE, pagination = T, paginationType = \"numbers\", defaultPageSize = 5)\n\ndiv(class = \"reactable-tbl-view\",\n    packages_reactable\n)\n\n\n\n\n\n\n\n\n\n\nCRAN Packages File\nAbove, we were able to scrape the site so we could gather the data for all pacakges pretty easily. That said, thanks to a helpful post from Jeroen Ooms, we can download all the package data from those pages (and then some) with the below code. Much simpler than all that scraping, but the data frame is 67 columns so I’m going to hold of on printing the entire thing. On that note, it’s always worth a little research to see if the site offers their data or an API before doing any scraping.\n\n\nCode\nmytemp &lt;- tempfile()\ndownload.file(\"http://cran.r-project.org/web/packages/packages.rds\", mytemp)\ncran_packages_df &lt;- as.data.frame(readRDS(mytemp), row.names=NA) |&gt; \n  clean_names(\"snake\")"
  },
  {
    "objectID": "posts/2023-08-08-web-scraping/index.html#etiquitte",
    "href": "posts/2023-08-08-web-scraping/index.html#etiquitte",
    "title": "Web Scraping w/ R",
    "section": "Etiquitte",
    "text": "Etiquitte\nNow that we’ve learned how to scrape, it’s time to discuss etiquette. The web has some rules regarding how you can programmatically use web sites. Sites publish their rules in their robots.txt file. Most sites on the web have a robots.txt file and it lives directly in the site’s root directory. As an example, here’s Netflix’s. These files tell us what pages the site administrator does not want us accessing and using programmatically. CRAN’s robot.txt asks that we not scrape the DESCRIPTION page of each package, but they don’t mention the index.html pages we scraped in our example so we didn’t violate their rules.\n\n\n\nThe polite library implements a framework to help users follow site rules and use best practices when scraping with a set of verbs. Beyond making it painless to follow site rules, polite also wraps in a number of features that may save you a lot of time in the long-run including caching. The first verb to learn in polite is bow(). We can us bow in place of rvest’s session() function. The bow() function provides some nice features, but it’s main purpose is to create a session and introduce us to the host before we make a request.\n\n\nCode\nlibrary(polite)\nsesh_cran_packages &lt;- bow(\n  url = \"https://cran.r-project.org/web/packages/available_packages_by_date.html\",\n  user_agent = \"www.mark-druffel.com (mark.druffel@gmail.com)\",\n  delay = 1,\n  force = F,\n  verbose = T\n)\n\n\nAfter bowing, we can use scrape() in place of read_html(). The scrape() function reads in the HTML document in the same way that read_html() does, but also directly provides some useful parameters including query. The query parameter provides an easy way to add URL parameters for filtering certain pages.\n\n\nCode\ncran_packages_df &lt;- sesh_cran_packages |&gt; \n  scrape(accept = \"text/html\") |&gt; \n  html_table() |&gt; \n  bind_rows() \n\ncran_packages_df$href &lt;- sesh_cran_packages |&gt; \n  scrape(accept = \"text/html\") |&gt; \n  html_elements(\"tr\") |&gt; \n  html_elements(\"td\") |&gt; \n  html_elements(\"a\") |&gt;\n  html_attr(\"href\")\n\n\nWe can also use nod() to ask the host if we can modify the session path before scraping the URL paths and it will tell us if we’re violating the site rules.\n\n\nCode\nsesh_cran_packages |&gt; \n  nod(path= cran_packages_df$href[1])\n\n\n&lt;polite session&gt; https://cran.r-project.org/../../web/packages/DrugSim2DR/index.html\n    User-agent: www.mark-druffel.com (mark.druffel@gmail.com)\n    robots.txt: 19919 rules are defined for 1 bots\n   Crawl delay: 1 sec\n  The path is scrapable for this user-agent\n\n\nCode\nsesh_cran_packages |&gt; \n  nod(path= cran_packages_df$href[1]) |&gt; \n  scrape(accept = \"text/html\") |&gt; \n  html_element(\"body\") |&gt; \n  html_element(\"div\") |&gt; \n  html_element(\"h2\") |&gt; \n  html_text() |&gt; \n  str_split_i(pattern = \":\", i = 1)\n\n\n[1] \"DrugSim2DR\"\n\n\n\n\n\nIf we are violating the rules, polite tell us when we nod and will stop us from going further.\n\n\nCode\nsesh_cran_packages |&gt; \n  nod(path= \"/web/packages/rvest/DESCRIPTION\")\n\n\n&lt;polite session&gt; https://cran.r-project.org/web/packages/rvest/DESCRIPTION\n    User-agent: www.mark-druffel.com (mark.druffel@gmail.com)\n    robots.txt: 19919 rules are defined for 1 bots\n   Crawl delay: 1 sec\n  The path is not scrapable for this user-agent\n\n\nCode\nsesh_cran_packages |&gt; \n  nod(path= \"/web/packages/rvest/DESCRIPTION\") |&gt; \n  scrape(accept = \"text/html\") \n\n\nWarning: No scraping allowed here!\n\n\nNULL\n\n\nFinally, polite provides a the rip() function to download files politely.\n\n\nCode\nmytemp &lt;- cran_packages_df &lt;- sesh_cran_packages |&gt; \n  nod(\"web/packages/packages.rds\") |&gt; \n  rip() \ncran_packages_df &lt;- as.data.frame(readRDS(mytemp), row.names=NA)  |&gt;  \n  clean_names(\"snake\")"
  },
  {
    "objectID": "posts/2023-08-08-web-scraping/index.html#dynamic-site",
    "href": "posts/2023-08-08-web-scraping/index.html#dynamic-site",
    "title": "Web Scraping w/ R",
    "section": "Dynamic Site",
    "text": "Dynamic Site\nWe’ve learned how to scrape, to page through a website, and to do all this with proper decorum, but we learned all of these skills on a completely static site. We didn’t have to interact with any buttons, sign in pages, on page filters, or hit any servers once we were on the page to see the data we wanted to scrape. These on-screen actions can be a bit more complicated and require the use of additional software, which I’ll demonstrate.\n\nHiQ Labs v. LinkedIn\nAwhile back, I was job searching and built a scraper that helped me search LinkedIn for jobs that met my criteria. LinkedIn’s job search filters just don’t do enough. Plus, LinkedIn has so many duplicated posts and all the sponsored posts are always at the top. Job searching basically becomes a full-time job because none of the tools are built for the users’ benefit, they’re built for the business model. So yeah, I didn’t want to endlessly scroll through the dystopian hellscape that is the LinkedIn experience to try to find jobs… so that’s why I built a scraper! At that time, screen scraping was frowned upon, but Linkedin had unsuccessfully sued HiQ Labs for scraping and lost so it seemed like the worst case scenario was LinkedIn would close my account. That would suck, but it wouldn’t be that big of a deal. As I’m writing this post, I’m realizing they were recently able to get that ruling reversed and win in court. Many of the articles I’m seeing from the business journalists mention “LinkedIn winning in a suit to protect user data and privacy…” Thank goodness Microsoft is protecting our data from nefarious start-ups so that only they can benefit from it… 🤢\n\n\n\nAnyhow, all this to say, I wrote this scraper before the suit and I’m not suggesting it be used by anyone. I’m only publishing this work to demonstrate how to use these tools, I’d encourage you use the polite framework in order to avoid any legal issues.\nFor fun, I tried to rerun my code using polite and build on our prior examples and bow() responded with….. 🤣\n\n\nCode\nsesh_li &lt;- bow(\n  url = \"https://www.linkedin.com/\",\n  user_agent = \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36\",\n  delay = 1,\n  force = F,\n  verbose = T\n)\n\n\nWarning: Psst!...It's not a good idea to scrape here!\n\n\n\n\n\n\n\nHTML Forms\nYou may have noticed rvest’s _form functions. Pages can have forms, the most common of which is a sign in. We cam use these to fill out forms and hit submit without using a full-fledged web driver like RSelenium. LinkedIn has a few pages that can be used to sign-in, but I worked from this one specifically because of it’s simplicity. You can inspect it with your web developer tools to easily see the structure - for larger sites like this I find it’s much easier to start there and then go to RStudio to use the tools we have up to this point.\n\n\n\nWhen I tried to use the rvest tools, I had a tough time getting them to work for this page. I’m not entirely sure, but it looks like LinkedIn’s submit button doesn’t have a name in the form when I scrape it. That said, in my browser tools I can see the name show up as Sign In. I tried using Sign In in the submit parameter in html_form_set(), but it didn’t work either. Not sure what I’m doing wrong, but that’s ok because we’ll need RSelenium if we go any further so we can just jump into it.\n\n\nCode\nlibrary(glue)\nli &lt;- session(\"https://www.linkedin.com/login?fromSignIn\")\nlogin_forms &lt;- li |&gt; \n  read_html() |&gt; \n  html_form() \nli &lt;- login_forms[[2]] |&gt; \n  html_form_set(session_key = Sys.getenv(\"LINKEDIN_UID\"), session_password = Sys.getenv(\"LINKEDIN_PWD\")) |&gt; \n  html_form_submit() \n\nglue(\"I requested `{li$request$url}` and was redirected to `{li$url}\") \n\n\n\n\nRSelenium\nAs I mentioned in the intro, RSelenium allows you to drive web browsers from R. RSelenium is an R package that provides bindings to Selenium, which is what actually drives the web browser. To use RSelenium, we have a couple options. Option one, run a server either on your machine or in docker and connect to it. Option two, run Selenium locally. For posterity, I’ll cover how to setup and connect to a docker instance running Selenium, but for the demonstration I’ll just us Selenium locally.\n\nDocker\nIf you’re not familiar with docker, I recommend Colin Faye’s An Introduction to docker for R Users. To get docker setup for Selenium, I heavily reference RSelenium’s docker documentation and the SeleniumHQ docker documentation. If you don’t have docker on your machine, you can install it here. You can also install docker desktop here if you like. The docker installation is required, but docker desktop is just helpful. Kind of like R is required and RStudio is just helpful.\n\nDocker Pull\nOnce you’ve installed docker, you need to pull an image with Selenium (or create one). To start, I followed the RSelenium docs. RSelenium refers to this image, which you can pull the image (i.e. get a copy of it to run on your machine) in terminal with the below code.\n\n\nCode\ndocker pull selenium/standalone-firefox:2.53.0\n\n\nThat standalone image works just fine for scraping, but if you want to be able to observe your code working, you’ll need to download the debug version.\n\n\nCode\ndocker pull selenium/standalone-firefox-debug:2.53.0\n\n\nRegardless, firefox:2.53.0 is pretty out of date. It uses Firefox version 47.0.1, which was released in 2016. The newest version on Selenium’s docker docs at the time of writing this is firefox:4.9.0-20230421, which uses Firefox version 112.0.1 (released in 2023). This later version also has debugging capbailities built in so there is no need to pull a -debug version. We can pull whatever is the latest version (right now 4.9.0-20230421) by removing the version or explicitly using latest. However, I couldn’t get version 4.9.0-20230421 and see in this issue that other users bumped into the same roadblock.\n\n\nCode\ndocker pull selenium/standalone-firefox:latest\n\n\nIn the Github issue comments, the fvalenduc said he tested the versions and the newest working version with RSelenium was 3.141.59.\n\n\nCode\ndocker pull selenium/standalone-firefox-debug:3.141.59\n\n\n\n\nStart Docker\nYou can start the docker container with the code below. The first port (i.e. -p 4445) is the Selenium server, the second is used to connect to a web server to observe the Selenium.\n\n\nCode\ndocker run -d -p 4445:4445 -p 5900:5900 selenium/standalone-firefox-debug:3.141.59\ndocker run -d -p 4444:4444 -p 7900:7900 --shm-size=\"2g\" selenium/standalone-firefox:latest\n\n\nWe can also start our Selenium server from R using the system command. This is helpful if you want to create a script or package your code. If you’re using Linux and docker is owned by root, you can either add your user to the docker group or pipe your sudo password into your command (shown in second system() call below).\n\n\nCode\nsystem(\n  command = \"docker run -d -p 4445:4445 -p 5900:5900 selenium/standalone-firefox-debug:3.141.59\",\n  intern = T\n)\nsystem(\n  command = glue::glue(\"echo {Sys.getenv('SUDO_PWD')} | sudo -S docker run -d -p 4445:4445 -p 5900:5900 selenium/standalone-firefox-debug:3.141.59\"),\n  intern = T\n)\n\n\n\n\nConnect to Docker\nWe can connect to the docker container using the remoteDriver() class. As I mentioned before, the latest version would not work for me… It would hang if I ran latest$open(). Once you open a connection, you can drive the browser from there. I’ll cover driving the web driver section.\n\n\nCode\nlibrary(RSelenium)\n# Connect to 3.141.59\nrdriver &lt;- remoteDriver(\n  remoteServerAddr = \"0.0.0.0\",\n  port = 4445L,\n  browserName = \"firefox\",\n  version = \"47.0.1\",\n  platform = \"LINUX\",\n  javascript = T\n)\n# Connect to latest\nlatest &lt;- remoteDriver(\n  remoteServerAddr = \"172.17.0.2\",\n  port = 4444L,\n  browserName = \"firefox\",\n  version = \"112.0\",\n  platform = \"LINUX\",\n  javascript = T\n)\n# Open the connection and navigate to a website\nrdriver$open()\nrdriver$navigate(\"www.google.com\")\n\n\nIf you don’t know the remoteServerAddr, there are several ways to get that info. First, you can run docker ps. The output can look a little confusing, but you should be able to figure out what the server’s IP address is.SDM\n\n\nCode\ndocker ps\n\n\nYou can also run docker ps from R. I couldn’t find a library that simplified interacting with Docker from R, but I’m sure one exists. I’ve written a few wrappers around ps, but never tried to do anything\n\n\nCode\nlibrary(tidyverse)\nimages &lt;- system(glue::glue(\"echo {Sys.getenv('SUDO_PWD')} | sudo -S docker ps\"), ignore.stdout = F, intern = T)\nips &lt;- as_tibble(str_locate(images, pattern = \"[0-9]??\\\\.[0-9]??\\\\.[0-9]??\\\\.[0-9]??:\"))\nips &lt;- pmap_dfr(list(i = images[2:5], s = ips[2:5, ]$start, e = ips[2:5, ]$end), function(i, s, e){\n  ip &lt;- str_sub(i, start = s, end = e-1)\n  tibble(ip = ip)\n})\nnames &lt;- as_tibble(str_locate(images, pattern = \"selenium.*firefox:[a-z0-9]+(?:[._-][a-z0-9]+)*|selenium.*firefox-debug:[a-z0-9]+(?:[._-][a-z0-9]+)*\"))\nnames &lt;- pmap_dfr(list(i = images[2:5], s = names[2:5, ]$start, e = names[2:5, ]$end), function(i, s, e){\n  name &lt;- str_sub(i, start = s, end = e)\n  tibble(name = name)\n}) \ndocker &lt;- bind_cols(names, ips) |&gt; \n  bind_cols(images[2:5]) |&gt; \n  rename(\"ps\" = 3L)\ndocker |&gt; \n  filter(str_detect(name, pattern = \"latest|firefox:4.\"))\n\n\n# A tibble: 0 × 3\n# ℹ 3 variables: name &lt;chr&gt;, ip &lt;chr&gt;, ps &lt;chr&gt;\n\n\nAnother option is to look on docker hub to see the environment variables set in the docker image. The NODE_HOST variable is the remote server address you should use. Alternatively, you can log into the web server, right click on the desktop, launch bash, and run hostname -I.\n\n\n\nIf you’re using a newer version of Selenium, once you have the IP, you can connect to Selenium Grid via endpoints using httr to get other info. This won’t work for the older versions that don’t use Selenium Grid.\n\n\nCode\nlibrary(httr)\nstatus &lt;- GET('http://localhost:4444/status')\ncontent(status)\n\n\n\n\nConnect to Web Server\nYou can log into the web server to get info from the machine, install updates, etc. if you want to, but the most useful aspect of doing so imho is the ability to watch your scraping program work in the browser while it’s running. You can make sure it’s doing what you intended so it doesn’t fly off the rails. To log into the web server, you’ll need a remote desktop app. The latest Selenium version comes with noVNC, but you’ll need to install your own when using the older -debug versions. I used Vinagre. You will be prompted for a password, I couldn’t figure out where that’s set in docker but found in the documentation that it is secret.\n \n\nWhen using the latest versuib you can simply access the web sever through your browser by going to http://localhost:7900/?autoconnect=1&resize=scale&password=secret.\n\n\n\nLocal\nRSelenium allows us to install binaries and run Selenium locally in our browser of chance through the wdman package with the rsDriver function which creates a remoteDriver locally (i.e. you can use it the same way you would use remoteDriver() once the connection is established). I had some issues with firefox on Ubuntu 22, but I was able to get Chrome working by updating Chrome and turning off check.\n\nSign-in\nFirst, we need to sign into LinkedIn so we can view all the pages. The findElement() method works similarly to html_element() from rvest, but we can access other methods to interact with the HTML element. Below, we used sendKeysToElement() in order to enter our credentials, the clickElement() to submit, and the getCurrentUrl() method to confirm the sign-in worked properly.\n\n\nCode\ndriver$client$navigate(\"https://www.linkedin.com/login?fromSignIn\")\nSys.sleep(3)\ndriver$client$screenshot(file = \"images/sign_in_page.png\")\ndriver$client$findElement(\"name\", \"session_key\")$sendKeysToElement(list(Sys.getenv(\"LINKEDIN_UID\")))\ndriver$client$findElement(\"name\", \"session_password\")$sendKeysToElement(list(Sys.getenv(\"LINKEDIN_PWD\")))\ndriver$client$findElement(\"tag name\", \"BUTTON\")$clickElement()\ndriver$client$maxWindowSize()\n\n\n  \n\n\nJob Search\nOnce we’ve signed in, we can interact with the site under our username. We could fill in the search forms, but I want to do a few different job searches and the parameters all go in the URL as query parameters so I’m just going to build a URL using glue and URLencode.\n\n\n\n\n\n\nMinimize Messaging\n\n\n\n\n\nYou may notice the use of minimize_messaging() in the job search code. I created a function to minimize personal messaging just so it wouldn’t be open for screen shots. This wouldn’t have been nessary if I wasn’t publishing automatically generated screenshots for this post…\n\n\nCode\nminimize_messaging &lt;- function(driver){\n  html &lt;- xml2::read_html(driver$client$getPageSource()[[1]], options = \"HUGE\")\n  df_messaging &lt;- tibble(\n    id = html |&gt; html_element(\"aside\") |&gt; html_elements(\"button\") |&gt; html_attr(\"id\"),\n    icon_type = html |&gt; html_element(\"aside\") |&gt; html_elements(\"button\") |&gt; html_element(\"li-icon\") |&gt; html_attr(\"type\")\n  ) |&gt; \n    filter(icon_type == \"chevron-down\")\n  if(nrow(df_messaging) == 1){\n    id &lt;- df_messaging$id\n    driver$client$findElement(\"id\", id)$clickElement()\n  }\n}\n\n\n\n\n\n\n\nCode\nremote_and_hybrid &lt;- \"f_WT=2,3\"\nkeywords &lt;- \"keywords=data analyst | data scientist\"\nlocation &lt;- \"Portland, Oregon Metropolitan Area\"\ngeo &lt;- \"geoId=90000079\"\njobs_url &lt;- URLencode(glue::glue(\"https://www.linkedin.com/jobs/search/?{remote_and_hybrid}&{geo}&{keywords}&{location}&refresh=T\"))\ndriver$client$navigate(jobs_url)\nSys.sleep(3)\nminimize_messaging(driver)\nSys.sleep(1)\ndriver$client$screenshot(file = \"images/job_search.png\")\njobs_html &lt;- xml2::read_html(driver$client$getPageSource()[[1]], options = \"HUGE\")\n\n\n  \n\n\nJob Listings\nThe search returns a bunch of jobs based on our search parameters in a number of pages. The list of jobs just has the role title, company name, and a few other basic job facts. The details of each job are in the detailed pages that we have to click into. We can extract the href attribute for each job listing to get a direct URL or the button id attribute so we can click the button through Selenium to open the job details pane. We can extract both of these things using the URL page links using html_attr. Remember, an &lt;a&gt; tag is used for hyperlinks, the href stores the URL and the id uniquely identifies the tag. Since each hyperlink will have one href and one id, I just extracted the data into a tibble to make it easier to filter to only the buttons we want. That said, if you look at the list of job links you may notice there are less jobs than are actually in the web page… the reason for that is you have to scroll down the page to load all the jobs.\n\n\nCode\ndf_jobs &lt;- tibble(\n  href = jobs_html |&gt; html_elements(\"body\") |&gt; html_elements(\"a\") |&gt; html_attr(\"href\"),\n  id = jobs_html |&gt; html_elements(\"body\") |&gt; html_elements(\"a\") |&gt; html_attr(\"id\")\n) |&gt; \n  filter(str_detect(href, pattern = \"/jobs/view/\")) |&gt; \n  mutate(href = paste0(\"https://www.linkedin.com\", href))\nglue::glue(\"df_jobs has {nrow(df_jobs)} hrefs\")\n\n\ndf_jobs has 9 hrefs\n\n\nTo scroll down, we need to find an anchor point to tell Selenium to scroll to. I chose to use the paging buttons at the bottom of the list because I’ll need those later for paging through the list.\n \n\nAfter you scroll down and retry scraping the jobs you’ll still be missing jobs from the list 😱 The site cleverly only loads as you scroll down the page. I tried a few more elegant ways to load the whole list including the down arrow (which is disabled in the jobs list view) and using a custom JavaScript script to send a mouse wheel event. Unfortunately, I couldn’t get those approaches to work and I was stuck with brute force…\n\n\nCode\njobs_html &lt;- xml2::read_html(driver$client$getPageSource()[[1]], options = \"HUGE\")\ndf_jobs &lt;- tibble(\n  href = jobs_html |&gt; html_elements(\"body\") |&gt; html_elements(\"a\") |&gt; html_attr(\"href\"),\n  id = jobs_html |&gt; html_elements(\"body\") |&gt; html_elements(\"a\") |&gt; html_attr(\"id\")\n) |&gt; \n  filter(str_detect(href, pattern = \"/jobs/view/\")) |&gt; \n  mutate(href = paste0(\"https://www.linkedin.com\", href))\nglue::glue(\"df_jobs has {nrow(df_jobs)} hrefs\")\n\n\ndf_jobs has 14 hrefs\n\n\nMy brute force approach was basically to repeatedly use the getElementLocationInView() method on a random walk over the job HTML id’s. This approach causes Selenium to scroll the browser to get the job into the view, and by randomly ordering the jobs the scroll goes in both directions getting jobs to load. I wrote this process in a recursive function then scrapes the jobs until the random walk doesn’t get any additional jobs to load.\n\n\nCode\nscrape_html &lt;- function(driver){\n  html &lt;- xml2::read_html(driver$client$getPageSource()[[1]], options = \"HUGE\")\n  return(html)\n}\nscrape_pages_df &lt;- function(html){\n  df_pages &lt;- tibble(\n    pagination_btn = html |&gt; html_element(\"body\") |&gt; html_elements(\"li\") |&gt; html_attr(\"data-test-pagination-page-btn\"),\n    id = html |&gt; html_elements(\"body\") |&gt; html_elements(\"li\") |&gt; html_attr(\"id\")) |&gt; \n    mutate(pagination_btn = as.numeric(pagination_btn)) |&gt; \n    mutate(pagination_btn_ellipsis = lag(pagination_btn) +1) |&gt; \n    mutate(pagination_btn = coalesce(pagination_btn, pagination_btn_ellipsis),   .keep = \"unused\") |&gt; \n    filter(str_detect(id, \"ember\") & !is.na(pagination_btn))\n  return(df_pages)\n}\nrandwalk_jobs &lt;- function(driver, ids){\n  ids &lt;- ids |&gt; \n    sort() |&gt; \n    sample()\n  purrr::walk(ids, function(id){\n    job_element &lt;- driver$client$findElement(\"id\", id)\n    job_element$getElementLocationInView()\n    naptime::naptime(.25)\n  })\n}\nscrape_jobs_df &lt;- function(driver, n_jobs = 0){\n  if(n_jobs == 0){\n    html &lt;- scrape_html(driver)\n    df_pages &lt;- scrape_pages_df(html)\n    first_page_button_webElem &lt;- driver$client$findElement(\"id\", df_pages$id[1])\n    first_page_button_webElem$getElementLocationInView()\n    naptime::naptime(1)\n  }\n  html &lt;- scrape_html(driver)\n  df_jobs &lt;- tibble(\n    url = html |&gt; html_elements(\"body\") |&gt;  html_elements(\"a\") |&gt; html_attr(\"href\"),\n    class = html |&gt; html_elements(\"body\") |&gt;  html_elements(\"a\") |&gt; html_attr(\"class\"),\n    id = html |&gt; html_elements(\"body\") |&gt;  html_elements(\"a\") |&gt; html_attr(\"id\"),\n    text = html |&gt; html_elements(\"body\") |&gt;  html_elements(\"a\") |&gt; html_text()\n  ) |&gt; \n    filter(str_detect(url, pattern = \"/jobs/view/\")) |&gt; \n    filter(str_detect(class, patter = \"job-card-list__title\"))\n  if(n_jobs &lt; nrow(df_jobs)){\n    randwalk_jobs(driver, df_jobs$id)\n    df_jobs &lt;- scrape_jobs_df(driver, n_jobs = nrow(df_jobs))\n  } else{\n    return(df_jobs)\n  }\n}\ndf_jobs &lt;- scrape_jobs_df(driver)\nglimpse(df_jobs)\n\n\nRows: 25\nColumns: 4\n$ url   &lt;chr&gt; \"/jobs/view/3643896946/?eBP=CwEAAAGJ3hqxnFFynjFRp74ikNndcmugP453…\n$ class &lt;chr&gt; \"disabled ember-view job-card-container__link job-card-list__tit…\n$ id    &lt;chr&gt; \"ember202\", \"ember214\", \"ember225\", \"ember236\", \"ember248\", \"emb…\n$ text  &lt;chr&gt; \"\\n                  Data Analyst - Machine Learning\\n          …\n\n\n\n\nJob Details\nNow that we have all the jobs loaded with href’s and id’s, we can open the job details to scrape out the data we want. We can either directly navigate to the job URL’s or open the job details pane. The former option is much easier, but it will cause another page load (slower) and it’s a user behavior that would not resemble a user on the site (i.e. it would look like a scraper). We’ve already covered all the methods required to do use either method (direct navigation or button clicking) in Job Search and Sign-in, respectively. I used the latter method to open the jobs using the id’s and the clickElement method.\n\n\nCode\ndriver$client$findElement(\"id\", df_jobs$id[1])$clickElement()\ndriver$client$screenshot(file = \"images/job_1_view.png\")\n\n\n\nThe job details pane was a total pain 😅 It was broken into a sections and I scraped a few of them - a top card which has some details about the company and role, the job description (text), salary data, and skills data. The skills data is inside a modal view. I’m not going to walk through all this code line by line because we’ve already walked through most these functions and methods, but I put everything in functions with long names to try to make it as easy to follow as possible. The main function, scrape_job_details() can pull all the data from the aforementioned sections (top card, description, salary, skills) and puts it all in a data frame. The one new idea that shows up here that didn’t in prior sections in this post is error handling. Error handling deserves its own separate post so I’m not going to dive into it here, but if you write a scraper that does interactive things (clicking, log-ins, paging) you will need error handling. Sometimes your code will be slightly faster than the browser and it causes random glitches. Also, sometimes if you scrape enough data some parts of the site might not be structured the same way due to edge cases, errors, etc. There are several libraries and base functions available for error handling, but I used purrr because I thought possibly() and insistently would be the easiest / best for what I was doing. If it wasn’t apparent up to now, you can see screen scraping can be a lot of work. Further, if the page changes some of the classes and structures may change and break our code 😅\n\n\nCode\n# pane is what we're scraping, but I wrote my functions to work for page if you click into the job's url (whole page job listing)\nscrape_job_details_df &lt;- function(driver, id = NULL, view = c(\"pane\", \"page\")){\n  html &lt;- scrape_html(driver)\n  html_job_details &lt;- extract_job_details_html(html, view)\n  html_top_card &lt;- extract_top_card_html(html_job_details, view)\n  df_job_details &lt;- scrape_top_card_df(html_top_card)\n  html_job_desc_card &lt;- extract_job_desc_card_html(html_job_details)\n  df_job_desc &lt;- scrape_job_desc_df(html_job_desc_card)\n  df_job_details &lt;- dplyr::bind_cols(df_job_details, df_job_desc)\n  html_salary_card &lt;- extract_salary_card_html(html_job_details)\n  df_job_details &lt;- dplyr::bind_cols(\n    df_job_details,\n    scrape_salary_df(html_salary_card))\n  open_skills_button_id &lt;- get_skills_open_button_id(html_job_details)\n  open_skills_click_result &lt;- click_button(driver, \"id\", open_skills_button_id, nap_length = \"long\")\n  if(open_skills_click_result == \"Success\"){\n    html &lt;- scrape_html(driver)\n    html_skills_modal &lt;- extract_skills_modal_html(html)\n    close_skills_button_id &lt;- get_skills_close_button_id(html_skills_modal)\n    close_skills_click_result &lt;- click_button(driver, \"id\", close_skills_button_id, nap_length = \"short\")\n    if(close_skills_click_result != \"Success\"){driver$client$goBack()}\n    skills &lt;- scrape_skills_chr(html_skills_modal)\n    df_job_details$skills &lt;- list(skills)\n  } else {\n    skills &lt;- NA_character_\n    df_job_details$skills &lt;- list(skills)\n  }\n  df_job_details$id &lt;- id\n  return(df_job_details)\n}\nextract_job_details_html &lt;- function(html, view = c(\"pane\", \"page\")){\n  view &lt;- match.arg(view, choices = c(\"pane\", \"page\"), several.ok = F)\n  if(view == \"pane\"){\n    class_pattern &lt;- \"scaffold-layout__detail\"\n  } else if(view == \"page\"){\n    class_pattern &lt;- \"job-view-layout jobs-details\"\n  } else{\n    class_pattern &lt;- \"jobs-details\"\n  }\n  job_details_html_index &lt;- tibble::tibble(class = html |&gt;  rvest::html_elements(\"body\") |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(class, pattern = class_pattern)) |&gt;\n    pull(rn)\n  html_job_details &lt;- html |&gt;  rvest::html_elements(\"body\") |&gt;  rvest::html_elements(\"div\") |&gt; _[job_details_html_index]\n  return(html_job_details)\n}\nextract_top_card_html &lt;- function(html_job_details, view = c(\"pane\", \"page\")){\n  view &lt;- match.arg(view, choices = c(\"pane\", \"page\"), several.ok = F)\n  if(view == \"pane\"){\n    top_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n      dplyr::mutate(rn = dplyr::row_number()) |&gt;\n      filter(class == \"jobs-unified-top-card__content--two-pane\") |&gt;\n      pull(rn)\n    html_top_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[top_card_html_index]\n  } else if(view == \"page\"){\n    top_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n      dplyr::mutate(rn = dplyr::row_number()) |&gt;\n      filter(stringr::str_detect(class, \"jobs-unified-top-card*.*artdeco-card\")) |&gt;\n      pull(rn)\n    html_top_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[top_card_html_index]\n  }\n  return(html_top_card)\n}\nscrape_top_card_df &lt;- function(html_top_card){\n  title &lt;- html_top_card |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_text() |&gt; str_squish() |&gt; _[1]\n  subline &lt;- html_top_card |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_text() |&gt; str_squish() |&gt; _[2]\n  company_name &lt;- str_split_i(subline, pattern = \"·\", i = 1) |&gt; str_squish()\n  location &lt;- str_extract(subline, pattern = \"([A-Z][a-z]+\\\\s?)+,\\\\s[A-Z]{2}\")\n  workplace_type &lt;- if_else(\n    str_detect(str_to_lower(subline), pattern = \"remote\"),\n    \"remote\",\n    if_else(\n      str_detect(str_to_lower(subline), pattern = \"hybrid\"),\n      \"hybrid\",\n      if_else(\n        str_detect(str_to_lower(subline), pattern = \"on-site\"),\n        \"on-site\",\n        \"not provided\")))\n  df_list_items &lt;- tibble::tibble(\n    class = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_attr(\"class\"),\n    text = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_text(),\n    icon_type = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_element(\"li-icon\") |&gt; html_attr(\"type\")\n  )\n  employment_type &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"job\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"Full-time|Part-time|Contract|Volunteer|Temporary|Internship|Other\")\n  job_level &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"job\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"Internship|Entry Level|Associate|Mid-Senior level|Director|Executive\")\n  company_size &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"company\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"[0-9]{1,},?[0-9]*(-|\\\\+)([0-9]{1,},?[0-9]*)?\")\n  company_industry &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"company\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_split_i(pattern = \"·\", i = 2) |&gt;\n    str_squish()\n  df_job_details &lt;- tibble::tibble(\n    job_title = if_length0_NA_character(title),\n    job_level = if_length0_NA_character(job_level),\n    company_name = if_length0_NA_character(company_name),\n    company_industry = if_length0_NA_character(company_industry),\n    company_size = if_length0_NA_character(company_size),\n    location = if_length0_NA_character(location),\n    workplace_type = if_length0_NA_character(workplace_type),\n    employment_type = if_length0_NA_character(employment_type)\n  )\n  return(df_job_details)\n}\nextract_job_desc_card_html &lt;- function(html_job_details){\n  job_desc_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(stringr::str_detect(class, \"jobs-description-content\")) |&gt;\n    pull(rn)\n  html_job_desc_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_elements(\"div\") |&gt;  _[job_desc_card_html_index]\n  return(html_job_desc_card)\n}\nscrape_job_desc_df &lt;- function(html_job_desc_card){\n  job_desc &lt;- html_job_desc_card |&gt; html_text() |&gt; paste(collapse = (\" \")) |&gt;  stringr::str_squish()\n  df_job_desc &lt;- tibble::tibble(job_desc = job_desc)\n  return(df_job_desc)\n}\nextract_salary_card_html &lt;- function(html_job_details){\n  salary_card_html_index &lt;- tibble::tibble(id = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"id\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(id == \"SALARY\") |&gt;\n    pull(rn)\n  html_salary_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[salary_card_html_index]\n  return(html_salary_card)\n}\nscrape_salary_df &lt;- function(html_salary_card){\n  salary_text_index &lt;- html_salary_card |&gt; rvest::html_elements(\"p\") |&gt; rvest::html_text() |&gt; stringr::str_detect(pattern = \"(from job description)|salary\")\n  salary_text &lt;- html_salary_card |&gt; rvest::html_elements(\"p\") |&gt; rvest::html_text() |&gt; _[salary_text_index] |&gt; stringr::str_squish()\n  if(length(salary_text) != 0){\n    salary_interval &lt;- str_extract_all(salary_text, pattern = \"[1-9][0-9]?[0-9]?,?[0-9]?[0-9]?[0-9]?\") |&gt; purrr::map(str_remove, pattern = \",\") |&gt; purrr::map(as.double)\n    df_salary &lt;- tibble(salary_min = min(unlist(salary_interval)), salary_max = max(unlist(salary_interval)))\n  } else {\n    df_salary &lt;- tibble(salary_min = NA_real_, salary_max = NA_real_)\n  }\n  return(df_salary)\n}\nget_skills_open_button_id &lt;- function(html_job_details){\n  skills_box_html_index &lt;- tibble::tibble(\n    class = html_job_details |&gt;   html_elements(\"div\")  |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(str_to_lower(class), pattern = \"job-details-how-you-match-card__container\")) |&gt;\n    pull(rn)\n  html_skills_box &lt;-html_job_details |&gt; html_elements(\"div\") |&gt; _[skills_box_html_index]\n  button_id &lt;- html_skills_box |&gt; html_elements(\"button\") |&gt; html_attr(\"id\")\n  return(button_id)\n}\nget_skills_close_button_id &lt;- function(html_skills_modal){\n  html_x_button_index &lt;- tibble::tibble(\n    aria_label = html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"aria-label\"),\n    id = html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"id\")\n  ) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_to_lower(aria_label) == \"dismiss\") |&gt;\n    pull(rn)\n  x_button_id &lt;- html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"id\") |&gt; _[html_x_button_index]\n}\nextract_skills_modal_html &lt;- function(html){\n  skills_modal_index &lt;- tibble::tibble(\n    role = html |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"role\"),\n    class = html |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")\n    ) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(class, \"job-details-skill-match-modal\") & role == \"dialog\") |&gt;\n    pull(rn)\n  html_skills_modal &lt;- html |&gt;  rvest::html_elements(\"div\") |&gt; _[skills_modal_index]\n  return(html_skills_modal)\n}\nscrape_skills_chr &lt;- function(html_skills_modal){\n  skills &lt;- html_skills_modal |&gt;\n    html_elements(\"li\") |&gt;\n    html_text() |&gt;\n    str_squish() |&gt;\n    str_remove(pattern = \"Add$\") |&gt;\n    str_squish()\n  skills &lt;- skills[skills != \"\"]\n  return(skills)\n}\nif_length0_NA_character &lt;- function(var){\n  if(length(var) == 0){\n    x &lt;- NA_character_\n  } else {\n    x &lt;- var\n  }\n  return(x)\n}\nclick_button &lt;- function(driver, using = c(\"xpath\", \"css selector\", \"id\", \"name\", \"tag name\", \"class name\", \"link text\", \"partial link text\"),  value, nap_length = \"short\"){\n  url &lt;- driver$client$getCurrentUrl()\n  click &lt;- function(driver, using, value, nap_length){\n    driver$client$findElement(using = using, value = value)$getElementLocationInView()\n    driver$client$findElement(using = using, value = value)$clickElement()\n    nap_rnorm(nap_length)\n    message &lt;- \"Success\"\n    return(message)\n  }\n  possibly_click &lt;- possibly(insistently(click, rate = rate_backoff(pause_base = 5, pause_cap = 45, max_times = 3, jitter = T), quiet = FALSE), otherwise = url)\n  click_result &lt;- possibly_click(driver = driver, using = using, value = value, nap_length = nap_length)\n  return(click_result)\n}\nnap_rnorm &lt;- function(length = c(\"short\", \"moderate\", \"long\")){\n  if(length[1] == \"short\"){\n    wait &lt;- abs(rnorm(n = 1, mean = 1.25, sd = .25))\n  } else if(length[1] == \"moderate\"){\n    wait &lt;- abs(rnorm(n = 1, mean = 3, sd = .5))\n  } else if(length[1] == \"long\") {\n    wait &lt;- abs(rnorm(n = 1, mean = 6, sd = 1.5))\n  } else{\n    wait &lt;- length\n  }\n  naptime::naptime(time = wait)\n}\n\ndf_job_details &lt;- scrape_job_details_df(driver)\nglimpse(df_job_details)\n\n\nRows: 1\nColumns: 12\n$ job_title        &lt;chr&gt; \"Data Analyst - Machine Learning\"\n$ job_level        &lt;chr&gt; \"Mid-Senior level\"\n$ company_name     &lt;chr&gt; \"AltaSource Group\"\n$ company_industry &lt;chr&gt; \"IT Services and IT Consulting\"\n$ company_size     &lt;chr&gt; \"51-200\"\n$ location         &lt;chr&gt; NA\n$ workplace_type   &lt;chr&gt; \"remote\"\n$ employment_type  &lt;chr&gt; \"Contract\"\n$ job_desc         &lt;chr&gt; \"About the job Data Analyst - Machine Learning AltaSo…\n$ salary_min       &lt;dbl&gt; NA\n$ salary_max       &lt;dbl&gt; NA\n$ skills           &lt;list&gt; &lt;\"Data Analysis\", \"Data Mining\", \"Data Science\", \"Mac…\n\n\nAll that code scraped details for one job, but we’ll want to do the same for all the jobs in df_jobs. We can create\n\n\nPaging\ndf_jobs only has the jobs on the first page, which means we’ll have to navigate through all the pages of job lists if we want data for all the jobs returned by the search. In order to do that, we first need to figure out how many times we have to page - which we can find at the bottom of the jobs list page with the page numbers. We can use developer tools to find the HTML tags and attributes of those page numbers. The HTML tag is li (list), which is what the page numbers at the bottom are in. We can use a similar approach as before to scrape this list, and again, filter to the items that match what we want. This gives use a list of ID’s we can use to page through. That said, you might notice that we don’t have all the pages. That’s because the site only displays some pages, a …, and then the last page number. I used lag() from dplyr to find the ID that is the ellipsis because it doesn’t have a data-test-pagination-page-btn attribute. That makes it simple to keep that HTML id in my data frame so I can easily click on the ellipsis and continue paging rather than skipping over all the pages inside the ellipsis. I’ll still need to pull the HTML id’s for additional pages inside the ellipsis, but we can do that as we page through…\n\n\nCode\nscrape_html &lt;- function(driver){\n  html &lt;- rvest::read_html(driver$client$getPageSource()[[1]])\n  return(html)\n}\nscrape_pages_df &lt;- function(html){\n  df_pages &lt;- tibble::tibble(\n    pagination_btn = html |&gt; rvest::html_element(\"body\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"data-test-pagination-page-btn\"),\n    id = html |&gt; rvest::html_elements(\"body\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"id\")\n  ) |&gt;\n    dplyr::mutate(pagination_btn = as.numeric(pagination_btn)) |&gt;\n    dplyr::mutate(pagination_btn_ellipsis = lag(pagination_btn) +1) |&gt;\n    dplyr::mutate(pagination_btn = coalesce(pagination_btn, pagination_btn_ellipsis),   .keep = \"unused\") |&gt;\n    dplyr::filter(stringr::str_detect(id, \"ember\") & !is.na(pagination_btn))\n  return(df_pages)\n}\nhtml &lt;- scrape_html(driver)\ndf_pages &lt;- scrape_pages_df(html)\npages &lt;- seq.int(from = 1, to = max(df_pages$pagination_btn), by = 1)\nprint(pages)\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21\n\n\nWe can use the info in df_page and walk() to click the next page button.\n\n\nCode\npurrr::walk(pages[1:3], function(i){\n    df_pages &lt;- scrape_pages_df(scrape_html(driver)) |&gt; dplyr::filter(pagination_btn == i + 1)\n    click_button(driver, \"id\", df_pages$id, nap_length = \"long\")\n    print(glue::glue(\"Selenium is now on page {df_pages$pagination_btn} at {driver$client$getCurrentUrl()}\"))\n})\n\n\nSelenium is now on page 2 at https://www.linkedin.com/jobs/search/?currentJobId=3640240357&f_WT=2%2C3&geoId=90000079&keywords=data%20analyst%20%7C%20data%20scientist&refresh=T&start=25\nSelenium is now on page 3 at https://www.linkedin.com/jobs/search/?currentJobId=3612521369&f_WT=2%2C3&geoId=90000079&keywords=data%20analyst%20%7C%20data%20scientist&refresh=T&start=50\nSelenium is now on page 4 at https://www.linkedin.com/jobs/search/?currentJobId=3674579573&f_WT=2%2C3&geoId=90000079&keywords=data%20analyst%20%7C%20data%20scientist&refresh=T&start=75\n\n\n\n\nFunctionalize\nIf you’re still with me you’re a trooper. I put everything we’ve done so far in Selenium into functions so that I can run our scraper with a few function calls.\n\n\nCode\n# Driver ####\nopen_driver &lt;- function(port = 1443L, browser = \"chrome\", check = F, verbose = F){\n  driver &lt;- rsDriver(port = port, browser = browser, check = check, verbose = F)\n  driver$client$maxWindowSize()\n  return(driver)\n}\nkill_driver &lt;- function(driver){\n  driver$server$process$kill()\n  rm(driver, envir = rlang::global_env())\n}\nnap_rnorm &lt;- function(length = c(\"short\", \"moderate\", \"long\")){\n  if(length[1] == \"short\"){\n    wait &lt;- abs(rnorm(n = 1, mean = 1.25, sd = .25))\n  } else if(length[1] == \"moderate\"){\n    wait &lt;- abs(rnorm(n = 1, mean = 3, sd = .5))\n  } else if(length[1] == \"long\") {\n    wait &lt;- abs(rnorm(n = 1, mean = 6, sd = 1.5))\n  } else{\n    wait &lt;- length\n  }\n  naptime::naptime(time = wait)\n}\n# Interact ####\nsignin &lt;- function(driver, session_key = Sys.getenv(\"LINKEDIN_UID\"), session_password = Sys.getenv(\"LINKEDIN_PWD\")){\n  url &lt;- \"https://www.linkedin.com/login?fromSignIn\"\n  driver$client$navigate(url)\n  nap_rnorm(\"moderate\")\n  driver$client$findElement(\"name\", \"session_key\")$sendKeysToElement(list(session_key))\n  driver$client$findElement(\"name\", \"session_password\")$sendKeysToElement(list(session_password))\n  driver$client$findElement(\"tag name\", \"BUTTON\")$clickElement()\n  nap_rnorm(\"moderate\")\n  message(glue::glue(\"Selenium is now at {driver$client$getCurrentUrl()}\"))\n}\nminimize_messaging &lt;- function(driver){\n  html &lt;- scrape_html(driver)\n  aside_index &lt;- html |&gt; rvest::html_elements(\"aside\") |&gt; rvest::html_attr(\"class\") |&gt; stringr::str_detect(pattern = \"msg-overlay\") |&gt; which()\n  html_aside &lt;- html |&gt; rvest::html_elements(\"aside\") |&gt; _[aside_index]\n  df_messaging &lt;- tibble::tibble(\n    id = html_aside |&gt; rvest::html_elements(\"button\") |&gt; rvest::html_attr(\"id\"),\n    icon_type = html_aside |&gt; rvest::html_elements(\"button\") |&gt; rvest::html_element(\"li-icon\") |&gt; rvest::html_attr(\"type\")\n  ) |&gt;\n    dplyr::filter(icon_type == \"chevron-down\")\n  if(nrow(df_messaging) == 1){\n    id &lt;- df_messaging$id\n    click_minimize &lt;- function(driver, id){\n      driver$client$findElement(\"id\", id)$clickElement()\n      return(\"Sucessefully minimized\")\n    }\n    possibly_click_minimize &lt;- possibly(click_minimize)\n    minimized &lt;- possibly_click_minimize(driver = driver, id = id)\n    nap_rnorm(\"short\")\n  }\n}\nsearch_jobs &lt;- function(driver, keywords = NULL, date_posted = c(\"anytime\", \"month\", \"week\", \"24hrs\"), workplace_types = c(\"on-site\", \"remote\", \"hybrid\"), location = NULL, distance_mi = 5){\n  assertthat::assert_that(!is.null(keywords))\n  keywords = glue::glue(\"&keywords={keywords}\")\n  date_posted &lt;- get_datePosted(date_posted)\n  workplace_types &lt;- get_workPlace_types(workplace_types)\n  if(!is.null(location)){\n    location &lt;- glue::glue(\"&location={location}\")\n  } else{\n    location &lt;- \"\"\n  }\n  if((\"on-site\" %in% workplace_types | \"hybrid\" %in% workplace_types) & !is.null(distance_mi)){\n    distance &lt;- glue::glue(\"&distance={distance_mi}\")\n  } else{\n    distance &lt;- \"\"\n  }\n  jobs_url &lt;- URLencode(glue::glue(\"https://www.linkedin.com/jobs/search/?{date_posted}{workplace_types}{distance}{location}{keywords}&refresh=T\"))\n  driver$client$navigate(jobs_url)\n  nap_rnorm(\"moderate\")\n  message(glue::glue(\"Selenium is now at {jobs_url}\"))\n  return(jobs_url)\n}\nget_datePosted &lt;- function(arg){\n  choices &lt;- c(\"anytime\", \"month\", \"week\", \"24hrs\")\n  arg &lt;- match.arg(arg, choices = choices, several.ok = F)\n  if(arg == \"anytime\"){\n    date_posted = \"\"\n  } else if(arg == \"month\"){\n    date_posted = \"f_TPR=r604800\"\n  } else if(arg == \"week\"){\n    date_posted = \"f_TPR=r2592000\"\n  } else if(arg == \"24hrs\"){\n    date_posted = \"f_TPR=r86400\"\n  } else {\n    message(\"Something went wrong with get_datePosted()\")\n  }\n  return(date_posted)\n}\nget_workPlace_types &lt;- function(args){\n  choices &lt;- c(\"on-site\", \"remote\", \"hybrid\")\n  args &lt;- match.arg(args, choices = choices, several.ok = T)\n  args &lt;- ifelse(args == \"on-site\"\n                            , \"1\", ifelse(args == \"remote\", \"2\", ifelse(args == \"hybrid\", \"3\", NA_character_)))\n  args &lt;- paste0(\"&f_WT=\", paste(args, collapse = \",\"))\n  return(args)\n}\nscrollto_element &lt;- function(driver, using = c(\"xpath\", \"css selector\", \"id\", \"name\", \"tag name\", \"class name\", \"link text\", \"partial link text\"), value, nap_length = \"short\"){\n  html &lt;- scrape_html(driver)\n  df_pages &lt;- scrape_pages_df(html)\n  scrollto &lt;- function(driver, using, value, nap_length){\n    webElem &lt;- driver$client$findElement(using, value)\n    webElem$getElementLocationInView()\n    nap_rnorm(nap_length)\n    return(\"Success\")\n  }\n  possibly_scrollto &lt;- possibly(insistently(scrollto, rate = rate_backoff(pause_base = 5, pause_cap = 45, max_times = 3, jitter = T), quiet = FALSE), otherwise = \"Fail\")\n  scrollto_result &lt;- possibly_scrollto(driver = driver, using = using, value = value, nap_length = nap_length)\n  return(scrollto_result)\n}\nscrollto_paging &lt;- function(driver){\n  html &lt;- scrape_html(driver)\n  df_pages &lt;- scrape_pages_df(html)\n  scrollto_element(driver, \"id\", df_pages$id[1])\n}\nload_jobs_list &lt;- function(driver, n_jobs = 0){\n  if(n_jobs == 0){\n    scrollto_paging(driver)\n  }\n  df_jobs_list &lt;- scrape_jobs_list_df(driver)\n  if(n_jobs &lt; nrow(df_jobs_list)){\n    randwalk_jobs(driver, df_jobs_list$id)\n    load_jobs_list(driver, n_jobs = nrow(df_jobs_list))\n  } else {\n    message(glue::glue(\"Loaded {n_jobs} jobs\"))\n  }\n}\nrandwalk_jobs &lt;- function(driver, ids){\n  ids &lt;- ids |&gt;\n    sort() |&gt;\n    sample()\n  purrr::walk(ids, function(id){\n    scrollto_element(driver, \"id\", id, nap_length = .25)\n  })\n}\nclick_button &lt;- function(driver, using = c(\"xpath\", \"css selector\", \"id\", \"name\", \"tag name\", \"class name\", \"link text\", \"partial link text\"),  value, nap_length = \"short\"){\n  url &lt;- driver$client$getCurrentUrl()\n  click &lt;- function(driver, using, value, nap_length){\n    driver$client$findElement(using = using, value = value)$getElementLocationInView()\n    driver$client$findElement(using = using, value = value)$clickElement()\n    nap_rnorm(nap_length)\n    message &lt;- \"Success\"\n    return(message)\n  }\n  possibly_click &lt;- possibly(insistently(click, rate = rate_backoff(pause_base = 5, pause_cap = 45, max_times = 3, jitter = T), quiet = FALSE), otherwise = url)\n  click_result &lt;- possibly_click(driver = driver, using = using, value = value, nap_length = nap_length)\n  return(click_result)\n}\n\n# Scrape #### \nscrape_html &lt;- function(driver){\n  html &lt;- rvest::read_html(driver$client$getPageSource()[[1]])\n  return(html)\n}\nif_length0_NA_character &lt;- function(var){\n  if(length(var) == 0){\n    x &lt;- NA_character_\n  } else {\n    x &lt;- var\n  }\n  return(x)\n}\nscrape_pages_df &lt;- function(html){\n  df_pages &lt;- tibble::tibble(\n    pagination_btn = html |&gt; rvest::html_element(\"body\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"data-test-pagination-page-btn\"),\n    id = html |&gt; rvest::html_elements(\"body\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"id\")\n  ) |&gt;\n    dplyr::mutate(pagination_btn = as.numeric(pagination_btn)) |&gt;\n    dplyr::mutate(pagination_btn_ellipsis = lag(pagination_btn) +1) |&gt;\n    dplyr::mutate(pagination_btn = coalesce(pagination_btn, pagination_btn_ellipsis),   .keep = \"unused\") |&gt;\n    dplyr::filter(stringr::str_detect(id, \"ember\") & !is.na(pagination_btn))\n  return(df_pages)\n}\nscrape_jobs_list_df &lt;- function(driver){\n  html &lt;- scrape_html(driver = driver)\n  job_li_ids &lt;- get_li_ids(html) |&gt; remove_blank_ids()\n  df_jobs_list &lt;- purrr::map(job_li_ids, function(li_id){\n    li_index &lt;- which(get_li_ids(html) == li_id)\n    li_element &lt;- get_li_elements(html, li_index)\n    id &lt;- li_element |&gt; rvest::html_elements(\"a\") |&gt; rvest::html_attr(\"id\")\n    class &lt;- li_element |&gt; rvest::html_elements(\"a\") |&gt; rvest::html_attr(\"class\")\n    href &lt;- li_element |&gt; rvest::html_elements(\"a\") |&gt; rvest::html_attr(\"href\")\n    url &lt;- paste0(\"https://www.linkedin.com/\", href)\n    img &lt;- li_element |&gt; rvest::html_element(\"div\") |&gt;rvest::html_element(\"img\") |&gt; rvest::html_attr(\"src\")\n    alt &lt;- li_element |&gt; rvest::html_elements(\"img\") |&gt; rvest::html_attr(\"alt\") |&gt; stringr::str_squish() |&gt; _[1]\n    text &lt;- li_element |&gt; rvest::html_elements(\"a\") |&gt; rvest::html_text() |&gt; stringr::str_squish()\n    tibble::tibble(\n      li_id = li_id,\n      id = id,\n      class = class,\n      url = url,\n      img = img,\n      alt = alt,\n      text = text,\n    )\n  }) |&gt;\n    purrr::list_rbind() |&gt;\n    dplyr::filter(stringr::str_detect(url, pattern = \"/jobs/view/\")) |&gt;\n    dplyr::filter(stringr::str_detect(class, pattern = \"job-card-list__title\"))\n  return(df_jobs_list)\n}\nget_li_ids &lt;- function(html){\n  li_ids &lt;- html |&gt; rvest::html_element(\"main\") |&gt; rvest::html_element(\"ul\") |&gt; rvest::html_elements(\"li\") |&gt; rvest::html_attr(\"id\")\n  return(li_ids)\n}\nremove_blank_ids &lt;- function(ids){\n  keep_ids &lt;- ids[!is.na(ids)]\n  return(keep_ids)\n}\nget_li_elements &lt;- function(html, index){\n  li_elements &lt;- html |&gt; rvest::html_element(\"main\") |&gt; rvest::html_element(\"ul\") |&gt; rvest::html_elements(\"li\")\n  li_element &lt;- li_elements[index]\n  return(li_element)\n}\nscrape_job_details_df &lt;- function(driver, id = NULL, view = c(\"pane\", \"page\")){\n  html &lt;- scrape_html(driver)\n  html_job_details &lt;- extract_job_details_html(html, view)\n  html_top_card &lt;- extract_top_card_html(html_job_details, view)\n  df_job_details &lt;- scrape_top_card_df(html_top_card)\n  html_job_desc_card &lt;- extract_job_desc_card_html(html_job_details)\n  df_job_desc &lt;- scrape_job_desc_df(html_job_desc_card)\n  df_job_details &lt;- dplyr::bind_cols(df_job_details, df_job_desc)\n  html_salary_card &lt;- extract_salary_card_html(html_job_details)\n  df_job_details &lt;- dplyr::bind_cols(\n    df_job_details,\n    scrape_salary_df(html_salary_card))\n  open_skills_button_id &lt;- get_skills_open_button_id(html_job_details)\n  open_skills_click_result &lt;- click_button(driver, \"id\", open_skills_button_id, nap_length = \"long\")\n  if(open_skills_click_result == \"Success\"){\n    html &lt;- scrape_html(driver)\n    html_skills_modal &lt;- extract_skills_modal_html(html)\n    close_skills_button_id &lt;- get_skills_close_button_id(html_skills_modal)\n    close_skills_click_result &lt;- click_button(driver, \"id\", close_skills_button_id, nap_length = \"short\")\n    if(close_skills_click_result != \"Success\"){driver$client$goBack()}\n    skills &lt;- scrape_skills_chr(html_skills_modal)\n    df_job_details$skills &lt;- list(skills)\n  } else {\n    skills &lt;- NA_character_\n    df_job_details$skills &lt;- list(skills)\n  }\n  df_job_details$id &lt;- id\n  return(df_job_details)\n}\n\nextract_job_details_html &lt;- function(html, view = c(\"pane\", \"page\")){\n  view &lt;- match.arg(view, choices = c(\"pane\", \"page\"), several.ok = F)\n  if(view == \"pane\"){\n    class_pattern &lt;- \"scaffold-layout__detail\"\n  } else if(view == \"page\"){\n    class_pattern &lt;- \"job-view-layout jobs-details\"\n  } else{\n    class_pattern &lt;- \"jobs-details\"\n  }\n  job_details_html_index &lt;- tibble::tibble(class = html |&gt;  rvest::html_elements(\"body\") |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(class, pattern = class_pattern)) |&gt;\n    pull(rn)\n  html_job_details &lt;- html |&gt;  rvest::html_elements(\"body\") |&gt;  rvest::html_elements(\"div\") |&gt; _[job_details_html_index]\n  return(html_job_details)\n}\nextract_top_card_html &lt;- function(html_job_details, view = c(\"pane\", \"page\")){\n  view &lt;- match.arg(view, choices = c(\"pane\", \"page\"), several.ok = F)\n  if(view == \"pane\"){\n    top_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n      dplyr::mutate(rn = dplyr::row_number()) |&gt;\n      filter(class == \"jobs-unified-top-card__content--two-pane\") |&gt;\n      pull(rn)\n    html_top_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[top_card_html_index]\n  } else if(view == \"page\"){\n    top_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n      dplyr::mutate(rn = dplyr::row_number()) |&gt;\n      filter(stringr::str_detect(class, \"jobs-unified-top-card*.*artdeco-card\")) |&gt;\n      pull(rn)\n    html_top_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[top_card_html_index]\n  }\n  return(html_top_card)\n}\nscrape_top_card_df &lt;- function(html_top_card){\n  title &lt;- html_top_card |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_text() |&gt; str_squish() |&gt; _[1]\n  subline &lt;- html_top_card |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_text() |&gt; str_squish() |&gt; _[2]\n  company_name &lt;- str_split_i(subline, pattern = \"·\", i = 1) |&gt; str_squish()\n  location &lt;- str_extract(subline, pattern = \"([A-Z][a-z]+\\\\s?)+,\\\\s[A-Z]{2}\")\n  workplace_type &lt;- if_else(\n    str_detect(str_to_lower(subline), pattern = \"remote\"),\n    \"remote\",\n    if_else(\n      str_detect(str_to_lower(subline), pattern = \"hybrid\"),\n      \"hybrid\",\n      if_else(\n        str_detect(str_to_lower(subline), pattern = \"on-site\"),\n        \"on-site\",\n        \"not provided\")))\n  df_list_items &lt;- tibble::tibble(\n    class = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_attr(\"class\"),\n    text = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_text(),\n    icon_type = html_top_card |&gt; html_element(\"ul\") |&gt; html_elements(\"li\") |&gt; html_element(\"li-icon\") |&gt; html_attr(\"type\")\n  )\n  employment_type &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"job\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"Full-time|Part-time|Contract|Volunteer|Temporary|Internship|Other\")\n  job_level &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"job\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"Internship|Entry Level|Associate|Mid-Senior level|Director|Executive\")\n  company_size &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"company\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_extract(pattern = \"[0-9]{1,},?[0-9]*(-|\\\\+)([0-9]{1,},?[0-9]*)?\")\n  company_industry &lt;- df_list_items |&gt;\n    dplyr::filter(icon_type == \"company\") |&gt;\n    dplyr::pull(text) |&gt;\n    str_split_i(pattern = \"·\", i = 2) |&gt;\n    str_squish()\n  df_job_details &lt;- tibble::tibble(\n    job_title = if_length0_NA_character(title),\n    job_level = if_length0_NA_character(job_level),\n    company_name = if_length0_NA_character(company_name),\n    company_industry = if_length0_NA_character(company_industry),\n    company_size = if_length0_NA_character(company_size),\n    location = if_length0_NA_character(location),\n    workplace_type = if_length0_NA_character(workplace_type),\n    employment_type = if_length0_NA_character(employment_type)\n  )\n  return(df_job_details)\n}\nextract_job_desc_card_html &lt;- function(html_job_details){\n  job_desc_card_html_index &lt;- tibble::tibble(class = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(stringr::str_detect(class, \"jobs-description-content\")) |&gt;\n    pull(rn)\n  html_job_desc_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_elements(\"div\") |&gt;  _[job_desc_card_html_index]\n  return(html_job_desc_card)\n}\nscrape_job_desc_df &lt;- function(html_job_desc_card){\n  job_desc &lt;- html_job_desc_card |&gt; html_text() |&gt; paste(collapse = (\" \")) |&gt;  stringr::str_squish()\n  df_job_desc &lt;- tibble::tibble(job_desc = job_desc)\n  return(df_job_desc)\n}\nextract_salary_card_html &lt;- function(html_job_details){\n  salary_card_html_index &lt;- tibble::tibble(id = html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"id\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(id == \"SALARY\") |&gt;\n    pull(rn)\n  html_salary_card &lt;- html_job_details |&gt;  rvest::html_elements(\"div\") |&gt; _[salary_card_html_index]\n  return(html_salary_card)\n}\nscrape_salary_df &lt;- function(html_salary_card){\n  salary_text_index &lt;- html_salary_card |&gt; rvest::html_elements(\"p\") |&gt; rvest::html_text() |&gt; stringr::str_detect(pattern = \"(from job description)|salary\")\n  salary_text &lt;- html_salary_card |&gt; rvest::html_elements(\"p\") |&gt; rvest::html_text() |&gt; _[salary_text_index] |&gt; stringr::str_squish()\n  if(length(salary_text) != 0){\n    salary_interval &lt;- str_extract_all(salary_text, pattern = \"[1-9][0-9]?[0-9]?,?[0-9]?[0-9]?[0-9]?\") |&gt; purrr::map(str_remove, pattern = \",\") |&gt; purrr::map(as.double)\n    df_salary &lt;- tibble(salary_min = min(unlist(salary_interval)), salary_max = max(unlist(salary_interval)))\n  } else {\n    df_salary &lt;- tibble(salary_min = NA_real_, salary_max = NA_real_)\n  }\n  return(df_salary)\n}\nget_skills_open_button_id &lt;- function(html_job_details){\n  skills_box_html_index &lt;- tibble::tibble(\n    class = html_job_details |&gt;   html_elements(\"div\")  |&gt; rvest::html_attr(\"class\")) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(str_to_lower(class), pattern = \"job-details-how-you-match-card__container\")) |&gt;\n    pull(rn)\n  html_skills_box &lt;-html_job_details |&gt; html_elements(\"div\") |&gt; _[skills_box_html_index]\n  button_id &lt;- html_skills_box |&gt; html_elements(\"button\") |&gt; html_attr(\"id\")\n  return(button_id)\n}\nget_skills_close_button_id &lt;- function(html_skills_modal){\n  html_x_button_index &lt;- tibble::tibble(\n    aria_label = html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"aria-label\"),\n    id = html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"id\")\n  ) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_to_lower(aria_label) == \"dismiss\") |&gt;\n    pull(rn)\n  x_button_id &lt;- html_skills_modal |&gt; html_elements(\"button\") |&gt; html_attr(\"id\") |&gt; _[html_x_button_index]\n}\nextract_skills_modal_html &lt;- function(html){\n  skills_modal_index &lt;- tibble::tibble(\n    role = html |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"role\"),\n    class = html |&gt; rvest::html_elements(\"div\") |&gt; rvest::html_attr(\"class\")\n    ) |&gt;\n    dplyr::mutate(rn = dplyr::row_number()) |&gt;\n    filter(str_detect(class, \"job-details-skill-match-modal\") & role == \"dialog\") |&gt;\n    pull(rn)\n  html_skills_modal &lt;- html |&gt;  rvest::html_elements(\"div\") |&gt; _[skills_modal_index]\n  return(html_skills_modal)\n}\nscrape_skills_chr &lt;- function(html_skills_modal){\n  skills &lt;- html_skills_modal |&gt;\n    html_elements(\"li\") |&gt;\n    html_text() |&gt;\n    str_squish() |&gt;\n    str_remove(pattern = \"Add$\") |&gt;\n    str_squish()\n  skills &lt;- skills[skills != \"\"]\n  return(skills)\n}\nscrape_jobs_df &lt;- function(driver){\n  load_jobs_list(driver)\n  df_job_list &lt;- scrape_jobs_list_df(driver)\n  df_job_details &lt;- map(df_job_list$id, function(id){\n    click_job_result &lt;- click_button(driver, \"id\", id, nap_length = \"moderate\")\n    if(click_job_result == \"Success\"){\n      df_job_details &lt;- scrape_job_details_df(driver, id, view = \"pane\")\n    } else {\n      df_job_details &lt;- get_df_job_details_schema(id = id)\n    }\n    return(df_job_details)\n  }) |&gt; bind_rows()\n  df_jobs &lt;- dplyr::left_join(df_job_list, df_job_details, by = c(\"id\" = \"id\"))\n  details_cols &lt;- get_df_job_details_schema() |&gt; select(-id) |&gt; names()\n  filter_blank_details &lt;- . %&gt;%\n    dplyr::filter(dplyr::if_all(.cols = details_cols, ~is.na(.)))\n  remove_details_columns &lt;- . %&gt;%\n    select(-tidyselect::all_of(details_cols))\n  df_missing_details &lt;- df_jobs |&gt;\n    filter_blank_details() |&gt;\n    remove_details_columns()\n  if(nrow(df_missing_details)&gt;0){\n    df_jobs &lt;- df_jobs |&gt; filter(id %in% df_missing_details$id)\n    df_retry_details &lt;-purrr::map2(df_missing_details$id, df_missing_details$url, function(id, url){\n      click_job_result &lt;- click_button(driver, \"id\", id, nap_length = \"moderate\")\n      driver$client$navigate(url)\n      nap_rnorm(\"long\")\n      df_retry &lt;- scrape_job_details_df(driver, id = id, view = \"page\")\n      return(df_retry)\n    })\n    df_missing_details &lt;- dplyr::left_join(df_missing_details, df_retry_details, by = c(\"id\" = \"id\"))\n    df_jobs &lt;- dplyr::bind_rows(df_jobs, df_missing_details)\n  }\n  return(df_jobs)\n}\nscrape_job_search &lt;- function(driver, page_cap = NULL){\n  df_pages &lt;- scrape_pages_df(scrape_html(driver))\n  pages &lt;- seq.int(from = 1, to = min(max(df_pages$pagination_btn), page_cap), by = 1)\n  message(glue::glue(\"Preparing to scrape {max(pages)} pages...\"))\n  df_job_search &lt;- purrr::map(pages, function(i){\n    df_jobs &lt;- scrape_jobs_df(driver)\n    df_pages &lt;- scrape_pages_df(scrape_html(driver)) |&gt; dplyr::filter(pagination_btn == i + 1)\n    if(nrow(df_pages) == 1){\n      page_click_result &lt;- click_button(driver, \"id\", df_pages$id, nap_length = \"long\")\n      if(page_click_result == \"Success\"){\n        message(glue::glue(\"Selenium is now on page {df_pages$pagination_btn} at {driver$client$getCurrentUrl()}\"))\n      } else {\n        next_page_url &lt;- modify_job_search_url_page_n(driver, n = i + 1)\n        driver$client$navigate(next_page_url)\n        message(glue::glue(\"scrape_pages_df() did not get a matching page, using alternative navigation to go to page {i + 1} at {driver$client$getCurrentUrl()}\"))\n      }\n    } else{\n      message(\"Done\")\n    }\n    return(df_jobs)\n  }) |&gt; dplyr::bind_rows()\n  return(df_job_search)\n}\n\n# Utils ####\nget_df_job_details_schema &lt;- function(id = \"\"){\n  tibble::tibble(\n    job_title = NA_character_,\n    job_level = NA_character_,\n    company_name = NA_character_,\n    company_industry = NA_character_,\n    company_size = NA_character_,\n    location = NA_character_,\n    workplace_type = NA_character_,\n    employment_type = NA_character_,\n    salary_min = NA_character_,\n    salary_max = NA_character_,\n    skills = NA_character_,\n    id = id\n  )\n}\nget_job_search_page_n &lt;- function(driver){\n  n &lt;- as.numeric(stringr::str_remove(stringr::str_extract(driver$client$getCurrentUrl(), \"start=[1-9][0-9][0-9]?[0-9]?[0-9]?\"), \"start=\")) / 25\n  return(n)\n}\nmodify_job_search_url_page_n &lt;- function(driver, n = 1){\n  if(n &lt;= 1){\n    start &lt;- 1\n  } else {\n    start &lt;- n * 25 - 25\n  }\n  url &lt;- stringr::str_remove(driver$client$getCurrentUrl(), \"&start=[1-9][0-9][0-9]?[0-9]?[0-9]?\")\n  url &lt;- glue::glue(\"{url}&start={start}\")\n  return(url)\n}\nkill_driver(driver)\n\n\nWhich we can test out for a few pages of jobs with the script below to get all the data in one data frame 😎\n\n\nCode\ndriver &lt;- open_driver(port = 1446L)\nsignin(driver)\nminimize_messaging(driver)\nsearch_jobs(driver, keywords = \"data scientist\", date_posted = \"anytime\", location = \"United States\", workplace_types = c(\"remote\"))\ndf_jobs &lt;- scrape_job_search(driver, page_cap = 2)\nkill_driver(driver)\nglimpse(df_jobs)\n\n\n\n\nRows: 50\nColumns: 19\n$ li_id            &lt;chr&gt; \"ember222\", \"ember235\", \"ember247\", \"ember260\", \"embe…\n$ id               &lt;chr&gt; \"ember228\", \"ember241\", \"ember253\", \"ember266\", \"embe…\n$ class            &lt;chr&gt; \"disabled ember-view job-card-container__link job-car…\n$ url              &lt;chr&gt; \"https://www.linkedin.com//jobs/view/3673567890/?eBP=…\n$ img              &lt;chr&gt; \"https://media.licdn.com/dms/image/C560BAQEKdbNtIQcvy…\n$ alt              &lt;chr&gt; \"innoVet Health (SDVOSB) logo\", \"TRM Labs logo\", \"Wha…\n$ text             &lt;chr&gt; \"Senior Data Scientist\", \"Senior Data Scientist\", \"Se…\n$ job_title        &lt;chr&gt; \"Senior Data Scientist\", \"Senior Data Scientist\", \"Se…\n$ job_level        &lt;chr&gt; NA, \"Mid-Senior level\", \"Mid-Senior level\", \"Mid-Seni…\n$ company_name     &lt;chr&gt; \"innoVet Health (SDVOSB)\", \"TRM Labs\", \"Whatnot\", \"Li…\n$ company_industry &lt;chr&gt; NA, \"Information Services\", \"Technology, Information …\n$ company_size     &lt;chr&gt; \"11-50\", \"51-200\", \"201-500\", \"51-200\", \"51-200\", \"11…\n$ location         &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, \"Seattle, WA\"…\n$ workplace_type   &lt;chr&gt; \"remote\", \"remote\", \"remote\", \"remote\", \"remote\", \"re…\n$ employment_type  &lt;chr&gt; \"Full-time\", \"Full-time\", \"Full-time\", \"Full-time\", \"…\n$ job_desc         &lt;chr&gt; \"About the job InnoVet Health, a small and growing bu…\n$ salary_min       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 157500, 144000, 88, 1…\n$ salary_max       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, 205000, 157000, 98, 1…\n$ skills           &lt;list&gt; &lt;\"Data Analysis\", \"Data Analytics\", \"Data Science\", …"
  }
]